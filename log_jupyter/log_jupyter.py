
  test_jupyter /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_jupyter', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_jupyter 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '14ea42490a753d0445fda074e8d3eac878d2aeed', 'workflow': 'test_jupyter'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_jupyter

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/14ea42490a753d0445fda074e8d3eac878d2aeed
Package                   Version    Location
------------------------- ---------- -----------------------------------
absl-py                   0.9.0
alembic                   1.4.2
astor                     0.8.1
attrs                     19.3.0
autogluon                 0.0.5
backcall                  0.1.0
bcrypt                    3.1.7
bleach                    3.1.5
blis                      0.4.1
boto                      2.49.0
boto3                     1.9.187
botocore                  1.12.253
catalogue                 1.0.0
catboost                  0.23.1
certifi                   2020.4.5.1
cffi                      1.14.0
chardet                   3.0.4
cli-code                  28.1.0
click                     7.1.2
cliff                     3.1.0
cloudpickle               1.4.1
cmd2                      0.8.9
cmdstanpy                 0.4.0
colorlog                  4.1.0
configparser              5.0.0
ConfigSpace               0.4.10
convertdate               2.2.1
cryptography              2.9.2
cycler                    0.10.0
cymem                     2.0.3
Cython                    0.29.17
dask                      2.6.0
databricks-cli            0.10.0
dataclasses               0.7
decorator                 4.4.2
deepctr                   0.7.4
defusedxml                0.6.0
dill                      0.3.1.1
distributed               2.6.0
docker                    4.2.0
docutils                  0.15.2
entrypoints               0.3
ephem                     3.7.7.1
fbprophet                 0.6
Flask                     1.1.2
future                    0.18.2
gast                      0.2.2
gensim                    3.8.3
gitdb                     4.0.5
GitPython                 3.1.2
gluoncv                   0.7.0
gluonnlp                  0.8.1
gluonts                   0.4.2
google-pasta              0.2.0
googleapis-common-protos  1.51.0
gorilla                   0.3.0
graphviz                  0.8.4
grpcio                    1.29.0
gunicorn                  20.0.4
h5py                      2.10.0
HeapDict                  1.0.1
holidays                  0.10.2
hyperopt                  0.1.2
idna                      2.9
importlib-metadata        1.6.0
ipykernel                 5.2.1
ipython                   7.14.0
ipython-genutils          0.2.0
itsdangerous              1.1.0
jedi                      0.17.0
Jinja2                    2.11.2
jmespath                  0.10.0
joblib                    0.15.0
jsonschema                3.2.0
jupyter-client            6.1.3
jupyter-core              4.6.3
Keras                     2.3.1
Keras-Applications        1.0.8
keras-contrib             2.0.8
keras-mdn-layer           0.2.1
Keras-Preprocessing       1.1.2
kiwisolver                1.2.0
korean-lunar-calendar     0.2.1
lightgbm                  2.3.0
LunarCalendar             0.0.9
Mako                      1.1.2
Markdown                  3.2.2
MarkupSafe                1.1.1
matchzoo-py               1.1.1
matplotlib                3.2.1
mistune                   0.8.4
mlflow                    1.7.1
mlmodels                  0.35.2     /home/runner/work/mlmodels/mlmodels
msgpack                   1.0.0
murmurhash                1.0.2
mxnet                     1.6.0
nbconvert                 5.6.1
nbformat                  5.0.6
networkx                  2.4
nltk                      3.5
notebook                  6.0.3
numexpr                   2.7.1
numpy                     1.18.2
opt-einsum                3.2.1
optuna                    1.1.0
packaging                 20.3
pandas                    0.25.3
pandocfilters             1.4.2
paramiko                  2.7.1
parso                     0.7.0
pbr                       5.4.5
pexpect                   4.8.0
pickleshare               0.7.5
Pillow                    6.2.1
pip                       20.1
plac                      1.1.3
plotly                    4.7.1
portalocker               1.7.0
preshed                   3.0.2
prettytable               0.7.2
prometheus-client         0.7.1
prometheus-flask-exporter 0.13.0
promise                   2.3
prompt-toolkit            3.0.5
protobuf                  3.12.0
psutil                    5.7.0
ptyprocess                0.6.0
pyaml                     20.4.0
pycparser                 2.20
pydantic                  1.4
Pygments                  2.6.1
PyMeeus                   0.3.7
pymongo                   3.10.1
PyNaCl                    1.3.0
pyparsing                 2.4.7
pyperclip                 1.8.0
pyrsistent                0.16.0
pystan                    2.19.1.1
python-dateutil           2.8.0
python-editor             1.0.4
pytorch-lightning         0.7.3
pytorch-transformers      1.2.0
pytz                      2020.1
PyYAML                    5.3.1
pyzmq                     19.0.1
querystring-parser        1.2.4
regex                     2020.5.14
requests                  2.23.0
retrying                  1.3.3
s3transfer                0.2.1
sacremoses                0.0.43
scikit-learn              0.21.2
scikit-optimize           0.7.4
scipy                     1.4.1
Send2Trash                1.5.0
sentence-transformers     0.2.4
sentencepiece             0.1.90
setuptools                45.2.0
setuptools-git            1.2
simplejson                3.17.0
six                       1.14.0
smart-open                2.0.0
smmap                     3.0.4
sortedcontainers          2.1.0
spacy                     2.2.4
SQLAlchemy                1.3.13
sqlparse                  0.3.1
srsly                     1.0.2
stevedore                 1.32.0
tabulate                  0.8.7
tblib                     1.6.0
tensorboard               1.15.0
tensorboardX              2.0
tensorflow                1.15.2
tensorflow-datasets       3.0.0
tensorflow-estimator      1.15.1
tensorflow-metadata       0.22.0
tensorflow-probability    0.7.0
termcolor                 1.1.0
terminado                 0.8.3
testpath                  0.4.4
thinc                     7.4.0
toml                      0.10.1
toolz                     0.10.0
torch                     1.2.0
torchtext                 0.6.0
torchvision               0.4.0
tornado                   6.0.4
tqdm                      4.46.0
traitlets                 4.3.3
transformers              2.3.0
typing                    3.7.4.1
ujson                     1.35
urllib3                   1.25.9
versioneer                0.18
wasabi                    0.6.0
wcwidth                   0.1.9
webencodings              0.5.1
websocket-client          0.57.0
Werkzeug                  1.0.1
wheel                     0.34.2
wrapt                     1.12.1
zict                      2.0.0
zipp                      3.1.0

 ************************************************************************************************************************

 ************************************************************************************************************************
/home/runner/work/mlmodels/mlmodels/mlmodels/example/
############ List of files ################################
['ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow_1_lstm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras-textcnn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_hyper.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m5.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_model.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py']





 ************************************************************************************************************************
############ Running Jupyter files ################################





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_svm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;34m[0m[0m
[0;32m----> 6[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;34m[0m[0m
[1;32m      6[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//fashion_MNIST_mlmodels.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_home_retail.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_home_retail.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_home_retail.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras_charcnn_reuters.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mmodel_pars[0m      [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'model_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mdata_pars[0m       [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'data_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mcompute_pars[0m    [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'compute_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mout_pars[0m        [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'out_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'reuters_charcnn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl.ipynb 

/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073
Warning: `hyperparameter_tune=True` is currently experimental and may cause the process to hang. Setting `auto_stack=True` instead is recommended to achieve maximum quality models.
Beginning AutoGluon training ... Time limit = 120s
AutoGluon will save models to dataset/
Train Data Rows:    39073
Train Data Columns: 15
Preprocessing data ...
Here are the first 10 unique label values in your data:  [' Tech-support' ' Transport-moving' ' Other-service' ' ?'
 ' Handlers-cleaners' ' Sales' ' Craft-repair' ' Adm-clerical'
 ' Exec-managerial' ' Prof-specialty']
AutoGluon infers your prediction problem is: multiclass  (because dtype of label-column == object)
If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])

Feature Generator processed 39073 data points with 14 features
Original Features:
	int features: 6
	object features: 8
Generated Features:
	int features: 0
All Features:
	int features: 6
	object features: 8
	Data preprocessing and feature engineering runtime = 0.21s ...
AutoGluon will gauge predictive performance using evaluation metric: accuracy
To change this, specify the eval_metric argument of fit()
AutoGluon will early stop models using evaluation metric: accuracy
Saving dataset/learner.pkl
Beginning hyperparameter tuning for Gradient Boosting Model...
Hyperparameter search space for Gradient Boosting Model: 
num_leaves:   Int: lower=26, upper=30
learning_rate:   Real: lower=0.005, upper=0.2
feature_fraction:   Real: lower=0.75, upper=1.0
min_data_in_leaf:   Int: lower=2, upper=30
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
Warning: Exception caused LightGBMClassifier to fail during hyperparameter tuning... Skipping this model.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
ValueError: Illegal default value 36
Saving dataset/models/trainer.pkl
Beginning hyperparameter tuning for Neural Network...
Hyperparameter search space for Neural Network: 
network_type:   Categorical['widedeep', 'feedforward']
layers:   Categorical[[100], [1000], [200, 100], [300, 200, 100]]
activation:   Categorical['relu', 'softrelu', 'tanh']
embedding_size_factor:   Real: lower=0.5, upper=1.5
use_batchnorm:   Categorical[True, False]
dropout_prob:   Real: lower=0.0, upper=0.5
learning_rate:   Real: lower=0.0001, upper=0.01
weight_decay:   Real: lower=1e-12, upper=0.1
AutoGluon Neural Network infers features are of the following types:
{
    "continuous": [
        "age",
        "education-num",
        "hours-per-week"
    ],
    "skewed": [
        "fnlwgt",
        "capital-gain",
        "capital-loss"
    ],
    "onehot": [
        "sex",
        "class"
    ],
    "embed": [
        "workclass",
        "education",
        "marital-status",
        "relationship",
        "race",
        "native-country"
    ],
    "language": []
}


Saving dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Saving dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
Starting Experiments
Num of Finished Tasks is 0
Num of Pending Tasks is 5
  0%|          | 0/5 [00:00<?, ?it/s]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Finished Task with config: {'activation.choice': 0, 'dropout_prob': 0.1, 'embedding_size_factor': 1.0, 'layers.choice': 0, 'learning_rate': 0.0005, 'network_type.choice': 0, 'use_batchnorm.choice': 0, 'weight_decay': 1e-06} and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:47<01:10, 23.50s/it]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Finished Task with config: {'activation.choice': 1, 'dropout_prob': 0.07547357258238448, 'embedding_size_factor': 0.5470991389597457, 'layers.choice': 2, 'learning_rate': 0.005218560041244485, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 0.07823277914467376} and reward: 0.2424
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xb3R<m\xf4 \xadX\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1\x81\xd6\r\xb0\x0fLX\r\x00\x00\x00layers.choiceq\x04K\x02X\r\x00\x00\x00learning_rateq\x05G?u`\x0e\x8bY\x879X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?\xb4\x07\x10;\xe6\xcc\xf1u.' and reward: 0.2424
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xb3R<m\xf4 \xadX\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1\x81\xd6\r\xb0\x0fLX\r\x00\x00\x00layers.choiceq\x04K\x02X\r\x00\x00\x00learning_rateq\x05G?u`\x0e\x8bY\x879X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?\xb4\x07\x10;\xe6\xcc\xf1u.' and reward: 0.2424
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:35<01:01, 30.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:35<01:03, 31.72s/it]
Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Finished Task with config: {'activation.choice': 0, 'dropout_prob': 0.11518656639428662, 'embedding_size_factor': 1.4974687129713549, 'layers.choice': 3, 'learning_rate': 0.0065705411970661464, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 3.896471354143596e-08} and reward: 0.3872
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xbd|\xdd\xe7\x9a\x1cBX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf7\xf5\xa1\xc0\xcf\xecBX\r\x00\x00\x00layers.choiceq\x04K\x03X\r\x00\x00\x00learning_rateq\x05G?z\xe9\xb68\xefI\x97X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G>d\xebD\xfa\xc5\x88\xc6u.' and reward: 0.3872
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xbd|\xdd\xe7\x9a\x1cBX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf7\xf5\xa1\xc0\xcf\xecBX\r\x00\x00\x00layers.choiceq\x04K\x03X\r\x00\x00\x00learning_rateq\x05G?z\xe9\xb68\xefI\x97X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G>d\xebD\xfa\xc5\x88\xc6u.' and reward: 0.3872
Please either provide filename or allow plot in get_training_curves
Time for Neural Network hyperparameter optimization: 145.14808893203735
Best hyperparameter configuration for Tabular Neural Network: 
{'activation.choice': 0, 'dropout_prob': 0.11518656639428662, 'embedding_size_factor': 1.4974687129713549, 'layers.choice': 3, 'learning_rate': 0.0065705411970661464, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 3.896471354143596e-08}
Saving dataset/models/trainer.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Fitting model: weighted_ensemble_k0_l1 ... Training model for up to 119.79s of the -27.53s of remaining time.
Ensemble size: 45
Ensemble weights: 
[0.6        0.17777778 0.22222222]
	0.3934	 = Validation accuracy score
	0.99s	 = Training runtime
	0.0s	 = Validation runtime
Saving dataset/models/weighted_ensemble_k0_l1/model.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
AutoGluon training complete, total runtime = 148.55s ...
Loading: dataset/models/trainer.pkl
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769
Loading: dataset/models/trainer.pkl
Loading: dataset/models/weighted_ensemble_k0_l1/model.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
test

  #### Module init   ############################################ 

  <module 'mlmodels.model_gluon.gluon_automl' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py'> 

  #### Loading params   ############################################## 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 453, in test_cli
    test_module(arg.model_uri, param_pars=param_pars)  # '1_lstm'
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 257, in test_module
    model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py", line 109, in get_params
    return model_pars, data_pars, compute_pars, out_pars
UnboundLocalError: local variable 'model_pars' referenced before assignment





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vison_fashion_MNIST.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow_1_lstm.ipynb 

/home/runner/work/mlmodels/mlmodels
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
test

  #### Module init   ############################################ 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  #### Model init   ############################################ 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f9b2be23ac8> 

  #### Fit   ######################################################## 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   #################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
[[ 0.          0.          0.          0.          0.          0.        ]
 [-0.0182854  -0.01676878  0.0564476   0.12226599 -0.12388188  0.03643176]
 [-0.09083334  0.1096662   0.04757109 -0.11926425  0.22789046  0.23131654]
 [-0.03405919  0.00382574  0.20210388 -0.05683132 -0.06393629 -0.06525341]
 [-0.09104627  0.36522019 -0.26204976  0.523       0.32685265  0.2541106 ]
 [-0.36501405  0.02774938  0.48663524  0.06449956 -0.19649227  0.07513807]
 [ 0.19228548 -0.3008514   0.48829988 -0.26739845 -0.09975055 -0.06476645]
 [ 0.10532055  0.20678449  0.04826843  0.51224905 -0.25326723  0.40797538]
 [-0.16293949  0.42345577 -0.01083543 -0.16587475 -0.09172773  0.19101809]
 [ 0.          0.          0.          0.          0.          0.        ]]

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 
model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.3974001407623291, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 08:39:39.116729: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 455, in test_cli
    test(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 189, in test
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.4012460298836231, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 08:39:40.205101: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 457, in test_cli
    test_global(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 200, in test_global
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;34m[0m[0m
[0;32m---> 10[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras-textcnn.ipynb 

WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 400)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 400, 50)      500         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 398, 128)     19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 397, 128)     25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 396, 128)     32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 78,069
Trainable params: 78,069
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
 2613248/17464789 [===>..........................] - ETA: 0s
11665408/17464789 [===================>..........] - ETA: 0s
17465344/17464789 [==============================] - 0s 0us/step
Pad sequences (samples x time)...
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-05-16 08:39:50.906877: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-16 08:39:50.910733: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095245000 Hz
2020-05-16 08:39:50.910886: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e56bd3ad60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-16 08:39:50.910899: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 25000 samples, validate on 25000 samples
Epoch 1/1

   32/25000 [..............................] - ETA: 4:26 - loss: 7.6666 - accuracy: 0.5000
   64/25000 [..............................] - ETA: 2:42 - loss: 7.9062 - accuracy: 0.4844
   96/25000 [..............................] - ETA: 2:08 - loss: 8.3055 - accuracy: 0.4583
  128/25000 [..............................] - ETA: 1:52 - loss: 8.3854 - accuracy: 0.4531
  160/25000 [..............................] - ETA: 1:39 - loss: 8.0500 - accuracy: 0.4750
  192/25000 [..............................] - ETA: 1:31 - loss: 8.2256 - accuracy: 0.4635
  224/25000 [..............................] - ETA: 1:26 - loss: 7.5982 - accuracy: 0.5045
  256/25000 [..............................] - ETA: 1:21 - loss: 7.3671 - accuracy: 0.5195
  288/25000 [..............................] - ETA: 1:19 - loss: 7.4004 - accuracy: 0.5174
  320/25000 [..............................] - ETA: 1:17 - loss: 7.6187 - accuracy: 0.5031
  352/25000 [..............................] - ETA: 1:15 - loss: 7.6666 - accuracy: 0.5000
  384/25000 [..............................] - ETA: 1:13 - loss: 7.6666 - accuracy: 0.5000
  416/25000 [..............................] - ETA: 1:12 - loss: 7.5929 - accuracy: 0.5048
  448/25000 [..............................] - ETA: 1:11 - loss: 7.6666 - accuracy: 0.5000
  480/25000 [..............................] - ETA: 1:10 - loss: 7.6666 - accuracy: 0.5000
  512/25000 [..............................] - ETA: 1:09 - loss: 7.6966 - accuracy: 0.4980
  544/25000 [..............................] - ETA: 1:08 - loss: 7.8075 - accuracy: 0.4908
  576/25000 [..............................] - ETA: 1:08 - loss: 7.7465 - accuracy: 0.4948
  608/25000 [..............................] - ETA: 1:07 - loss: 7.6666 - accuracy: 0.5000
  640/25000 [..............................] - ETA: 1:06 - loss: 7.7145 - accuracy: 0.4969
  672/25000 [..............................] - ETA: 1:06 - loss: 7.8035 - accuracy: 0.4911
  704/25000 [..............................] - ETA: 1:06 - loss: 7.9715 - accuracy: 0.4801
  736/25000 [..............................] - ETA: 1:06 - loss: 7.9375 - accuracy: 0.4823
  768/25000 [..............................] - ETA: 1:05 - loss: 7.9661 - accuracy: 0.4805
  800/25000 [..............................] - ETA: 1:05 - loss: 7.9158 - accuracy: 0.4837
  832/25000 [..............................] - ETA: 1:05 - loss: 7.8693 - accuracy: 0.4868
  864/25000 [>.............................] - ETA: 1:04 - loss: 7.9683 - accuracy: 0.4803
  896/25000 [>.............................] - ETA: 1:04 - loss: 7.9918 - accuracy: 0.4788
  928/25000 [>.............................] - ETA: 1:03 - loss: 8.0466 - accuracy: 0.4752
  960/25000 [>.............................] - ETA: 1:03 - loss: 8.0819 - accuracy: 0.4729
  992/25000 [>.............................] - ETA: 1:02 - loss: 8.0067 - accuracy: 0.4778
 1024/25000 [>.............................] - ETA: 1:02 - loss: 7.9661 - accuracy: 0.4805
 1056/25000 [>.............................] - ETA: 1:02 - loss: 7.9280 - accuracy: 0.4830
 1088/25000 [>.............................] - ETA: 1:02 - loss: 7.9344 - accuracy: 0.4825
 1120/25000 [>.............................] - ETA: 1:01 - loss: 7.8857 - accuracy: 0.4857
 1152/25000 [>.............................] - ETA: 1:01 - loss: 7.9328 - accuracy: 0.4826
 1184/25000 [>.............................] - ETA: 1:01 - loss: 7.9904 - accuracy: 0.4789
 1216/25000 [>.............................] - ETA: 1:00 - loss: 7.9819 - accuracy: 0.4794
 1248/25000 [>.............................] - ETA: 1:00 - loss: 7.9001 - accuracy: 0.4848
 1280/25000 [>.............................] - ETA: 1:00 - loss: 7.8822 - accuracy: 0.4859
 1312/25000 [>.............................] - ETA: 59s - loss: 7.9588 - accuracy: 0.4809 
 1344/25000 [>.............................] - ETA: 59s - loss: 7.9518 - accuracy: 0.4814
 1376/25000 [>.............................] - ETA: 59s - loss: 7.9341 - accuracy: 0.4826
 1408/25000 [>.............................] - ETA: 59s - loss: 7.9280 - accuracy: 0.4830
 1440/25000 [>.............................] - ETA: 59s - loss: 7.8902 - accuracy: 0.4854
 1472/25000 [>.............................] - ETA: 59s - loss: 7.8645 - accuracy: 0.4871
 1504/25000 [>.............................] - ETA: 58s - loss: 7.8603 - accuracy: 0.4874
 1536/25000 [>.............................] - ETA: 58s - loss: 7.8663 - accuracy: 0.4870
 1568/25000 [>.............................] - ETA: 58s - loss: 7.8818 - accuracy: 0.4860
 1600/25000 [>.............................] - ETA: 58s - loss: 7.9062 - accuracy: 0.4844
 1632/25000 [>.............................] - ETA: 58s - loss: 7.9015 - accuracy: 0.4847
 1664/25000 [>.............................] - ETA: 58s - loss: 7.9246 - accuracy: 0.4832
 1696/25000 [=>............................] - ETA: 58s - loss: 7.9288 - accuracy: 0.4829
 1728/25000 [=>............................] - ETA: 57s - loss: 7.9506 - accuracy: 0.4815
 1760/25000 [=>............................] - ETA: 57s - loss: 7.9454 - accuracy: 0.4818
 1792/25000 [=>............................] - ETA: 57s - loss: 7.9832 - accuracy: 0.4794
 1824/25000 [=>............................] - ETA: 57s - loss: 7.9608 - accuracy: 0.4808
 1856/25000 [=>............................] - ETA: 57s - loss: 7.9558 - accuracy: 0.4811
 1888/25000 [=>............................] - ETA: 57s - loss: 7.9590 - accuracy: 0.4809
 1920/25000 [=>............................] - ETA: 57s - loss: 7.9381 - accuracy: 0.4823
 1952/25000 [=>............................] - ETA: 57s - loss: 7.9415 - accuracy: 0.4821
 1984/25000 [=>............................] - ETA: 56s - loss: 7.9217 - accuracy: 0.4834
 2016/25000 [=>............................] - ETA: 56s - loss: 7.9252 - accuracy: 0.4831
 2048/25000 [=>............................] - ETA: 56s - loss: 7.9436 - accuracy: 0.4819
 2080/25000 [=>............................] - ETA: 56s - loss: 7.9615 - accuracy: 0.4808
 2112/25000 [=>............................] - ETA: 56s - loss: 7.9498 - accuracy: 0.4815
 2144/25000 [=>............................] - ETA: 56s - loss: 7.9026 - accuracy: 0.4846
 2176/25000 [=>............................] - ETA: 56s - loss: 7.9273 - accuracy: 0.4830
 2208/25000 [=>............................] - ETA: 56s - loss: 7.9722 - accuracy: 0.4801
 2240/25000 [=>............................] - ETA: 56s - loss: 7.9678 - accuracy: 0.4804
 2272/25000 [=>............................] - ETA: 56s - loss: 7.9366 - accuracy: 0.4824
 2304/25000 [=>............................] - ETA: 55s - loss: 7.9062 - accuracy: 0.4844
 2336/25000 [=>............................] - ETA: 55s - loss: 7.8964 - accuracy: 0.4850
 2368/25000 [=>............................] - ETA: 55s - loss: 7.8674 - accuracy: 0.4869
 2400/25000 [=>............................] - ETA: 55s - loss: 7.8647 - accuracy: 0.4871
 2432/25000 [=>............................] - ETA: 55s - loss: 7.8305 - accuracy: 0.4893
 2464/25000 [=>............................] - ETA: 55s - loss: 7.8035 - accuracy: 0.4911
 2496/25000 [=>............................] - ETA: 55s - loss: 7.8079 - accuracy: 0.4908
 2528/25000 [==>...........................] - ETA: 55s - loss: 7.7879 - accuracy: 0.4921
 2560/25000 [==>...........................] - ETA: 55s - loss: 7.7864 - accuracy: 0.4922
 2592/25000 [==>...........................] - ETA: 55s - loss: 7.7435 - accuracy: 0.4950
 2624/25000 [==>...........................] - ETA: 55s - loss: 7.7426 - accuracy: 0.4950
 2656/25000 [==>...........................] - ETA: 55s - loss: 7.7301 - accuracy: 0.4959
 2688/25000 [==>...........................] - ETA: 54s - loss: 7.7351 - accuracy: 0.4955
 2720/25000 [==>...........................] - ETA: 54s - loss: 7.7455 - accuracy: 0.4949
 2752/25000 [==>...........................] - ETA: 54s - loss: 7.7279 - accuracy: 0.4960
 2784/25000 [==>...........................] - ETA: 54s - loss: 7.7162 - accuracy: 0.4968
 2816/25000 [==>...........................] - ETA: 54s - loss: 7.7211 - accuracy: 0.4964
 2848/25000 [==>...........................] - ETA: 54s - loss: 7.7151 - accuracy: 0.4968
 2880/25000 [==>...........................] - ETA: 54s - loss: 7.7358 - accuracy: 0.4955
 2912/25000 [==>...........................] - ETA: 54s - loss: 7.7245 - accuracy: 0.4962
 2944/25000 [==>...........................] - ETA: 54s - loss: 7.7083 - accuracy: 0.4973
 2976/25000 [==>...........................] - ETA: 54s - loss: 7.7130 - accuracy: 0.4970
 3008/25000 [==>...........................] - ETA: 53s - loss: 7.7125 - accuracy: 0.4970
 3040/25000 [==>...........................] - ETA: 53s - loss: 7.7120 - accuracy: 0.4970
 3072/25000 [==>...........................] - ETA: 53s - loss: 7.7115 - accuracy: 0.4971
 3104/25000 [==>...........................] - ETA: 53s - loss: 7.7061 - accuracy: 0.4974
 3136/25000 [==>...........................] - ETA: 53s - loss: 7.6960 - accuracy: 0.4981
 3168/25000 [==>...........................] - ETA: 53s - loss: 7.7150 - accuracy: 0.4968
 3200/25000 [==>...........................] - ETA: 53s - loss: 7.7002 - accuracy: 0.4978
 3232/25000 [==>...........................] - ETA: 53s - loss: 7.6998 - accuracy: 0.4978
 3264/25000 [==>...........................] - ETA: 53s - loss: 7.7042 - accuracy: 0.4975
 3296/25000 [==>...........................] - ETA: 53s - loss: 7.7224 - accuracy: 0.4964
 3328/25000 [==>...........................] - ETA: 53s - loss: 7.7311 - accuracy: 0.4958
 3360/25000 [===>..........................] - ETA: 52s - loss: 7.7077 - accuracy: 0.4973
 3392/25000 [===>..........................] - ETA: 52s - loss: 7.7389 - accuracy: 0.4953
 3424/25000 [===>..........................] - ETA: 52s - loss: 7.7383 - accuracy: 0.4953
 3456/25000 [===>..........................] - ETA: 52s - loss: 7.7465 - accuracy: 0.4948
 3488/25000 [===>..........................] - ETA: 52s - loss: 7.7633 - accuracy: 0.4937
 3520/25000 [===>..........................] - ETA: 52s - loss: 7.7712 - accuracy: 0.4932
 3552/25000 [===>..........................] - ETA: 52s - loss: 7.7659 - accuracy: 0.4935
 3584/25000 [===>..........................] - ETA: 52s - loss: 7.7693 - accuracy: 0.4933
 3616/25000 [===>..........................] - ETA: 52s - loss: 7.7684 - accuracy: 0.4934
 3648/25000 [===>..........................] - ETA: 52s - loss: 7.7717 - accuracy: 0.4931
 3680/25000 [===>..........................] - ETA: 52s - loss: 7.7708 - accuracy: 0.4932
 3712/25000 [===>..........................] - ETA: 51s - loss: 7.7740 - accuracy: 0.4930
 3744/25000 [===>..........................] - ETA: 51s - loss: 7.7731 - accuracy: 0.4931
 3776/25000 [===>..........................] - ETA: 51s - loss: 7.7844 - accuracy: 0.4923
 3808/25000 [===>..........................] - ETA: 51s - loss: 7.7713 - accuracy: 0.4932
 3840/25000 [===>..........................] - ETA: 51s - loss: 7.7784 - accuracy: 0.4927
 3872/25000 [===>..........................] - ETA: 51s - loss: 7.7735 - accuracy: 0.4930
 3904/25000 [===>..........................] - ETA: 51s - loss: 7.7530 - accuracy: 0.4944
 3936/25000 [===>..........................] - ETA: 51s - loss: 7.7718 - accuracy: 0.4931
 3968/25000 [===>..........................] - ETA: 51s - loss: 7.7787 - accuracy: 0.4927
 4000/25000 [===>..........................] - ETA: 51s - loss: 7.7855 - accuracy: 0.4922
 4032/25000 [===>..........................] - ETA: 51s - loss: 7.7807 - accuracy: 0.4926
 4064/25000 [===>..........................] - ETA: 51s - loss: 7.7760 - accuracy: 0.4929
 4096/25000 [===>..........................] - ETA: 51s - loss: 7.7677 - accuracy: 0.4934
 4128/25000 [===>..........................] - ETA: 50s - loss: 7.7632 - accuracy: 0.4937
 4160/25000 [===>..........................] - ETA: 50s - loss: 7.7735 - accuracy: 0.4930
 4192/25000 [====>.........................] - ETA: 50s - loss: 7.7654 - accuracy: 0.4936
 4224/25000 [====>.........................] - ETA: 50s - loss: 7.7646 - accuracy: 0.4936
 4256/25000 [====>.........................] - ETA: 50s - loss: 7.7675 - accuracy: 0.4934
 4288/25000 [====>.........................] - ETA: 50s - loss: 7.7632 - accuracy: 0.4937
 4320/25000 [====>.........................] - ETA: 50s - loss: 7.7589 - accuracy: 0.4940
 4352/25000 [====>.........................] - ETA: 50s - loss: 7.7653 - accuracy: 0.4936
 4384/25000 [====>.........................] - ETA: 50s - loss: 7.7646 - accuracy: 0.4936
 4416/25000 [====>.........................] - ETA: 50s - loss: 7.7708 - accuracy: 0.4932
 4448/25000 [====>.........................] - ETA: 50s - loss: 7.7631 - accuracy: 0.4937
 4480/25000 [====>.........................] - ETA: 50s - loss: 7.7556 - accuracy: 0.4942
 4512/25000 [====>.........................] - ETA: 50s - loss: 7.7584 - accuracy: 0.4940
 4544/25000 [====>.........................] - ETA: 49s - loss: 7.7679 - accuracy: 0.4934
 4576/25000 [====>.........................] - ETA: 49s - loss: 7.7738 - accuracy: 0.4930
 4608/25000 [====>.........................] - ETA: 49s - loss: 7.7764 - accuracy: 0.4928
 4640/25000 [====>.........................] - ETA: 49s - loss: 7.7724 - accuracy: 0.4931
 4672/25000 [====>.........................] - ETA: 49s - loss: 7.7848 - accuracy: 0.4923
 4704/25000 [====>.........................] - ETA: 49s - loss: 7.7840 - accuracy: 0.4923
 4736/25000 [====>.........................] - ETA: 49s - loss: 7.7832 - accuracy: 0.4924
 4768/25000 [====>.........................] - ETA: 49s - loss: 7.7920 - accuracy: 0.4918
 4800/25000 [====>.........................] - ETA: 49s - loss: 7.7976 - accuracy: 0.4915
 4832/25000 [====>.........................] - ETA: 49s - loss: 7.7840 - accuracy: 0.4923
 4864/25000 [====>.........................] - ETA: 49s - loss: 7.7959 - accuracy: 0.4916
 4896/25000 [====>.........................] - ETA: 48s - loss: 7.7825 - accuracy: 0.4924
 4928/25000 [====>.........................] - ETA: 48s - loss: 7.7755 - accuracy: 0.4929
 4960/25000 [====>.........................] - ETA: 48s - loss: 7.7655 - accuracy: 0.4935
 4992/25000 [====>.........................] - ETA: 48s - loss: 7.7649 - accuracy: 0.4936
 5024/25000 [=====>........................] - ETA: 48s - loss: 7.7582 - accuracy: 0.4940
 5056/25000 [=====>........................] - ETA: 48s - loss: 7.7546 - accuracy: 0.4943
 5088/25000 [=====>........................] - ETA: 48s - loss: 7.7480 - accuracy: 0.4947
 5120/25000 [=====>........................] - ETA: 48s - loss: 7.7445 - accuracy: 0.4949
 5152/25000 [=====>........................] - ETA: 48s - loss: 7.7410 - accuracy: 0.4951
 5184/25000 [=====>........................] - ETA: 48s - loss: 7.7317 - accuracy: 0.4958
 5216/25000 [=====>........................] - ETA: 48s - loss: 7.7254 - accuracy: 0.4962
 5248/25000 [=====>........................] - ETA: 48s - loss: 7.7104 - accuracy: 0.4971
 5280/25000 [=====>........................] - ETA: 48s - loss: 7.6957 - accuracy: 0.4981
 5312/25000 [=====>........................] - ETA: 47s - loss: 7.7041 - accuracy: 0.4976
 5344/25000 [=====>........................] - ETA: 47s - loss: 7.7125 - accuracy: 0.4970
 5376/25000 [=====>........................] - ETA: 47s - loss: 7.7294 - accuracy: 0.4959
 5408/25000 [=====>........................] - ETA: 47s - loss: 7.7290 - accuracy: 0.4959
 5440/25000 [=====>........................] - ETA: 47s - loss: 7.7286 - accuracy: 0.4960
 5472/25000 [=====>........................] - ETA: 47s - loss: 7.7339 - accuracy: 0.4956
 5504/25000 [=====>........................] - ETA: 47s - loss: 7.7335 - accuracy: 0.4956
 5536/25000 [=====>........................] - ETA: 47s - loss: 7.7331 - accuracy: 0.4957
 5568/25000 [=====>........................] - ETA: 47s - loss: 7.7355 - accuracy: 0.4955
 5600/25000 [=====>........................] - ETA: 47s - loss: 7.7323 - accuracy: 0.4957
 5632/25000 [=====>........................] - ETA: 47s - loss: 7.7129 - accuracy: 0.4970
 5664/25000 [=====>........................] - ETA: 47s - loss: 7.7099 - accuracy: 0.4972
 5696/25000 [=====>........................] - ETA: 46s - loss: 7.7151 - accuracy: 0.4968
 5728/25000 [=====>........................] - ETA: 46s - loss: 7.7175 - accuracy: 0.4967
 5760/25000 [=====>........................] - ETA: 46s - loss: 7.7172 - accuracy: 0.4967
 5792/25000 [=====>........................] - ETA: 46s - loss: 7.7196 - accuracy: 0.4965
 5824/25000 [=====>........................] - ETA: 46s - loss: 7.7140 - accuracy: 0.4969
 5856/25000 [======>.......................] - ETA: 46s - loss: 7.7242 - accuracy: 0.4962
 5888/25000 [======>.......................] - ETA: 46s - loss: 7.7161 - accuracy: 0.4968
 5920/25000 [======>.......................] - ETA: 46s - loss: 7.7236 - accuracy: 0.4963
 5952/25000 [======>.......................] - ETA: 46s - loss: 7.7233 - accuracy: 0.4963
 5984/25000 [======>.......................] - ETA: 46s - loss: 7.7204 - accuracy: 0.4965
 6016/25000 [======>.......................] - ETA: 46s - loss: 7.7278 - accuracy: 0.4960
 6048/25000 [======>.......................] - ETA: 46s - loss: 7.7173 - accuracy: 0.4967
 6080/25000 [======>.......................] - ETA: 46s - loss: 7.7120 - accuracy: 0.4970
 6112/25000 [======>.......................] - ETA: 46s - loss: 7.7118 - accuracy: 0.4971
 6144/25000 [======>.......................] - ETA: 45s - loss: 7.7115 - accuracy: 0.4971
 6176/25000 [======>.......................] - ETA: 45s - loss: 7.7138 - accuracy: 0.4969
 6208/25000 [======>.......................] - ETA: 45s - loss: 7.7037 - accuracy: 0.4976
 6240/25000 [======>.......................] - ETA: 45s - loss: 7.7059 - accuracy: 0.4974
 6272/25000 [======>.......................] - ETA: 45s - loss: 7.7008 - accuracy: 0.4978
 6304/25000 [======>.......................] - ETA: 45s - loss: 7.7104 - accuracy: 0.4971
 6336/25000 [======>.......................] - ETA: 45s - loss: 7.7053 - accuracy: 0.4975
 6368/25000 [======>.......................] - ETA: 45s - loss: 7.7076 - accuracy: 0.4973
 6400/25000 [======>.......................] - ETA: 45s - loss: 7.7145 - accuracy: 0.4969
 6432/25000 [======>.......................] - ETA: 45s - loss: 7.7286 - accuracy: 0.4960
 6464/25000 [======>.......................] - ETA: 45s - loss: 7.7212 - accuracy: 0.4964
 6496/25000 [======>.......................] - ETA: 45s - loss: 7.7185 - accuracy: 0.4966
 6528/25000 [======>.......................] - ETA: 45s - loss: 7.7183 - accuracy: 0.4966
 6560/25000 [======>.......................] - ETA: 44s - loss: 7.7157 - accuracy: 0.4968
 6592/25000 [======>.......................] - ETA: 44s - loss: 7.7155 - accuracy: 0.4968
 6624/25000 [======>.......................] - ETA: 44s - loss: 7.7175 - accuracy: 0.4967
 6656/25000 [======>.......................] - ETA: 44s - loss: 7.7104 - accuracy: 0.4971
 6688/25000 [=======>......................] - ETA: 44s - loss: 7.7125 - accuracy: 0.4970
 6720/25000 [=======>......................] - ETA: 44s - loss: 7.7191 - accuracy: 0.4966
 6752/25000 [=======>......................] - ETA: 44s - loss: 7.7120 - accuracy: 0.4970
 6784/25000 [=======>......................] - ETA: 44s - loss: 7.7186 - accuracy: 0.4966
 6816/25000 [=======>......................] - ETA: 44s - loss: 7.7094 - accuracy: 0.4972
 6848/25000 [=======>......................] - ETA: 44s - loss: 7.7181 - accuracy: 0.4966
 6880/25000 [=======>......................] - ETA: 44s - loss: 7.7090 - accuracy: 0.4972
 6912/25000 [=======>......................] - ETA: 44s - loss: 7.7065 - accuracy: 0.4974
 6944/25000 [=======>......................] - ETA: 44s - loss: 7.7042 - accuracy: 0.4976
 6976/25000 [=======>......................] - ETA: 44s - loss: 7.7018 - accuracy: 0.4977
 7008/25000 [=======>......................] - ETA: 43s - loss: 7.6951 - accuracy: 0.4981
 7040/25000 [=======>......................] - ETA: 43s - loss: 7.6993 - accuracy: 0.4979
 7072/25000 [=======>......................] - ETA: 43s - loss: 7.7100 - accuracy: 0.4972
 7104/25000 [=======>......................] - ETA: 43s - loss: 7.7163 - accuracy: 0.4968
 7136/25000 [=======>......................] - ETA: 43s - loss: 7.7139 - accuracy: 0.4969
 7168/25000 [=======>......................] - ETA: 43s - loss: 7.7158 - accuracy: 0.4968
 7200/25000 [=======>......................] - ETA: 43s - loss: 7.7156 - accuracy: 0.4968
 7232/25000 [=======>......................] - ETA: 43s - loss: 7.7111 - accuracy: 0.4971
 7264/25000 [=======>......................] - ETA: 43s - loss: 7.7046 - accuracy: 0.4975
 7296/25000 [=======>......................] - ETA: 43s - loss: 7.7065 - accuracy: 0.4974
 7328/25000 [=======>......................] - ETA: 43s - loss: 7.7147 - accuracy: 0.4969
 7360/25000 [=======>......................] - ETA: 43s - loss: 7.7104 - accuracy: 0.4971
 7392/25000 [=======>......................] - ETA: 42s - loss: 7.7019 - accuracy: 0.4977
 7424/25000 [=======>......................] - ETA: 42s - loss: 7.6976 - accuracy: 0.4980
 7456/25000 [=======>......................] - ETA: 42s - loss: 7.6872 - accuracy: 0.4987
 7488/25000 [=======>......................] - ETA: 42s - loss: 7.6871 - accuracy: 0.4987
 7520/25000 [========>.....................] - ETA: 42s - loss: 7.6931 - accuracy: 0.4983
 7552/25000 [========>.....................] - ETA: 42s - loss: 7.6950 - accuracy: 0.4981
 7584/25000 [========>.....................] - ETA: 42s - loss: 7.6889 - accuracy: 0.4985
 7616/25000 [========>.....................] - ETA: 42s - loss: 7.6928 - accuracy: 0.4983
 7648/25000 [========>.....................] - ETA: 42s - loss: 7.7007 - accuracy: 0.4978
 7680/25000 [========>.....................] - ETA: 42s - loss: 7.7046 - accuracy: 0.4975
 7712/25000 [========>.....................] - ETA: 42s - loss: 7.7104 - accuracy: 0.4971
 7744/25000 [========>.....................] - ETA: 42s - loss: 7.7082 - accuracy: 0.4973
 7776/25000 [========>.....................] - ETA: 42s - loss: 7.7120 - accuracy: 0.4970
 7808/25000 [========>.....................] - ETA: 41s - loss: 7.7098 - accuracy: 0.4972
 7840/25000 [========>.....................] - ETA: 41s - loss: 7.7136 - accuracy: 0.4969
 7872/25000 [========>.....................] - ETA: 41s - loss: 7.7173 - accuracy: 0.4967
 7904/25000 [========>.....................] - ETA: 41s - loss: 7.7112 - accuracy: 0.4971
 7936/25000 [========>.....................] - ETA: 41s - loss: 7.7169 - accuracy: 0.4967
 7968/25000 [========>.....................] - ETA: 41s - loss: 7.7128 - accuracy: 0.4970
 8000/25000 [========>.....................] - ETA: 41s - loss: 7.7165 - accuracy: 0.4967
 8032/25000 [========>.....................] - ETA: 41s - loss: 7.7220 - accuracy: 0.4964
 8064/25000 [========>.....................] - ETA: 41s - loss: 7.7275 - accuracy: 0.4960
 8096/25000 [========>.....................] - ETA: 41s - loss: 7.7329 - accuracy: 0.4957
 8128/25000 [========>.....................] - ETA: 41s - loss: 7.7402 - accuracy: 0.4952
 8160/25000 [========>.....................] - ETA: 41s - loss: 7.7324 - accuracy: 0.4957
 8192/25000 [========>.....................] - ETA: 41s - loss: 7.7415 - accuracy: 0.4951
 8224/25000 [========>.....................] - ETA: 40s - loss: 7.7412 - accuracy: 0.4951
 8256/25000 [========>.....................] - ETA: 40s - loss: 7.7446 - accuracy: 0.4949
 8288/25000 [========>.....................] - ETA: 40s - loss: 7.7406 - accuracy: 0.4952
 8320/25000 [========>.....................] - ETA: 40s - loss: 7.7385 - accuracy: 0.4953
 8352/25000 [=========>....................] - ETA: 40s - loss: 7.7456 - accuracy: 0.4949
 8384/25000 [=========>....................] - ETA: 40s - loss: 7.7416 - accuracy: 0.4951
 8416/25000 [=========>....................] - ETA: 40s - loss: 7.7413 - accuracy: 0.4951
 8448/25000 [=========>....................] - ETA: 40s - loss: 7.7356 - accuracy: 0.4955
 8480/25000 [=========>....................] - ETA: 40s - loss: 7.7426 - accuracy: 0.4950
 8512/25000 [=========>....................] - ETA: 40s - loss: 7.7441 - accuracy: 0.4949
 8544/25000 [=========>....................] - ETA: 40s - loss: 7.7348 - accuracy: 0.4956
 8576/25000 [=========>....................] - ETA: 40s - loss: 7.7346 - accuracy: 0.4956
 8608/25000 [=========>....................] - ETA: 40s - loss: 7.7307 - accuracy: 0.4958
 8640/25000 [=========>....................] - ETA: 39s - loss: 7.7394 - accuracy: 0.4953
 8672/25000 [=========>....................] - ETA: 39s - loss: 7.7426 - accuracy: 0.4950
 8704/25000 [=========>....................] - ETA: 39s - loss: 7.7477 - accuracy: 0.4947
 8736/25000 [=========>....................] - ETA: 39s - loss: 7.7456 - accuracy: 0.4948
 8768/25000 [=========>....................] - ETA: 39s - loss: 7.7418 - accuracy: 0.4951
 8800/25000 [=========>....................] - ETA: 39s - loss: 7.7363 - accuracy: 0.4955
 8832/25000 [=========>....................] - ETA: 39s - loss: 7.7378 - accuracy: 0.4954
 8864/25000 [=========>....................] - ETA: 39s - loss: 7.7445 - accuracy: 0.4949
 8896/25000 [=========>....................] - ETA: 39s - loss: 7.7373 - accuracy: 0.4954
 8928/25000 [=========>....................] - ETA: 39s - loss: 7.7439 - accuracy: 0.4950
 8960/25000 [=========>....................] - ETA: 39s - loss: 7.7436 - accuracy: 0.4950
 8992/25000 [=========>....................] - ETA: 39s - loss: 7.7416 - accuracy: 0.4951
 9024/25000 [=========>....................] - ETA: 39s - loss: 7.7397 - accuracy: 0.4952
 9056/25000 [=========>....................] - ETA: 39s - loss: 7.7428 - accuracy: 0.4950
 9088/25000 [=========>....................] - ETA: 38s - loss: 7.7341 - accuracy: 0.4956
 9120/25000 [=========>....................] - ETA: 38s - loss: 7.7204 - accuracy: 0.4965
 9152/25000 [=========>....................] - ETA: 38s - loss: 7.7152 - accuracy: 0.4968
 9184/25000 [==========>...................] - ETA: 38s - loss: 7.7084 - accuracy: 0.4973
 9216/25000 [==========>...................] - ETA: 38s - loss: 7.7082 - accuracy: 0.4973
 9248/25000 [==========>...................] - ETA: 38s - loss: 7.7064 - accuracy: 0.4974
 9280/25000 [==========>...................] - ETA: 38s - loss: 7.6997 - accuracy: 0.4978
 9312/25000 [==========>...................] - ETA: 38s - loss: 7.6946 - accuracy: 0.4982
 9344/25000 [==========>...................] - ETA: 38s - loss: 7.6962 - accuracy: 0.4981
 9376/25000 [==========>...................] - ETA: 38s - loss: 7.6912 - accuracy: 0.4984
 9408/25000 [==========>...................] - ETA: 38s - loss: 7.6894 - accuracy: 0.4985
 9440/25000 [==========>...................] - ETA: 38s - loss: 7.6975 - accuracy: 0.4980
 9472/25000 [==========>...................] - ETA: 38s - loss: 7.7039 - accuracy: 0.4976
 9504/25000 [==========>...................] - ETA: 37s - loss: 7.6973 - accuracy: 0.4980
 9536/25000 [==========>...................] - ETA: 37s - loss: 7.7004 - accuracy: 0.4978
 9568/25000 [==========>...................] - ETA: 37s - loss: 7.6955 - accuracy: 0.4981
 9600/25000 [==========>...................] - ETA: 37s - loss: 7.6922 - accuracy: 0.4983
 9632/25000 [==========>...................] - ETA: 37s - loss: 7.6969 - accuracy: 0.4980
 9664/25000 [==========>...................] - ETA: 37s - loss: 7.6952 - accuracy: 0.4981
 9696/25000 [==========>...................] - ETA: 37s - loss: 7.6935 - accuracy: 0.4982
 9728/25000 [==========>...................] - ETA: 37s - loss: 7.6887 - accuracy: 0.4986
 9760/25000 [==========>...................] - ETA: 37s - loss: 7.6949 - accuracy: 0.4982
 9792/25000 [==========>...................] - ETA: 37s - loss: 7.6901 - accuracy: 0.4985
 9824/25000 [==========>...................] - ETA: 37s - loss: 7.6885 - accuracy: 0.4986
 9856/25000 [==========>...................] - ETA: 37s - loss: 7.6977 - accuracy: 0.4980
 9888/25000 [==========>...................] - ETA: 36s - loss: 7.7007 - accuracy: 0.4978
 9920/25000 [==========>...................] - ETA: 36s - loss: 7.7099 - accuracy: 0.4972
 9952/25000 [==========>...................] - ETA: 36s - loss: 7.7067 - accuracy: 0.4974
 9984/25000 [==========>...................] - ETA: 36s - loss: 7.7142 - accuracy: 0.4969
10016/25000 [===========>..................] - ETA: 36s - loss: 7.7110 - accuracy: 0.4971
10048/25000 [===========>..................] - ETA: 36s - loss: 7.7063 - accuracy: 0.4974
10080/25000 [===========>..................] - ETA: 36s - loss: 7.7123 - accuracy: 0.4970
10112/25000 [===========>..................] - ETA: 36s - loss: 7.7167 - accuracy: 0.4967
10144/25000 [===========>..................] - ETA: 36s - loss: 7.7089 - accuracy: 0.4972
10176/25000 [===========>..................] - ETA: 36s - loss: 7.7148 - accuracy: 0.4969
10208/25000 [===========>..................] - ETA: 36s - loss: 7.7117 - accuracy: 0.4971
10240/25000 [===========>..................] - ETA: 36s - loss: 7.7130 - accuracy: 0.4970
10272/25000 [===========>..................] - ETA: 36s - loss: 7.7129 - accuracy: 0.4970
10304/25000 [===========>..................] - ETA: 35s - loss: 7.7098 - accuracy: 0.4972
10336/25000 [===========>..................] - ETA: 35s - loss: 7.7052 - accuracy: 0.4975
10368/25000 [===========>..................] - ETA: 35s - loss: 7.7065 - accuracy: 0.4974
10400/25000 [===========>..................] - ETA: 35s - loss: 7.7079 - accuracy: 0.4973
10432/25000 [===========>..................] - ETA: 35s - loss: 7.7107 - accuracy: 0.4971
10464/25000 [===========>..................] - ETA: 35s - loss: 7.7150 - accuracy: 0.4968
10496/25000 [===========>..................] - ETA: 35s - loss: 7.7046 - accuracy: 0.4975
10528/25000 [===========>..................] - ETA: 35s - loss: 7.7045 - accuracy: 0.4975
10560/25000 [===========>..................] - ETA: 35s - loss: 7.7029 - accuracy: 0.4976
10592/25000 [===========>..................] - ETA: 35s - loss: 7.7086 - accuracy: 0.4973
10624/25000 [===========>..................] - ETA: 35s - loss: 7.7157 - accuracy: 0.4968
10656/25000 [===========>..................] - ETA: 35s - loss: 7.7055 - accuracy: 0.4975
10688/25000 [===========>..................] - ETA: 34s - loss: 7.7054 - accuracy: 0.4975
10720/25000 [===========>..................] - ETA: 34s - loss: 7.7052 - accuracy: 0.4975
10752/25000 [===========>..................] - ETA: 34s - loss: 7.7051 - accuracy: 0.4975
10784/25000 [===========>..................] - ETA: 34s - loss: 7.7064 - accuracy: 0.4974
10816/25000 [===========>..................] - ETA: 34s - loss: 7.7134 - accuracy: 0.4969
10848/25000 [============>.................] - ETA: 34s - loss: 7.7090 - accuracy: 0.4972
10880/25000 [============>.................] - ETA: 34s - loss: 7.7047 - accuracy: 0.4975
10912/25000 [============>.................] - ETA: 34s - loss: 7.7060 - accuracy: 0.4974
10944/25000 [============>.................] - ETA: 34s - loss: 7.7129 - accuracy: 0.4970
10976/25000 [============>.................] - ETA: 34s - loss: 7.7141 - accuracy: 0.4969
11008/25000 [============>.................] - ETA: 34s - loss: 7.7154 - accuracy: 0.4968
11040/25000 [============>.................] - ETA: 34s - loss: 7.7180 - accuracy: 0.4966
11072/25000 [============>.................] - ETA: 34s - loss: 7.7206 - accuracy: 0.4965
11104/25000 [============>.................] - ETA: 33s - loss: 7.7232 - accuracy: 0.4963
11136/25000 [============>.................] - ETA: 33s - loss: 7.7231 - accuracy: 0.4963
11168/25000 [============>.................] - ETA: 33s - loss: 7.7202 - accuracy: 0.4965
11200/25000 [============>.................] - ETA: 33s - loss: 7.7214 - accuracy: 0.4964
11232/25000 [============>.................] - ETA: 33s - loss: 7.7281 - accuracy: 0.4960
11264/25000 [============>.................] - ETA: 33s - loss: 7.7320 - accuracy: 0.4957
11296/25000 [============>.................] - ETA: 33s - loss: 7.7358 - accuracy: 0.4955
11328/25000 [============>.................] - ETA: 33s - loss: 7.7370 - accuracy: 0.4954
11360/25000 [============>.................] - ETA: 33s - loss: 7.7395 - accuracy: 0.4952
11392/25000 [============>.................] - ETA: 33s - loss: 7.7393 - accuracy: 0.4953
11424/25000 [============>.................] - ETA: 33s - loss: 7.7364 - accuracy: 0.4954
11456/25000 [============>.................] - ETA: 33s - loss: 7.7349 - accuracy: 0.4955
11488/25000 [============>.................] - ETA: 33s - loss: 7.7334 - accuracy: 0.4956
11520/25000 [============>.................] - ETA: 32s - loss: 7.7398 - accuracy: 0.4952
11552/25000 [============>.................] - ETA: 32s - loss: 7.7396 - accuracy: 0.4952
11584/25000 [============>.................] - ETA: 32s - loss: 7.7447 - accuracy: 0.4949
11616/25000 [============>.................] - ETA: 32s - loss: 7.7419 - accuracy: 0.4951
11648/25000 [============>.................] - ETA: 32s - loss: 7.7443 - accuracy: 0.4949
11680/25000 [=============>................] - ETA: 32s - loss: 7.7428 - accuracy: 0.4950
11712/25000 [=============>................] - ETA: 32s - loss: 7.7386 - accuracy: 0.4953
11744/25000 [=============>................] - ETA: 32s - loss: 7.7358 - accuracy: 0.4955
11776/25000 [=============>................] - ETA: 32s - loss: 7.7356 - accuracy: 0.4955
11808/25000 [=============>................] - ETA: 32s - loss: 7.7341 - accuracy: 0.4956
11840/25000 [=============>................] - ETA: 32s - loss: 7.7366 - accuracy: 0.4954
11872/25000 [=============>................] - ETA: 32s - loss: 7.7338 - accuracy: 0.4956
11904/25000 [=============>................] - ETA: 32s - loss: 7.7336 - accuracy: 0.4956
11936/25000 [=============>................] - ETA: 32s - loss: 7.7373 - accuracy: 0.4954
11968/25000 [=============>................] - ETA: 31s - loss: 7.7409 - accuracy: 0.4952
12000/25000 [=============>................] - ETA: 31s - loss: 7.7497 - accuracy: 0.4946
12032/25000 [=============>................] - ETA: 31s - loss: 7.7533 - accuracy: 0.4943
12064/25000 [=============>................] - ETA: 31s - loss: 7.7543 - accuracy: 0.4943
12096/25000 [=============>................] - ETA: 31s - loss: 7.7541 - accuracy: 0.4943
12128/25000 [=============>................] - ETA: 31s - loss: 7.7513 - accuracy: 0.4945
12160/25000 [=============>................] - ETA: 31s - loss: 7.7511 - accuracy: 0.4945
12192/25000 [=============>................] - ETA: 31s - loss: 7.7484 - accuracy: 0.4947
12224/25000 [=============>................] - ETA: 31s - loss: 7.7431 - accuracy: 0.4950
12256/25000 [=============>................] - ETA: 31s - loss: 7.7379 - accuracy: 0.4953
12288/25000 [=============>................] - ETA: 31s - loss: 7.7365 - accuracy: 0.4954
12320/25000 [=============>................] - ETA: 31s - loss: 7.7313 - accuracy: 0.4958
12352/25000 [=============>................] - ETA: 30s - loss: 7.7324 - accuracy: 0.4957
12384/25000 [=============>................] - ETA: 30s - loss: 7.7310 - accuracy: 0.4958
12416/25000 [=============>................] - ETA: 30s - loss: 7.7321 - accuracy: 0.4957
12448/25000 [=============>................] - ETA: 30s - loss: 7.7270 - accuracy: 0.4961
12480/25000 [=============>................] - ETA: 30s - loss: 7.7256 - accuracy: 0.4962
12512/25000 [==============>...............] - ETA: 30s - loss: 7.7303 - accuracy: 0.4958
12544/25000 [==============>...............] - ETA: 30s - loss: 7.7326 - accuracy: 0.4957
12576/25000 [==============>...............] - ETA: 30s - loss: 7.7337 - accuracy: 0.4956
12608/25000 [==============>...............] - ETA: 30s - loss: 7.7335 - accuracy: 0.4956
12640/25000 [==============>...............] - ETA: 30s - loss: 7.7333 - accuracy: 0.4956
12672/25000 [==============>...............] - ETA: 30s - loss: 7.7392 - accuracy: 0.4953
12704/25000 [==============>...............] - ETA: 30s - loss: 7.7330 - accuracy: 0.4957
12736/25000 [==============>...............] - ETA: 30s - loss: 7.7304 - accuracy: 0.4958
12768/25000 [==============>...............] - ETA: 30s - loss: 7.7303 - accuracy: 0.4958
12800/25000 [==============>...............] - ETA: 29s - loss: 7.7289 - accuracy: 0.4959
12832/25000 [==============>...............] - ETA: 29s - loss: 7.7300 - accuracy: 0.4959
12864/25000 [==============>...............] - ETA: 29s - loss: 7.7262 - accuracy: 0.4961
12896/25000 [==============>...............] - ETA: 29s - loss: 7.7249 - accuracy: 0.4962
12928/25000 [==============>...............] - ETA: 29s - loss: 7.7295 - accuracy: 0.4959
12960/25000 [==============>...............] - ETA: 29s - loss: 7.7293 - accuracy: 0.4959
12992/25000 [==============>...............] - ETA: 29s - loss: 7.7292 - accuracy: 0.4959
13024/25000 [==============>...............] - ETA: 29s - loss: 7.7302 - accuracy: 0.4959
13056/25000 [==============>...............] - ETA: 29s - loss: 7.7324 - accuracy: 0.4957
13088/25000 [==============>...............] - ETA: 29s - loss: 7.7287 - accuracy: 0.4960
13120/25000 [==============>...............] - ETA: 29s - loss: 7.7251 - accuracy: 0.4962
13152/25000 [==============>...............] - ETA: 29s - loss: 7.7202 - accuracy: 0.4965
13184/25000 [==============>...............] - ETA: 28s - loss: 7.7213 - accuracy: 0.4964
13216/25000 [==============>...............] - ETA: 28s - loss: 7.7211 - accuracy: 0.4964
13248/25000 [==============>...............] - ETA: 28s - loss: 7.7164 - accuracy: 0.4968
13280/25000 [==============>...............] - ETA: 28s - loss: 7.7140 - accuracy: 0.4969
13312/25000 [==============>...............] - ETA: 28s - loss: 7.7138 - accuracy: 0.4969
13344/25000 [===============>..............] - ETA: 28s - loss: 7.7149 - accuracy: 0.4969
13376/25000 [===============>..............] - ETA: 28s - loss: 7.7102 - accuracy: 0.4972
13408/25000 [===============>..............] - ETA: 28s - loss: 7.7089 - accuracy: 0.4972
13440/25000 [===============>..............] - ETA: 28s - loss: 7.7100 - accuracy: 0.4972
13472/25000 [===============>..............] - ETA: 28s - loss: 7.7121 - accuracy: 0.4970
13504/25000 [===============>..............] - ETA: 28s - loss: 7.7109 - accuracy: 0.4971
13536/25000 [===============>..............] - ETA: 28s - loss: 7.7165 - accuracy: 0.4967
13568/25000 [===============>..............] - ETA: 28s - loss: 7.7186 - accuracy: 0.4966
13600/25000 [===============>..............] - ETA: 28s - loss: 7.7174 - accuracy: 0.4967
13632/25000 [===============>..............] - ETA: 27s - loss: 7.7161 - accuracy: 0.4968
13664/25000 [===============>..............] - ETA: 27s - loss: 7.7182 - accuracy: 0.4966
13696/25000 [===============>..............] - ETA: 27s - loss: 7.7192 - accuracy: 0.4966
13728/25000 [===============>..............] - ETA: 27s - loss: 7.7202 - accuracy: 0.4965
13760/25000 [===============>..............] - ETA: 27s - loss: 7.7223 - accuracy: 0.4964
13792/25000 [===============>..............] - ETA: 27s - loss: 7.7222 - accuracy: 0.4964
13824/25000 [===============>..............] - ETA: 27s - loss: 7.7254 - accuracy: 0.4962
13856/25000 [===============>..............] - ETA: 27s - loss: 7.7264 - accuracy: 0.4961
13888/25000 [===============>..............] - ETA: 27s - loss: 7.7307 - accuracy: 0.4958
13920/25000 [===============>..............] - ETA: 27s - loss: 7.7283 - accuracy: 0.4960
13952/25000 [===============>..............] - ETA: 27s - loss: 7.7282 - accuracy: 0.4960
13984/25000 [===============>..............] - ETA: 27s - loss: 7.7258 - accuracy: 0.4961
14016/25000 [===============>..............] - ETA: 27s - loss: 7.7235 - accuracy: 0.4963
14048/25000 [===============>..............] - ETA: 27s - loss: 7.7223 - accuracy: 0.4964
14080/25000 [===============>..............] - ETA: 26s - loss: 7.7200 - accuracy: 0.4965
14112/25000 [===============>..............] - ETA: 26s - loss: 7.7188 - accuracy: 0.4966
14144/25000 [===============>..............] - ETA: 26s - loss: 7.7187 - accuracy: 0.4966
14176/25000 [================>.............] - ETA: 26s - loss: 7.7164 - accuracy: 0.4968
14208/25000 [================>.............] - ETA: 26s - loss: 7.7173 - accuracy: 0.4967
14240/25000 [================>.............] - ETA: 26s - loss: 7.7172 - accuracy: 0.4967
14272/25000 [================>.............] - ETA: 26s - loss: 7.7182 - accuracy: 0.4966
14304/25000 [================>.............] - ETA: 26s - loss: 7.7202 - accuracy: 0.4965
14336/25000 [================>.............] - ETA: 26s - loss: 7.7222 - accuracy: 0.4964
14368/25000 [================>.............] - ETA: 26s - loss: 7.7189 - accuracy: 0.4966
14400/25000 [================>.............] - ETA: 26s - loss: 7.7124 - accuracy: 0.4970
14432/25000 [================>.............] - ETA: 26s - loss: 7.7102 - accuracy: 0.4972
14464/25000 [================>.............] - ETA: 26s - loss: 7.7111 - accuracy: 0.4971
14496/25000 [================>.............] - ETA: 25s - loss: 7.7121 - accuracy: 0.4970
14528/25000 [================>.............] - ETA: 25s - loss: 7.7046 - accuracy: 0.4975
14560/25000 [================>.............] - ETA: 25s - loss: 7.7014 - accuracy: 0.4977
14592/25000 [================>.............] - ETA: 25s - loss: 7.7044 - accuracy: 0.4975
14624/25000 [================>.............] - ETA: 25s - loss: 7.7033 - accuracy: 0.4976
14656/25000 [================>.............] - ETA: 25s - loss: 7.7043 - accuracy: 0.4975
14688/25000 [================>.............] - ETA: 25s - loss: 7.7021 - accuracy: 0.4977
14720/25000 [================>.............] - ETA: 25s - loss: 7.7031 - accuracy: 0.4976
14752/25000 [================>.............] - ETA: 25s - loss: 7.7020 - accuracy: 0.4977
14784/25000 [================>.............] - ETA: 25s - loss: 7.6957 - accuracy: 0.4981
14816/25000 [================>.............] - ETA: 25s - loss: 7.6977 - accuracy: 0.4980
14848/25000 [================>.............] - ETA: 25s - loss: 7.7007 - accuracy: 0.4978
14880/25000 [================>.............] - ETA: 25s - loss: 7.7058 - accuracy: 0.4974
14912/25000 [================>.............] - ETA: 24s - loss: 7.7067 - accuracy: 0.4974
14944/25000 [================>.............] - ETA: 24s - loss: 7.7046 - accuracy: 0.4975
14976/25000 [================>.............] - ETA: 24s - loss: 7.7065 - accuracy: 0.4974
15008/25000 [=================>............] - ETA: 24s - loss: 7.7054 - accuracy: 0.4975
15040/25000 [=================>............] - ETA: 24s - loss: 7.7033 - accuracy: 0.4976
15072/25000 [=================>............] - ETA: 24s - loss: 7.7043 - accuracy: 0.4975
15104/25000 [=================>............] - ETA: 24s - loss: 7.7072 - accuracy: 0.4974
15136/25000 [=================>............] - ETA: 24s - loss: 7.7051 - accuracy: 0.4975
15168/25000 [=================>............] - ETA: 24s - loss: 7.7091 - accuracy: 0.4972
15200/25000 [=================>............] - ETA: 24s - loss: 7.7110 - accuracy: 0.4971
15232/25000 [=================>............] - ETA: 24s - loss: 7.7099 - accuracy: 0.4972
15264/25000 [=================>............] - ETA: 24s - loss: 7.7098 - accuracy: 0.4972
15296/25000 [=================>............] - ETA: 24s - loss: 7.7107 - accuracy: 0.4971
15328/25000 [=================>............] - ETA: 24s - loss: 7.7076 - accuracy: 0.4973
15360/25000 [=================>............] - ETA: 23s - loss: 7.7046 - accuracy: 0.4975
15392/25000 [=================>............] - ETA: 23s - loss: 7.7065 - accuracy: 0.4974
15424/25000 [=================>............] - ETA: 23s - loss: 7.7074 - accuracy: 0.4973
15456/25000 [=================>............] - ETA: 23s - loss: 7.7103 - accuracy: 0.4972
15488/25000 [=================>............] - ETA: 23s - loss: 7.7052 - accuracy: 0.4975
15520/25000 [=================>............] - ETA: 23s - loss: 7.7022 - accuracy: 0.4977
15552/25000 [=================>............] - ETA: 23s - loss: 7.7061 - accuracy: 0.4974
15584/25000 [=================>............] - ETA: 23s - loss: 7.7030 - accuracy: 0.4976
15616/25000 [=================>............] - ETA: 23s - loss: 7.7039 - accuracy: 0.4976
15648/25000 [=================>............] - ETA: 23s - loss: 7.7058 - accuracy: 0.4974
15680/25000 [=================>............] - ETA: 23s - loss: 7.7067 - accuracy: 0.4974
15712/25000 [=================>............] - ETA: 23s - loss: 7.7066 - accuracy: 0.4974
15744/25000 [=================>............] - ETA: 22s - loss: 7.7007 - accuracy: 0.4978
15776/25000 [=================>............] - ETA: 22s - loss: 7.6997 - accuracy: 0.4978
15808/25000 [=================>............] - ETA: 22s - loss: 7.6986 - accuracy: 0.4979
15840/25000 [==================>...........] - ETA: 22s - loss: 7.6976 - accuracy: 0.4980
15872/25000 [==================>...........] - ETA: 22s - loss: 7.6966 - accuracy: 0.4980
15904/25000 [==================>...........] - ETA: 22s - loss: 7.6907 - accuracy: 0.4984
15936/25000 [==================>...........] - ETA: 22s - loss: 7.6887 - accuracy: 0.4986
15968/25000 [==================>...........] - ETA: 22s - loss: 7.6877 - accuracy: 0.4986
16000/25000 [==================>...........] - ETA: 22s - loss: 7.6915 - accuracy: 0.4984
16032/25000 [==================>...........] - ETA: 22s - loss: 7.6896 - accuracy: 0.4985
16064/25000 [==================>...........] - ETA: 22s - loss: 7.6905 - accuracy: 0.4984
16096/25000 [==================>...........] - ETA: 22s - loss: 7.6914 - accuracy: 0.4984
16128/25000 [==================>...........] - ETA: 22s - loss: 7.6913 - accuracy: 0.4984
16160/25000 [==================>...........] - ETA: 21s - loss: 7.6913 - accuracy: 0.4984
16192/25000 [==================>...........] - ETA: 21s - loss: 7.6922 - accuracy: 0.4983
16224/25000 [==================>...........] - ETA: 21s - loss: 7.6931 - accuracy: 0.4983
16256/25000 [==================>...........] - ETA: 21s - loss: 7.6949 - accuracy: 0.4982
16288/25000 [==================>...........] - ETA: 21s - loss: 7.6911 - accuracy: 0.4984
16320/25000 [==================>...........] - ETA: 21s - loss: 7.6929 - accuracy: 0.4983
16352/25000 [==================>...........] - ETA: 21s - loss: 7.6966 - accuracy: 0.4980
16384/25000 [==================>...........] - ETA: 21s - loss: 7.6966 - accuracy: 0.4980
16416/25000 [==================>...........] - ETA: 21s - loss: 7.6993 - accuracy: 0.4979
16448/25000 [==================>...........] - ETA: 21s - loss: 7.7011 - accuracy: 0.4978
16480/25000 [==================>...........] - ETA: 21s - loss: 7.6992 - accuracy: 0.4979
16512/25000 [==================>...........] - ETA: 21s - loss: 7.7038 - accuracy: 0.4976
16544/25000 [==================>...........] - ETA: 21s - loss: 7.7065 - accuracy: 0.4974
16576/25000 [==================>...........] - ETA: 20s - loss: 7.7082 - accuracy: 0.4973
16608/25000 [==================>...........] - ETA: 20s - loss: 7.7054 - accuracy: 0.4975
16640/25000 [==================>...........] - ETA: 20s - loss: 7.7053 - accuracy: 0.4975
16672/25000 [===================>..........] - ETA: 20s - loss: 7.7034 - accuracy: 0.4976
16704/25000 [===================>..........] - ETA: 20s - loss: 7.7033 - accuracy: 0.4976
16736/25000 [===================>..........] - ETA: 20s - loss: 7.7033 - accuracy: 0.4976
16768/25000 [===================>..........] - ETA: 20s - loss: 7.7032 - accuracy: 0.4976
16800/25000 [===================>..........] - ETA: 20s - loss: 7.7013 - accuracy: 0.4977
16832/25000 [===================>..........] - ETA: 20s - loss: 7.7031 - accuracy: 0.4976
16864/25000 [===================>..........] - ETA: 20s - loss: 7.7021 - accuracy: 0.4977
16896/25000 [===================>..........] - ETA: 20s - loss: 7.7020 - accuracy: 0.4977
16928/25000 [===================>..........] - ETA: 20s - loss: 7.7001 - accuracy: 0.4978
16960/25000 [===================>..........] - ETA: 20s - loss: 7.7001 - accuracy: 0.4978
16992/25000 [===================>..........] - ETA: 19s - loss: 7.7027 - accuracy: 0.4976
17024/25000 [===================>..........] - ETA: 19s - loss: 7.7053 - accuracy: 0.4975
17056/25000 [===================>..........] - ETA: 19s - loss: 7.7062 - accuracy: 0.4974
17088/25000 [===================>..........] - ETA: 19s - loss: 7.7025 - accuracy: 0.4977
17120/25000 [===================>..........] - ETA: 19s - loss: 7.7069 - accuracy: 0.4974
17152/25000 [===================>..........] - ETA: 19s - loss: 7.7077 - accuracy: 0.4973
17184/25000 [===================>..........] - ETA: 19s - loss: 7.7068 - accuracy: 0.4974
17216/25000 [===================>..........] - ETA: 19s - loss: 7.7120 - accuracy: 0.4970
17248/25000 [===================>..........] - ETA: 19s - loss: 7.7084 - accuracy: 0.4973
17280/25000 [===================>..........] - ETA: 19s - loss: 7.7048 - accuracy: 0.4975
17312/25000 [===================>..........] - ETA: 19s - loss: 7.7065 - accuracy: 0.4974
17344/25000 [===================>..........] - ETA: 19s - loss: 7.7064 - accuracy: 0.4974
17376/25000 [===================>..........] - ETA: 18s - loss: 7.7090 - accuracy: 0.4972
17408/25000 [===================>..........] - ETA: 18s - loss: 7.7063 - accuracy: 0.4974
17440/25000 [===================>..........] - ETA: 18s - loss: 7.7106 - accuracy: 0.4971
17472/25000 [===================>..........] - ETA: 18s - loss: 7.7123 - accuracy: 0.4970
17504/25000 [====================>.........] - ETA: 18s - loss: 7.7122 - accuracy: 0.4970
17536/25000 [====================>.........] - ETA: 18s - loss: 7.7165 - accuracy: 0.4967
17568/25000 [====================>.........] - ETA: 18s - loss: 7.7216 - accuracy: 0.4964
17600/25000 [====================>.........] - ETA: 18s - loss: 7.7250 - accuracy: 0.4962
17632/25000 [====================>.........] - ETA: 18s - loss: 7.7266 - accuracy: 0.4961
17664/25000 [====================>.........] - ETA: 18s - loss: 7.7256 - accuracy: 0.4962
17696/25000 [====================>.........] - ETA: 18s - loss: 7.7247 - accuracy: 0.4962
17728/25000 [====================>.........] - ETA: 18s - loss: 7.7289 - accuracy: 0.4959
17760/25000 [====================>.........] - ETA: 18s - loss: 7.7279 - accuracy: 0.4960
17792/25000 [====================>.........] - ETA: 17s - loss: 7.7244 - accuracy: 0.4962
17824/25000 [====================>.........] - ETA: 17s - loss: 7.7243 - accuracy: 0.4962
17856/25000 [====================>.........] - ETA: 17s - loss: 7.7233 - accuracy: 0.4963
17888/25000 [====================>.........] - ETA: 17s - loss: 7.7232 - accuracy: 0.4963
17920/25000 [====================>.........] - ETA: 17s - loss: 7.7214 - accuracy: 0.4964
17952/25000 [====================>.........] - ETA: 17s - loss: 7.7213 - accuracy: 0.4964
17984/25000 [====================>.........] - ETA: 17s - loss: 7.7246 - accuracy: 0.4962
18016/25000 [====================>.........] - ETA: 17s - loss: 7.7228 - accuracy: 0.4963
18048/25000 [====================>.........] - ETA: 17s - loss: 7.7218 - accuracy: 0.4964
18080/25000 [====================>.........] - ETA: 17s - loss: 7.7243 - accuracy: 0.4962
18112/25000 [====================>.........] - ETA: 17s - loss: 7.7250 - accuracy: 0.4962
18144/25000 [====================>.........] - ETA: 17s - loss: 7.7241 - accuracy: 0.4963
18176/25000 [====================>.........] - ETA: 17s - loss: 7.7274 - accuracy: 0.4960
18208/25000 [====================>.........] - ETA: 16s - loss: 7.7306 - accuracy: 0.4958
18240/25000 [====================>.........] - ETA: 16s - loss: 7.7297 - accuracy: 0.4959
18272/25000 [====================>.........] - ETA: 16s - loss: 7.7312 - accuracy: 0.4958
18304/25000 [====================>.........] - ETA: 16s - loss: 7.7286 - accuracy: 0.4960
18336/25000 [=====================>........] - ETA: 16s - loss: 7.7302 - accuracy: 0.4959
18368/25000 [=====================>........] - ETA: 16s - loss: 7.7292 - accuracy: 0.4959
18400/25000 [=====================>........] - ETA: 16s - loss: 7.7316 - accuracy: 0.4958
18432/25000 [=====================>........] - ETA: 16s - loss: 7.7273 - accuracy: 0.4960
18464/25000 [=====================>........] - ETA: 16s - loss: 7.7248 - accuracy: 0.4962
18496/25000 [=====================>........] - ETA: 16s - loss: 7.7213 - accuracy: 0.4964
18528/25000 [=====================>........] - ETA: 16s - loss: 7.7254 - accuracy: 0.4962
18560/25000 [=====================>........] - ETA: 16s - loss: 7.7319 - accuracy: 0.4957
18592/25000 [=====================>........] - ETA: 15s - loss: 7.7318 - accuracy: 0.4958
18624/25000 [=====================>........] - ETA: 15s - loss: 7.7275 - accuracy: 0.4960
18656/25000 [=====================>........] - ETA: 15s - loss: 7.7283 - accuracy: 0.4960
18688/25000 [=====================>........] - ETA: 15s - loss: 7.7282 - accuracy: 0.4960
18720/25000 [=====================>........] - ETA: 15s - loss: 7.7272 - accuracy: 0.4960
18752/25000 [=====================>........] - ETA: 15s - loss: 7.7255 - accuracy: 0.4962
18784/25000 [=====================>........] - ETA: 15s - loss: 7.7229 - accuracy: 0.4963
18816/25000 [=====================>........] - ETA: 15s - loss: 7.7220 - accuracy: 0.4964
18848/25000 [=====================>........] - ETA: 15s - loss: 7.7219 - accuracy: 0.4964
18880/25000 [=====================>........] - ETA: 15s - loss: 7.7235 - accuracy: 0.4963
18912/25000 [=====================>........] - ETA: 15s - loss: 7.7234 - accuracy: 0.4963
18944/25000 [=====================>........] - ETA: 15s - loss: 7.7241 - accuracy: 0.4963
18976/25000 [=====================>........] - ETA: 15s - loss: 7.7256 - accuracy: 0.4962
19008/25000 [=====================>........] - ETA: 14s - loss: 7.7239 - accuracy: 0.4963
19040/25000 [=====================>........] - ETA: 14s - loss: 7.7214 - accuracy: 0.4964
19072/25000 [=====================>........] - ETA: 14s - loss: 7.7269 - accuracy: 0.4961
19104/25000 [=====================>........] - ETA: 14s - loss: 7.7292 - accuracy: 0.4959
19136/25000 [=====================>........] - ETA: 14s - loss: 7.7299 - accuracy: 0.4959
19168/25000 [======================>.......] - ETA: 14s - loss: 7.7282 - accuracy: 0.4960
19200/25000 [======================>.......] - ETA: 14s - loss: 7.7313 - accuracy: 0.4958
19232/25000 [======================>.......] - ETA: 14s - loss: 7.7296 - accuracy: 0.4959
19264/25000 [======================>.......] - ETA: 14s - loss: 7.7295 - accuracy: 0.4959
19296/25000 [======================>.......] - ETA: 14s - loss: 7.7302 - accuracy: 0.4959
19328/25000 [======================>.......] - ETA: 14s - loss: 7.7293 - accuracy: 0.4959
19360/25000 [======================>.......] - ETA: 14s - loss: 7.7308 - accuracy: 0.4958
19392/25000 [======================>.......] - ETA: 13s - loss: 7.7291 - accuracy: 0.4959
19424/25000 [======================>.......] - ETA: 13s - loss: 7.7242 - accuracy: 0.4962
19456/25000 [======================>.......] - ETA: 13s - loss: 7.7242 - accuracy: 0.4962
19488/25000 [======================>.......] - ETA: 13s - loss: 7.7193 - accuracy: 0.4966
19520/25000 [======================>.......] - ETA: 13s - loss: 7.7200 - accuracy: 0.4965
19552/25000 [======================>.......] - ETA: 13s - loss: 7.7207 - accuracy: 0.4965
19584/25000 [======================>.......] - ETA: 13s - loss: 7.7222 - accuracy: 0.4964
19616/25000 [======================>.......] - ETA: 13s - loss: 7.7237 - accuracy: 0.4963
19648/25000 [======================>.......] - ETA: 13s - loss: 7.7236 - accuracy: 0.4963
19680/25000 [======================>.......] - ETA: 13s - loss: 7.7212 - accuracy: 0.4964
19712/25000 [======================>.......] - ETA: 13s - loss: 7.7203 - accuracy: 0.4965
19744/25000 [======================>.......] - ETA: 13s - loss: 7.7171 - accuracy: 0.4967
19776/25000 [======================>.......] - ETA: 13s - loss: 7.7155 - accuracy: 0.4968
19808/25000 [======================>.......] - ETA: 12s - loss: 7.7138 - accuracy: 0.4969
19840/25000 [======================>.......] - ETA: 12s - loss: 7.7138 - accuracy: 0.4969
19872/25000 [======================>.......] - ETA: 12s - loss: 7.7152 - accuracy: 0.4968
19904/25000 [======================>.......] - ETA: 12s - loss: 7.7167 - accuracy: 0.4967
19936/25000 [======================>.......] - ETA: 12s - loss: 7.7158 - accuracy: 0.4968
19968/25000 [======================>.......] - ETA: 12s - loss: 7.7188 - accuracy: 0.4966
20000/25000 [=======================>......] - ETA: 12s - loss: 7.7172 - accuracy: 0.4967
20032/25000 [=======================>......] - ETA: 12s - loss: 7.7187 - accuracy: 0.4966
20064/25000 [=======================>......] - ETA: 12s - loss: 7.7209 - accuracy: 0.4965
20096/25000 [=======================>......] - ETA: 12s - loss: 7.7216 - accuracy: 0.4964
20128/25000 [=======================>......] - ETA: 12s - loss: 7.7245 - accuracy: 0.4962
20160/25000 [=======================>......] - ETA: 12s - loss: 7.7252 - accuracy: 0.4962
20192/25000 [=======================>......] - ETA: 11s - loss: 7.7205 - accuracy: 0.4965
20224/25000 [=======================>......] - ETA: 11s - loss: 7.7220 - accuracy: 0.4964
20256/25000 [=======================>......] - ETA: 11s - loss: 7.7166 - accuracy: 0.4967
20288/25000 [=======================>......] - ETA: 11s - loss: 7.7180 - accuracy: 0.4966
20320/25000 [=======================>......] - ETA: 11s - loss: 7.7157 - accuracy: 0.4968
20352/25000 [=======================>......] - ETA: 11s - loss: 7.7126 - accuracy: 0.4970
20384/25000 [=======================>......] - ETA: 11s - loss: 7.7125 - accuracy: 0.4970
20416/25000 [=======================>......] - ETA: 11s - loss: 7.7132 - accuracy: 0.4970
20448/25000 [=======================>......] - ETA: 11s - loss: 7.7124 - accuracy: 0.4970
20480/25000 [=======================>......] - ETA: 11s - loss: 7.7138 - accuracy: 0.4969
20512/25000 [=======================>......] - ETA: 11s - loss: 7.7130 - accuracy: 0.4970
20544/25000 [=======================>......] - ETA: 11s - loss: 7.7114 - accuracy: 0.4971
20576/25000 [=======================>......] - ETA: 11s - loss: 7.7136 - accuracy: 0.4969
20608/25000 [=======================>......] - ETA: 10s - loss: 7.7128 - accuracy: 0.4970
20640/25000 [=======================>......] - ETA: 10s - loss: 7.7097 - accuracy: 0.4972
20672/25000 [=======================>......] - ETA: 10s - loss: 7.7119 - accuracy: 0.4970
20704/25000 [=======================>......] - ETA: 10s - loss: 7.7096 - accuracy: 0.4972
20736/25000 [=======================>......] - ETA: 10s - loss: 7.7095 - accuracy: 0.4972
20768/25000 [=======================>......] - ETA: 10s - loss: 7.7072 - accuracy: 0.4974
20800/25000 [=======================>......] - ETA: 10s - loss: 7.7050 - accuracy: 0.4975
20832/25000 [=======================>......] - ETA: 10s - loss: 7.7071 - accuracy: 0.4974
20864/25000 [========================>.....] - ETA: 10s - loss: 7.7085 - accuracy: 0.4973
20896/25000 [========================>.....] - ETA: 10s - loss: 7.7040 - accuracy: 0.4976
20928/25000 [========================>.....] - ETA: 10s - loss: 7.7047 - accuracy: 0.4975
20960/25000 [========================>.....] - ETA: 10s - loss: 7.7025 - accuracy: 0.4977
20992/25000 [========================>.....] - ETA: 9s - loss: 7.7046 - accuracy: 0.4975 
21024/25000 [========================>.....] - ETA: 9s - loss: 7.7060 - accuracy: 0.4974
21056/25000 [========================>.....] - ETA: 9s - loss: 7.7052 - accuracy: 0.4975
21088/25000 [========================>.....] - ETA: 9s - loss: 7.7059 - accuracy: 0.4974
21120/25000 [========================>.....] - ETA: 9s - loss: 7.7065 - accuracy: 0.4974
21152/25000 [========================>.....] - ETA: 9s - loss: 7.7043 - accuracy: 0.4975
21184/25000 [========================>.....] - ETA: 9s - loss: 7.7079 - accuracy: 0.4973
21216/25000 [========================>.....] - ETA: 9s - loss: 7.7071 - accuracy: 0.4974
21248/25000 [========================>.....] - ETA: 9s - loss: 7.7092 - accuracy: 0.4972
21280/25000 [========================>.....] - ETA: 9s - loss: 7.7070 - accuracy: 0.4974
21312/25000 [========================>.....] - ETA: 9s - loss: 7.7076 - accuracy: 0.4973
21344/25000 [========================>.....] - ETA: 9s - loss: 7.7061 - accuracy: 0.4974
21376/25000 [========================>.....] - ETA: 9s - loss: 7.7068 - accuracy: 0.4974
21408/25000 [========================>.....] - ETA: 8s - loss: 7.7046 - accuracy: 0.4975
21440/25000 [========================>.....] - ETA: 8s - loss: 7.7024 - accuracy: 0.4977
21472/25000 [========================>.....] - ETA: 8s - loss: 7.7023 - accuracy: 0.4977
21504/25000 [========================>.....] - ETA: 8s - loss: 7.6994 - accuracy: 0.4979
21536/25000 [========================>.....] - ETA: 8s - loss: 7.6965 - accuracy: 0.4980
21568/25000 [========================>.....] - ETA: 8s - loss: 7.6986 - accuracy: 0.4979
21600/25000 [========================>.....] - ETA: 8s - loss: 7.6957 - accuracy: 0.4981
21632/25000 [========================>.....] - ETA: 8s - loss: 7.6978 - accuracy: 0.4980
21664/25000 [========================>.....] - ETA: 8s - loss: 7.6985 - accuracy: 0.4979
21696/25000 [=========================>....] - ETA: 8s - loss: 7.6998 - accuracy: 0.4978
21728/25000 [=========================>....] - ETA: 8s - loss: 7.6991 - accuracy: 0.4979
21760/25000 [=========================>....] - ETA: 8s - loss: 7.6948 - accuracy: 0.4982
21792/25000 [=========================>....] - ETA: 7s - loss: 7.6955 - accuracy: 0.4981
21824/25000 [=========================>....] - ETA: 7s - loss: 7.6940 - accuracy: 0.4982
21856/25000 [=========================>....] - ETA: 7s - loss: 7.6954 - accuracy: 0.4981
21888/25000 [=========================>....] - ETA: 7s - loss: 7.6953 - accuracy: 0.4981
21920/25000 [=========================>....] - ETA: 7s - loss: 7.6904 - accuracy: 0.4984
21952/25000 [=========================>....] - ETA: 7s - loss: 7.6876 - accuracy: 0.4986
21984/25000 [=========================>....] - ETA: 7s - loss: 7.6868 - accuracy: 0.4987
22016/25000 [=========================>....] - ETA: 7s - loss: 7.6840 - accuracy: 0.4989
22048/25000 [=========================>....] - ETA: 7s - loss: 7.6840 - accuracy: 0.4989
22080/25000 [=========================>....] - ETA: 7s - loss: 7.6812 - accuracy: 0.4990
22112/25000 [=========================>....] - ETA: 7s - loss: 7.6826 - accuracy: 0.4990
22144/25000 [=========================>....] - ETA: 7s - loss: 7.6791 - accuracy: 0.4992
22176/25000 [=========================>....] - ETA: 7s - loss: 7.6825 - accuracy: 0.4990
22208/25000 [=========================>....] - ETA: 6s - loss: 7.6839 - accuracy: 0.4989
22240/25000 [=========================>....] - ETA: 6s - loss: 7.6839 - accuracy: 0.4989
22272/25000 [=========================>....] - ETA: 6s - loss: 7.6831 - accuracy: 0.4989
22304/25000 [=========================>....] - ETA: 6s - loss: 7.6859 - accuracy: 0.4987
22336/25000 [=========================>....] - ETA: 6s - loss: 7.6852 - accuracy: 0.4988
22368/25000 [=========================>....] - ETA: 6s - loss: 7.6810 - accuracy: 0.4991
22400/25000 [=========================>....] - ETA: 6s - loss: 7.6817 - accuracy: 0.4990
22432/25000 [=========================>....] - ETA: 6s - loss: 7.6817 - accuracy: 0.4990
22464/25000 [=========================>....] - ETA: 6s - loss: 7.6803 - accuracy: 0.4991
22496/25000 [=========================>....] - ETA: 6s - loss: 7.6803 - accuracy: 0.4991
22528/25000 [==========================>...] - ETA: 6s - loss: 7.6823 - accuracy: 0.4990
22560/25000 [==========================>...] - ETA: 6s - loss: 7.6816 - accuracy: 0.4990
22592/25000 [==========================>...] - ETA: 5s - loss: 7.6809 - accuracy: 0.4991
22624/25000 [==========================>...] - ETA: 5s - loss: 7.6781 - accuracy: 0.4992
22656/25000 [==========================>...] - ETA: 5s - loss: 7.6774 - accuracy: 0.4993
22688/25000 [==========================>...] - ETA: 5s - loss: 7.6815 - accuracy: 0.4990
22720/25000 [==========================>...] - ETA: 5s - loss: 7.6801 - accuracy: 0.4991
22752/25000 [==========================>...] - ETA: 5s - loss: 7.6814 - accuracy: 0.4990
22784/25000 [==========================>...] - ETA: 5s - loss: 7.6808 - accuracy: 0.4991
22816/25000 [==========================>...] - ETA: 5s - loss: 7.6807 - accuracy: 0.4991
22848/25000 [==========================>...] - ETA: 5s - loss: 7.6827 - accuracy: 0.4989
22880/25000 [==========================>...] - ETA: 5s - loss: 7.6861 - accuracy: 0.4987
22912/25000 [==========================>...] - ETA: 5s - loss: 7.6860 - accuracy: 0.4987
22944/25000 [==========================>...] - ETA: 5s - loss: 7.6840 - accuracy: 0.4989
22976/25000 [==========================>...] - ETA: 5s - loss: 7.6840 - accuracy: 0.4989
23008/25000 [==========================>...] - ETA: 4s - loss: 7.6826 - accuracy: 0.4990
23040/25000 [==========================>...] - ETA: 4s - loss: 7.6813 - accuracy: 0.4990
23072/25000 [==========================>...] - ETA: 4s - loss: 7.6826 - accuracy: 0.4990
23104/25000 [==========================>...] - ETA: 4s - loss: 7.6819 - accuracy: 0.4990
23136/25000 [==========================>...] - ETA: 4s - loss: 7.6812 - accuracy: 0.4990
23168/25000 [==========================>...] - ETA: 4s - loss: 7.6818 - accuracy: 0.4990
23200/25000 [==========================>...] - ETA: 4s - loss: 7.6798 - accuracy: 0.4991
23232/25000 [==========================>...] - ETA: 4s - loss: 7.6792 - accuracy: 0.4992
23264/25000 [==========================>...] - ETA: 4s - loss: 7.6772 - accuracy: 0.4993
23296/25000 [==========================>...] - ETA: 4s - loss: 7.6772 - accuracy: 0.4993
23328/25000 [==========================>...] - ETA: 4s - loss: 7.6785 - accuracy: 0.4992
23360/25000 [===========================>..] - ETA: 4s - loss: 7.6797 - accuracy: 0.4991
23392/25000 [===========================>..] - ETA: 3s - loss: 7.6804 - accuracy: 0.4991
23424/25000 [===========================>..] - ETA: 3s - loss: 7.6791 - accuracy: 0.4992
23456/25000 [===========================>..] - ETA: 3s - loss: 7.6823 - accuracy: 0.4990
23488/25000 [===========================>..] - ETA: 3s - loss: 7.6842 - accuracy: 0.4989
23520/25000 [===========================>..] - ETA: 3s - loss: 7.6842 - accuracy: 0.4989
23552/25000 [===========================>..] - ETA: 3s - loss: 7.6822 - accuracy: 0.4990
23584/25000 [===========================>..] - ETA: 3s - loss: 7.6829 - accuracy: 0.4989
23616/25000 [===========================>..] - ETA: 3s - loss: 7.6822 - accuracy: 0.4990
23648/25000 [===========================>..] - ETA: 3s - loss: 7.6809 - accuracy: 0.4991
23680/25000 [===========================>..] - ETA: 3s - loss: 7.6776 - accuracy: 0.4993
23712/25000 [===========================>..] - ETA: 3s - loss: 7.6783 - accuracy: 0.4992
23744/25000 [===========================>..] - ETA: 3s - loss: 7.6763 - accuracy: 0.4994
23776/25000 [===========================>..] - ETA: 3s - loss: 7.6789 - accuracy: 0.4992
23808/25000 [===========================>..] - ETA: 2s - loss: 7.6795 - accuracy: 0.4992
23840/25000 [===========================>..] - ETA: 2s - loss: 7.6788 - accuracy: 0.4992
23872/25000 [===========================>..] - ETA: 2s - loss: 7.6782 - accuracy: 0.4992
23904/25000 [===========================>..] - ETA: 2s - loss: 7.6756 - accuracy: 0.4994
23936/25000 [===========================>..] - ETA: 2s - loss: 7.6769 - accuracy: 0.4993
23968/25000 [===========================>..] - ETA: 2s - loss: 7.6775 - accuracy: 0.4993
24000/25000 [===========================>..] - ETA: 2s - loss: 7.6800 - accuracy: 0.4991
24032/25000 [===========================>..] - ETA: 2s - loss: 7.6819 - accuracy: 0.4990
24064/25000 [===========================>..] - ETA: 2s - loss: 7.6806 - accuracy: 0.4991
24096/25000 [===========================>..] - ETA: 2s - loss: 7.6793 - accuracy: 0.4992
24128/25000 [===========================>..] - ETA: 2s - loss: 7.6787 - accuracy: 0.4992
24160/25000 [===========================>..] - ETA: 2s - loss: 7.6787 - accuracy: 0.4992
24192/25000 [============================>.] - ETA: 1s - loss: 7.6774 - accuracy: 0.4993
24224/25000 [============================>.] - ETA: 1s - loss: 7.6761 - accuracy: 0.4994
24256/25000 [============================>.] - ETA: 1s - loss: 7.6767 - accuracy: 0.4993
24288/25000 [============================>.] - ETA: 1s - loss: 7.6742 - accuracy: 0.4995
24320/25000 [============================>.] - ETA: 1s - loss: 7.6717 - accuracy: 0.4997
24352/25000 [============================>.] - ETA: 1s - loss: 7.6717 - accuracy: 0.4997
24384/25000 [============================>.] - ETA: 1s - loss: 7.6698 - accuracy: 0.4998
24416/25000 [============================>.] - ETA: 1s - loss: 7.6704 - accuracy: 0.4998
24448/25000 [============================>.] - ETA: 1s - loss: 7.6710 - accuracy: 0.4997
24480/25000 [============================>.] - ETA: 1s - loss: 7.6716 - accuracy: 0.4997
24512/25000 [============================>.] - ETA: 1s - loss: 7.6697 - accuracy: 0.4998
24544/25000 [============================>.] - ETA: 1s - loss: 7.6679 - accuracy: 0.4999
24576/25000 [============================>.] - ETA: 1s - loss: 7.6679 - accuracy: 0.4999
24608/25000 [============================>.] - ETA: 0s - loss: 7.6672 - accuracy: 0.5000
24640/25000 [============================>.] - ETA: 0s - loss: 7.6697 - accuracy: 0.4998
24672/25000 [============================>.] - ETA: 0s - loss: 7.6691 - accuracy: 0.4998
24704/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24736/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
24768/25000 [============================>.] - ETA: 0s - loss: 7.6672 - accuracy: 0.5000
24800/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24832/25000 [============================>.] - ETA: 0s - loss: 7.6641 - accuracy: 0.5002
24864/25000 [============================>.] - ETA: 0s - loss: 7.6623 - accuracy: 0.5003
24896/25000 [============================>.] - ETA: 0s - loss: 7.6648 - accuracy: 0.5001
24928/25000 [============================>.] - ETA: 0s - loss: 7.6642 - accuracy: 0.5002
24960/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
24992/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
25000/25000 [==============================] - 72s 3ms/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
Loading data...
Using TensorFlow backend.





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'../mlmodels/dataset/json/hyper_titanic_randomForest.json'[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      7[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/hyper_titanic_randomForest.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//mnist_mlmodels_.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl_titanic.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m     [0mchoice[0m[0;34m=[0m[0;34m'json'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m     [0mconfig_mode[0m[0;34m=[0m [0;34m'test'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m     [0mdata_path[0m[0;34m=[0m [0;34m'../mlmodels/dataset/json/gluon_automl.json'[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m )

[0;32m~/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py[0m in [0;36mget_params[0;34m(choice, data_path, config_mode, **kw)[0m
[1;32m     80[0m             __file__)).parent.parent / "model_gluon/gluon_automl.json" if data_path == "dataset/" else data_path
[1;32m     81[0m [0;34m[0m[0m
[0;32m---> 82[0;31m         [0;32mwith[0m [0mopen[0m[0;34m([0m[0mdata_path[0m[0;34m,[0m [0mencoding[0m[0;34m=[0m[0;34m'utf-8'[0m[0;34m)[0m [0;32mas[0m [0mconfig_f[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     83[0m             [0mconfig[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mconfig_f[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     84[0m             [0mconfig[0m [0;34m=[0m [0mconfig[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/gluon_automl.json'
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow__lstm_json.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      5[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0mmodel[0m         [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mModel[0m[0;34m([0m[0mmodel_pars[0m[0;34m=[0m[0mmodel_pars[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m)[0m             [0;31m# Create Model instance[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mmodel[0m[0;34m,[0m [0msess[0m   [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m,[0m [0mout_pars[0m[0;34m=[0m[0mout_pars[0m[0;34m)[0m          [0;31m# fit the model[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_titanic.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.py 

[0;36m  File [0;32m"/home/runner/work/mlmodels/mlmodels/mlmodels/example/vision_mnist.py"[0;36m, line [0;32m15[0m
[0;31m    !git clone https://github.com/ahmed3bbas/mlmodels.git[0m
[0m    ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_hyper.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_hyper.py[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mutil[0m [0;32mimport[0m [0mpath_norm_dict[0m[0;34m,[0m [0mpath_norm[0m[0;34m,[0m [0mparams_json_load[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mprint[0m[0;34m([0m[0mmlmodels[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;34m[0m[0m

[0;31mNameError[0m: name 'mlmodels' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.py 

Deprecaton set to False
/home/runner/work/mlmodels/mlmodels
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/lightgbm_glass.py[0m in [0;36m<module>[0;34m[0m
[1;32m     20[0m [0;34m[0m[0m
[1;32m     21[0m [0;34m[0m[0m
[0;32m---> 22[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     23[0m [0mprint[0m[0;34m([0m[0mpars[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     24[0m [0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_glass.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_model.py 

<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_model.py[0m in [0;36m<module>[0;34m[0m
[1;32m     25[0m [0;31m# Model Parameters[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m     26[0m [0;31m# model_pars, data_pars, compute_pars, out_pars[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 27[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m[0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     28[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     29[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpdict[0m   [0;34m)[0m   [0;31m###Normalize path[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'

  test_jupyter /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_jupyter', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_jupyter 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '14ea42490a753d0445fda074e8d3eac878d2aeed', 'workflow': 'test_jupyter'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_jupyter

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/14ea42490a753d0445fda074e8d3eac878d2aeed
Package                   Version    Location
------------------------- ---------- -----------------------------------
absl-py                   0.9.0
alembic                   1.4.2
astor                     0.8.1
attrs                     19.3.0
autogluon                 0.0.5
backcall                  0.1.0
bcrypt                    3.1.7
bleach                    3.1.5
blis                      0.4.1
boto                      2.49.0
boto3                     1.9.187
botocore                  1.12.253
catalogue                 1.0.0
catboost                  0.23.1
certifi                   2020.4.5.1
cffi                      1.14.0
chardet                   3.0.4
cli-code                  28.1.0
click                     7.1.2
cliff                     3.1.0
cloudpickle               1.4.1
cmd2                      0.8.9
cmdstanpy                 0.4.0
colorlog                  4.1.0
configparser              5.0.0
ConfigSpace               0.4.10
convertdate               2.2.1
cryptography              2.9.2
cycler                    0.10.0
cymem                     2.0.3
Cython                    0.29.17
dask                      2.6.0
databricks-cli            0.10.0
dataclasses               0.7
decorator                 4.4.2
deepctr                   0.7.4
defusedxml                0.6.0
dill                      0.3.1.1
distributed               2.6.0
docker                    4.2.0
docutils                  0.15.2
entrypoints               0.3
ephem                     3.7.7.1
fbprophet                 0.6
Flask                     1.1.2
future                    0.18.2
gast                      0.2.2
gensim                    3.8.3
gitdb                     4.0.5
GitPython                 3.1.2
gluoncv                   0.7.0
gluonnlp                  0.8.1
gluonts                   0.4.2
google-pasta              0.2.0
googleapis-common-protos  1.51.0
gorilla                   0.3.0
graphviz                  0.8.4
grpcio                    1.29.0
gunicorn                  20.0.4
h5py                      2.10.0
HeapDict                  1.0.1
holidays                  0.10.2
hyperopt                  0.1.2
idna                      2.9
importlib-metadata        1.6.0
ipykernel                 5.2.1
ipython                   7.14.0
ipython-genutils          0.2.0
itsdangerous              1.1.0
jedi                      0.17.0
Jinja2                    2.11.2
jmespath                  0.10.0
joblib                    0.15.0
jsonschema                3.2.0
jupyter-client            6.1.3
jupyter-core              4.6.3
Keras                     2.3.1
Keras-Applications        1.0.8
keras-contrib             2.0.8
keras-mdn-layer           0.2.1
Keras-Preprocessing       1.1.2
kiwisolver                1.2.0
korean-lunar-calendar     0.2.1
lightgbm                  2.3.0
LunarCalendar             0.0.9
Mako                      1.1.2
Markdown                  3.2.2
MarkupSafe                1.1.1
matchzoo-py               1.1.1
matplotlib                3.2.1
mistune                   0.8.4
mlflow                    1.7.1
mlmodels                  0.35.2     /home/runner/work/mlmodels/mlmodels
msgpack                   1.0.0
murmurhash                1.0.2
mxnet                     1.6.0
nbconvert                 5.6.1
nbformat                  5.0.6
networkx                  2.4
nltk                      3.5
notebook                  6.0.3
numexpr                   2.7.1
numpy                     1.18.2
opt-einsum                3.2.1
optuna                    1.1.0
packaging                 20.3
pandas                    0.25.3
pandocfilters             1.4.2
paramiko                  2.7.1
parso                     0.7.0
pbr                       5.4.5
pexpect                   4.8.0
pickleshare               0.7.5
Pillow                    6.2.1
pip                       20.1
plac                      1.1.3
plotly                    4.7.1
portalocker               1.7.0
preshed                   3.0.2
prettytable               0.7.2
prometheus-client         0.7.1
prometheus-flask-exporter 0.13.0
promise                   2.3
prompt-toolkit            3.0.5
protobuf                  3.12.0
psutil                    5.7.0
ptyprocess                0.6.0
pyaml                     20.4.0
pycparser                 2.20
pydantic                  1.4
Pygments                  2.6.1
PyMeeus                   0.3.7
pymongo                   3.10.1
PyNaCl                    1.3.0
pyparsing                 2.4.7
pyperclip                 1.8.0
pyrsistent                0.16.0
pystan                    2.19.1.1
python-dateutil           2.8.0
python-editor             1.0.4
pytorch-lightning         0.7.3
pytorch-transformers      1.2.0
pytz                      2020.1
PyYAML                    5.3.1
pyzmq                     19.0.1
querystring-parser        1.2.4
regex                     2020.5.14
requests                  2.23.0
retrying                  1.3.3
s3transfer                0.2.1
sacremoses                0.0.43
scikit-learn              0.21.2
scikit-optimize           0.7.4
scipy                     1.4.1
Send2Trash                1.5.0
sentence-transformers     0.2.4
sentencepiece             0.1.90
setuptools                45.2.0
setuptools-git            1.2
simplejson                3.17.0
six                       1.14.0
smart-open                2.0.0
smmap                     3.0.4
sortedcontainers          2.1.0
spacy                     2.2.4
SQLAlchemy                1.3.13
sqlparse                  0.3.1
srsly                     1.0.2
stevedore                 1.32.0
tabulate                  0.8.7
tblib                     1.6.0
tensorboard               1.15.0
tensorboardX              2.0
tensorflow                1.15.2
tensorflow-datasets       3.0.0
tensorflow-estimator      1.15.1
tensorflow-metadata       0.22.0
tensorflow-probability    0.7.0
termcolor                 1.1.0
terminado                 0.8.3
testpath                  0.4.4
thinc                     7.4.0
toml                      0.10.1
toolz                     0.10.0
torch                     1.2.0
torchtext                 0.6.0
torchvision               0.4.0
tornado                   6.0.4
tqdm                      4.46.0
traitlets                 4.3.3
transformers              2.3.0
typing                    3.7.4.1
ujson                     1.35
urllib3                   1.25.9
versioneer                0.18
wasabi                    0.6.0
wcwidth                   0.1.9
webencodings              0.5.1
websocket-client          0.57.0
Werkzeug                  1.0.1
wheel                     0.34.2
wrapt                     1.12.1
zict                      2.0.0
zipp                      3.1.0

 ************************************************************************************************************************

 ************************************************************************************************************************
/home/runner/work/mlmodels/mlmodels/mlmodels/example/
############ List of files ################################
['ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow_1_lstm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras-textcnn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_hyper.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m5.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_model.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py']





 ************************************************************************************************************************
############ Running Jupyter files ################################





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_svm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;34m[0m[0m
[0;32m----> 6[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;34m[0m[0m
[1;32m      6[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//fashion_MNIST_mlmodels.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_home_retail.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_home_retail.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_home_retail.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras_charcnn_reuters.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mmodel_pars[0m      [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'model_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mdata_pars[0m       [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'data_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mcompute_pars[0m    [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'compute_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mout_pars[0m        [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'out_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'reuters_charcnn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl.ipynb 

/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073
Warning: `hyperparameter_tune=True` is currently experimental and may cause the process to hang. Setting `auto_stack=True` instead is recommended to achieve maximum quality models.
Beginning AutoGluon training ... Time limit = 120s
AutoGluon will save models to dataset/
Train Data Rows:    39073
Train Data Columns: 15
Preprocessing data ...
Here are the first 10 unique label values in your data:  [' Tech-support' ' Transport-moving' ' Other-service' ' ?'
 ' Handlers-cleaners' ' Sales' ' Craft-repair' ' Adm-clerical'
 ' Exec-managerial' ' Prof-specialty']
AutoGluon infers your prediction problem is: multiclass  (because dtype of label-column == object)
If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])

Feature Generator processed 39073 data points with 14 features
Original Features:
	int features: 6
	object features: 8
Generated Features:
	int features: 0
All Features:
	int features: 6
	object features: 8
	Data preprocessing and feature engineering runtime = 0.23s ...
AutoGluon will gauge predictive performance using evaluation metric: accuracy
To change this, specify the eval_metric argument of fit()
AutoGluon will early stop models using evaluation metric: accuracy
Saving dataset/learner.pkl
Beginning hyperparameter tuning for Gradient Boosting Model...
Hyperparameter search space for Gradient Boosting Model: 
num_leaves:   Int: lower=26, upper=30
learning_rate:   Real: lower=0.005, upper=0.2
feature_fraction:   Real: lower=0.75, upper=1.0
min_data_in_leaf:   Int: lower=2, upper=30
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
Warning: Exception caused LightGBMClassifier to fail during hyperparameter tuning... Skipping this model.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
ValueError: Illegal default value 36
Saving dataset/models/trainer.pkl
Beginning hyperparameter tuning for Neural Network...
Hyperparameter search space for Neural Network: 
network_type:   Categorical['widedeep', 'feedforward']
layers:   Categorical[[100], [1000], [200, 100], [300, 200, 100]]
activation:   Categorical['relu', 'softrelu', 'tanh']
embedding_size_factor:   Real: lower=0.5, upper=1.5
use_batchnorm:   Categorical[True, False]
dropout_prob:   Real: lower=0.0, upper=0.5
learning_rate:   Real: lower=0.0001, upper=0.01
weight_decay:   Real: lower=1e-12, upper=0.1
AutoGluon Neural Network infers features are of the following types:
{
    "continuous": [
        "age",
        "education-num",
        "hours-per-week"
    ],
    "skewed": [
        "fnlwgt",
        "capital-gain",
        "capital-loss"
    ],
    "onehot": [
        "sex",
        "class"
    ],
    "embed": [
        "workclass",
        "education",
        "marital-status",
        "relationship",
        "race",
        "native-country"
    ],
    "language": []
}


Saving dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Saving dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
Starting Experiments
Num of Finished Tasks is 0
Num of Pending Tasks is 5
  0%|          | 0/5 [00:00<?, ?it/s]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Finished Task with config: {'activation.choice': 0, 'dropout_prob': 0.1, 'embedding_size_factor': 1.0, 'layers.choice': 0, 'learning_rate': 0.0005, 'network_type.choice': 0, 'use_batchnorm.choice': 0, 'weight_decay': 1e-06} and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:50<01:15, 25.08s/it]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Finished Task with config: {'activation.choice': 1, 'dropout_prob': 0.16857086532812196, 'embedding_size_factor': 0.5456400607094054, 'layers.choice': 1, 'learning_rate': 0.007004575707259909, 'network_type.choice': 0, 'use_batchnorm.choice': 0, 'weight_decay': 0.028942508360418444} and reward: 0.3488
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xc5\x93\xba\xe8\xd2D!X\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1u\xe2%\x04L\x9aX\r\x00\x00\x00layers.choiceq\x04K\x01X\r\x00\x00\x00learning_rateq\x05G?|\xb0\xd4y\\P(X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G?\x9d\xa3\x1a\xdb}"\xfeu.' and reward: 0.3488
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xc5\x93\xba\xe8\xd2D!X\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1u\xe2%\x04L\x9aX\r\x00\x00\x00layers.choiceq\x04K\x01X\r\x00\x00\x00learning_rateq\x05G?|\xb0\xd4y\\P(X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G?\x9d\xa3\x1a\xdb}"\xfeu.' and reward: 0.3488
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:42<01:42, 51.18s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:42<01:48, 54.08s/it]
Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Finished Task with config: {'activation.choice': 2, 'dropout_prob': 0.3767974679605539, 'embedding_size_factor': 1.3279252669428065, 'layers.choice': 1, 'learning_rate': 0.0011456062292310091, 'network_type.choice': 1, 'use_batchnorm.choice': 1, 'weight_decay': 0.0005349536213320226} and reward: 0.3668
Finished Task with config: b"\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x02X\x0c\x00\x00\x00dropout_probq\x02G?\xd8\x1ds \x86\xcb\xc4X\x15\x00\x00\x00embedding_size_factorq\x03G?\xf5?.\x90\x90\xd2\xa2X\r\x00\x00\x00layers.choiceq\x04K\x01X\r\x00\x00\x00learning_rateq\x05G?R\xc5\x05Ry\x18^X\x13\x00\x00\x00network_type.choiceq\x06K\x01X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?A\x87\x84'|\xdb(u." and reward: 0.3668
Finished Task with config: b"\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x02X\x0c\x00\x00\x00dropout_probq\x02G?\xd8\x1ds \x86\xcb\xc4X\x15\x00\x00\x00embedding_size_factorq\x03G?\xf5?.\x90\x90\xd2\xa2X\r\x00\x00\x00layers.choiceq\x04K\x01X\r\x00\x00\x00learning_rateq\x05G?R\xc5\x05Ry\x18^X\x13\x00\x00\x00network_type.choiceq\x06K\x01X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?A\x87\x84'|\xdb(u." and reward: 0.3668
Please either provide filename or allow plot in get_training_curves
Time for Neural Network hyperparameter optimization: 242.65290904045105
Best hyperparameter configuration for Tabular Neural Network: 
{'activation.choice': 0, 'dropout_prob': 0.1, 'embedding_size_factor': 1.0, 'layers.choice': 0, 'learning_rate': 0.0005, 'network_type.choice': 0, 'use_batchnorm.choice': 0, 'weight_decay': 1e-06}
Saving dataset/models/trainer.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Fitting model: weighted_ensemble_k0_l1 ... Training model for up to 119.77s of the -125.51s of remaining time.
Ensemble size: 9
Ensemble weights: 
[0.55555556 0.44444444 0.        ]
	0.3894	 = Validation accuracy score
	1.03s	 = Training runtime
	0.0s	 = Validation runtime
Saving dataset/models/weighted_ensemble_k0_l1/model.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
AutoGluon training complete, total runtime = 246.57s ...
Loading: dataset/models/trainer.pkl
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769
Loading: dataset/models/trainer.pkl
Loading: dataset/models/weighted_ensemble_k0_l1/model.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
test

  #### Module init   ############################################ 

  <module 'mlmodels.model_gluon.gluon_automl' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py'> 

  #### Loading params   ############################################## 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 453, in test_cli
    test_module(arg.model_uri, param_pars=param_pars)  # '1_lstm'
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 257, in test_module
    model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py", line 109, in get_params
    return model_pars, data_pars, compute_pars, out_pars
UnboundLocalError: local variable 'model_pars' referenced before assignment





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vison_fashion_MNIST.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow_1_lstm.ipynb 

/home/runner/work/mlmodels/mlmodels
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
test

  #### Module init   ############################################ 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  #### Model init   ############################################ 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f833a4cda90> 

  #### Fit   ######################################################## 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   #################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
[[ 0.          0.          0.          0.          0.          0.        ]
 [ 0.14518699  0.06513854  0.05077588 -0.01556904 -0.0377832   0.01143115]
 [-0.00226833 -0.20542315 -0.09949753 -0.13039699 -0.00810954 -0.04939499]
 [ 0.08235636  0.01528763 -0.1460554   0.20937544 -0.05050689 -0.07261261]
 [ 0.38797438  0.08608831 -0.19035907 -0.08192153 -0.49334198  0.01810492]
 [ 0.54295182  0.55312842  0.23170575 -0.1161572  -0.0644007  -0.14967208]
 [ 0.1808674   0.16483311  0.56253123 -0.39907017  0.15493059 -0.21370611]
 [ 0.18195316  0.17123272 -0.54547733  0.1997067   0.20272771  0.03305534]
 [-0.1474648   0.0929461  -0.1005443  -0.11242972  0.00321031  0.50743848]
 [ 0.          0.          0.          0.          0.          0.        ]]

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 
model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.42107200622558594, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 10:23:33.071142: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 455, in test_cli
    test(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 189, in test
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.4681723043322563, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 10:23:34.176931: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 457, in test_cli
    test_global(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 200, in test_global
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;34m[0m[0m
[0;32m---> 10[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras-textcnn.ipynb 

WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 400)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 400, 50)      500         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 398, 128)     19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 397, 128)     25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 396, 128)     32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 78,069
Trainable params: 78,069
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
   24576/17464789 [..............................] - ETA: 46s
   57344/17464789 [..............................] - ETA: 40s
   90112/17464789 [..............................] - ETA: 38s
  229376/17464789 [..............................] - ETA: 19s
  475136/17464789 [..............................] - ETA: 11s
  958464/17464789 [>.............................] - ETA: 6s 
 1941504/17464789 [==>...........................] - ETA: 3s
 3874816/17464789 [=====>........................] - ETA: 1s
 6955008/17464789 [==========>...................] - ETA: 0s
 9994240/17464789 [================>.............] - ETA: 0s
10985472/17464789 [=================>............] - ETA: 0s
13492224/17464789 [======================>.......] - ETA: 0s
16293888/17464789 [==========================>...] - ETA: 0s
17465344/17464789 [==============================] - 1s 0us/step
Pad sequences (samples x time)...
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-05-16 10:23:46.279279: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-16 10:23:46.283178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095215000 Hz
2020-05-16 10:23:46.283333: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559057d0afa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-16 10:23:46.283347: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 25000 samples, validate on 25000 samples
Epoch 1/1

   32/25000 [..............................] - ETA: 4:25 - loss: 8.6249 - accuracy: 0.4375
   64/25000 [..............................] - ETA: 2:43 - loss: 8.1458 - accuracy: 0.4688
   96/25000 [..............................] - ETA: 2:07 - loss: 7.3472 - accuracy: 0.5208
  128/25000 [..............................] - ETA: 1:49 - loss: 7.4270 - accuracy: 0.5156
  160/25000 [..............................] - ETA: 1:39 - loss: 7.5708 - accuracy: 0.5063
  192/25000 [..............................] - ETA: 1:32 - loss: 7.2673 - accuracy: 0.5260
  224/25000 [..............................] - ETA: 1:27 - loss: 7.2559 - accuracy: 0.5268
  256/25000 [..............................] - ETA: 1:23 - loss: 7.4270 - accuracy: 0.5156
  288/25000 [..............................] - ETA: 1:20 - loss: 7.6666 - accuracy: 0.5000
  320/25000 [..............................] - ETA: 1:18 - loss: 7.5708 - accuracy: 0.5063
  352/25000 [..............................] - ETA: 1:16 - loss: 7.4924 - accuracy: 0.5114
  384/25000 [..............................] - ETA: 1:14 - loss: 7.5468 - accuracy: 0.5078
  416/25000 [..............................] - ETA: 1:13 - loss: 7.6666 - accuracy: 0.5000
  448/25000 [..............................] - ETA: 1:12 - loss: 7.6666 - accuracy: 0.5000
  480/25000 [..............................] - ETA: 1:11 - loss: 7.6027 - accuracy: 0.5042
  512/25000 [..............................] - ETA: 1:10 - loss: 7.6367 - accuracy: 0.5020
  544/25000 [..............................] - ETA: 1:10 - loss: 7.6948 - accuracy: 0.4982
  576/25000 [..............................] - ETA: 1:09 - loss: 7.6400 - accuracy: 0.5017
  608/25000 [..............................] - ETA: 1:08 - loss: 7.7423 - accuracy: 0.4951
  640/25000 [..............................] - ETA: 1:08 - loss: 7.6666 - accuracy: 0.5000
  672/25000 [..............................] - ETA: 1:07 - loss: 7.6438 - accuracy: 0.5015
  704/25000 [..............................] - ETA: 1:06 - loss: 7.7102 - accuracy: 0.4972
  736/25000 [..............................] - ETA: 1:06 - loss: 7.7291 - accuracy: 0.4959
  768/25000 [..............................] - ETA: 1:05 - loss: 7.7465 - accuracy: 0.4948
  800/25000 [..............................] - ETA: 1:05 - loss: 7.8200 - accuracy: 0.4900
  832/25000 [..............................] - ETA: 1:04 - loss: 7.8693 - accuracy: 0.4868
  864/25000 [>.............................] - ETA: 1:04 - loss: 7.8263 - accuracy: 0.4896
  896/25000 [>.............................] - ETA: 1:04 - loss: 7.7351 - accuracy: 0.4955
  928/25000 [>.............................] - ETA: 1:03 - loss: 7.7492 - accuracy: 0.4946
  960/25000 [>.............................] - ETA: 1:03 - loss: 7.6666 - accuracy: 0.5000
  992/25000 [>.............................] - ETA: 1:02 - loss: 7.6202 - accuracy: 0.5030
 1024/25000 [>.............................] - ETA: 1:02 - loss: 7.6816 - accuracy: 0.4990
 1056/25000 [>.............................] - ETA: 1:02 - loss: 7.6521 - accuracy: 0.5009
 1088/25000 [>.............................] - ETA: 1:02 - loss: 7.6243 - accuracy: 0.5028
 1120/25000 [>.............................] - ETA: 1:02 - loss: 7.5571 - accuracy: 0.5071
 1152/25000 [>.............................] - ETA: 1:01 - loss: 7.5335 - accuracy: 0.5087
 1184/25000 [>.............................] - ETA: 1:01 - loss: 7.5242 - accuracy: 0.5093
 1216/25000 [>.............................] - ETA: 1:01 - loss: 7.5531 - accuracy: 0.5074
 1248/25000 [>.............................] - ETA: 1:00 - loss: 7.5069 - accuracy: 0.5104
 1280/25000 [>.............................] - ETA: 1:00 - loss: 7.4989 - accuracy: 0.5109
 1312/25000 [>.............................] - ETA: 1:00 - loss: 7.5614 - accuracy: 0.5069
 1344/25000 [>.............................] - ETA: 1:00 - loss: 7.5297 - accuracy: 0.5089
 1376/25000 [>.............................] - ETA: 1:00 - loss: 7.5106 - accuracy: 0.5102
 1408/25000 [>.............................] - ETA: 1:00 - loss: 7.4924 - accuracy: 0.5114
 1440/25000 [>.............................] - ETA: 59s - loss: 7.5175 - accuracy: 0.5097 
 1472/25000 [>.............................] - ETA: 59s - loss: 7.4895 - accuracy: 0.5115
 1504/25000 [>.............................] - ETA: 59s - loss: 7.4933 - accuracy: 0.5113
 1536/25000 [>.............................] - ETA: 59s - loss: 7.4470 - accuracy: 0.5143
 1568/25000 [>.............................] - ETA: 58s - loss: 7.4515 - accuracy: 0.5140
 1600/25000 [>.............................] - ETA: 58s - loss: 7.4654 - accuracy: 0.5131
 1632/25000 [>.............................] - ETA: 58s - loss: 7.4881 - accuracy: 0.5116
 1664/25000 [>.............................] - ETA: 58s - loss: 7.4823 - accuracy: 0.5120
 1696/25000 [=>............................] - ETA: 58s - loss: 7.4587 - accuracy: 0.5136
 1728/25000 [=>............................] - ETA: 58s - loss: 7.4448 - accuracy: 0.5145
 1760/25000 [=>............................] - ETA: 58s - loss: 7.3878 - accuracy: 0.5182
 1792/25000 [=>............................] - ETA: 58s - loss: 7.3586 - accuracy: 0.5201
 1824/25000 [=>............................] - ETA: 57s - loss: 7.3640 - accuracy: 0.5197
 1856/25000 [=>............................] - ETA: 57s - loss: 7.3692 - accuracy: 0.5194
 1888/25000 [=>............................] - ETA: 57s - loss: 7.3255 - accuracy: 0.5222
 1920/25000 [=>............................] - ETA: 57s - loss: 7.3392 - accuracy: 0.5214
 1952/25000 [=>............................] - ETA: 57s - loss: 7.3288 - accuracy: 0.5220
 1984/25000 [=>............................] - ETA: 57s - loss: 7.3575 - accuracy: 0.5202
 2016/25000 [=>............................] - ETA: 57s - loss: 7.3396 - accuracy: 0.5213
 2048/25000 [=>............................] - ETA: 57s - loss: 7.3821 - accuracy: 0.5186
 2080/25000 [=>............................] - ETA: 56s - loss: 7.3939 - accuracy: 0.5178
 2112/25000 [=>............................] - ETA: 56s - loss: 7.4053 - accuracy: 0.5170
 2144/25000 [=>............................] - ETA: 56s - loss: 7.4378 - accuracy: 0.5149
 2176/25000 [=>............................] - ETA: 56s - loss: 7.4693 - accuracy: 0.5129
 2208/25000 [=>............................] - ETA: 56s - loss: 7.4930 - accuracy: 0.5113
 2240/25000 [=>............................] - ETA: 56s - loss: 7.4818 - accuracy: 0.5121
 2272/25000 [=>............................] - ETA: 56s - loss: 7.4979 - accuracy: 0.5110
 2304/25000 [=>............................] - ETA: 56s - loss: 7.5002 - accuracy: 0.5109
 2336/25000 [=>............................] - ETA: 55s - loss: 7.5025 - accuracy: 0.5107
 2368/25000 [=>............................] - ETA: 55s - loss: 7.5242 - accuracy: 0.5093
 2400/25000 [=>............................] - ETA: 55s - loss: 7.5197 - accuracy: 0.5096
 2432/25000 [=>............................] - ETA: 55s - loss: 7.5216 - accuracy: 0.5095
 2464/25000 [=>............................] - ETA: 55s - loss: 7.5359 - accuracy: 0.5085
 2496/25000 [=>............................] - ETA: 55s - loss: 7.5376 - accuracy: 0.5084
 2528/25000 [==>...........................] - ETA: 55s - loss: 7.5756 - accuracy: 0.5059
 2560/25000 [==>...........................] - ETA: 55s - loss: 7.5648 - accuracy: 0.5066
 2592/25000 [==>...........................] - ETA: 55s - loss: 7.5542 - accuracy: 0.5073
 2624/25000 [==>...........................] - ETA: 55s - loss: 7.5614 - accuracy: 0.5069
 2656/25000 [==>...........................] - ETA: 55s - loss: 7.5916 - accuracy: 0.5049
 2688/25000 [==>...........................] - ETA: 54s - loss: 7.5982 - accuracy: 0.5045
 2720/25000 [==>...........................] - ETA: 54s - loss: 7.6159 - accuracy: 0.5033
 2752/25000 [==>...........................] - ETA: 54s - loss: 7.6165 - accuracy: 0.5033
 2784/25000 [==>...........................] - ETA: 54s - loss: 7.6226 - accuracy: 0.5029
 2816/25000 [==>...........................] - ETA: 54s - loss: 7.6394 - accuracy: 0.5018
 2848/25000 [==>...........................] - ETA: 54s - loss: 7.6343 - accuracy: 0.5021
 2880/25000 [==>...........................] - ETA: 54s - loss: 7.6400 - accuracy: 0.5017
 2912/25000 [==>...........................] - ETA: 54s - loss: 7.6350 - accuracy: 0.5021
 2944/25000 [==>...........................] - ETA: 54s - loss: 7.6458 - accuracy: 0.5014
 2976/25000 [==>...........................] - ETA: 54s - loss: 7.6409 - accuracy: 0.5017
 3008/25000 [==>...........................] - ETA: 54s - loss: 7.6513 - accuracy: 0.5010
 3040/25000 [==>...........................] - ETA: 54s - loss: 7.6717 - accuracy: 0.4997
 3072/25000 [==>...........................] - ETA: 54s - loss: 7.6716 - accuracy: 0.4997
 3104/25000 [==>...........................] - ETA: 53s - loss: 7.6814 - accuracy: 0.4990
 3136/25000 [==>...........................] - ETA: 53s - loss: 7.6715 - accuracy: 0.4997
 3168/25000 [==>...........................] - ETA: 53s - loss: 7.6618 - accuracy: 0.5003
 3200/25000 [==>...........................] - ETA: 53s - loss: 7.6714 - accuracy: 0.4997
 3232/25000 [==>...........................] - ETA: 53s - loss: 7.6951 - accuracy: 0.4981
 3264/25000 [==>...........................] - ETA: 53s - loss: 7.7042 - accuracy: 0.4975
 3296/25000 [==>...........................] - ETA: 53s - loss: 7.7178 - accuracy: 0.4967
 3328/25000 [==>...........................] - ETA: 53s - loss: 7.7311 - accuracy: 0.4958
 3360/25000 [===>..........................] - ETA: 53s - loss: 7.7123 - accuracy: 0.4970
 3392/25000 [===>..........................] - ETA: 53s - loss: 7.7163 - accuracy: 0.4968
 3424/25000 [===>..........................] - ETA: 53s - loss: 7.7204 - accuracy: 0.4965
 3456/25000 [===>..........................] - ETA: 52s - loss: 7.7154 - accuracy: 0.4968
 3488/25000 [===>..........................] - ETA: 52s - loss: 7.7150 - accuracy: 0.4968
 3520/25000 [===>..........................] - ETA: 52s - loss: 7.7102 - accuracy: 0.4972
 3552/25000 [===>..........................] - ETA: 52s - loss: 7.7012 - accuracy: 0.4977
 3584/25000 [===>..........................] - ETA: 52s - loss: 7.6795 - accuracy: 0.4992
 3616/25000 [===>..........................] - ETA: 52s - loss: 7.6878 - accuracy: 0.4986
 3648/25000 [===>..........................] - ETA: 52s - loss: 7.7171 - accuracy: 0.4967
 3680/25000 [===>..........................] - ETA: 52s - loss: 7.7333 - accuracy: 0.4957
 3712/25000 [===>..........................] - ETA: 52s - loss: 7.7327 - accuracy: 0.4957
 3744/25000 [===>..........................] - ETA: 52s - loss: 7.7240 - accuracy: 0.4963
 3776/25000 [===>..........................] - ETA: 52s - loss: 7.7275 - accuracy: 0.4960
 3808/25000 [===>..........................] - ETA: 51s - loss: 7.7109 - accuracy: 0.4971
 3840/25000 [===>..........................] - ETA: 51s - loss: 7.7145 - accuracy: 0.4969
 3872/25000 [===>..........................] - ETA: 51s - loss: 7.7062 - accuracy: 0.4974
 3904/25000 [===>..........................] - ETA: 51s - loss: 7.7020 - accuracy: 0.4977
 3936/25000 [===>..........................] - ETA: 51s - loss: 7.7017 - accuracy: 0.4977
 3968/25000 [===>..........................] - ETA: 51s - loss: 7.7207 - accuracy: 0.4965
 4000/25000 [===>..........................] - ETA: 51s - loss: 7.7165 - accuracy: 0.4967
 4032/25000 [===>..........................] - ETA: 51s - loss: 7.6894 - accuracy: 0.4985
 4064/25000 [===>..........................] - ETA: 51s - loss: 7.6968 - accuracy: 0.4980
 4096/25000 [===>..........................] - ETA: 51s - loss: 7.7003 - accuracy: 0.4978
 4128/25000 [===>..........................] - ETA: 51s - loss: 7.6778 - accuracy: 0.4993
 4160/25000 [===>..........................] - ETA: 51s - loss: 7.6666 - accuracy: 0.5000
 4192/25000 [====>.........................] - ETA: 51s - loss: 7.6483 - accuracy: 0.5012
 4224/25000 [====>.........................] - ETA: 50s - loss: 7.6448 - accuracy: 0.5014
 4256/25000 [====>.........................] - ETA: 50s - loss: 7.6558 - accuracy: 0.5007
 4288/25000 [====>.........................] - ETA: 50s - loss: 7.6523 - accuracy: 0.5009
 4320/25000 [====>.........................] - ETA: 50s - loss: 7.6666 - accuracy: 0.5000
 4352/25000 [====>.........................] - ETA: 50s - loss: 7.6772 - accuracy: 0.4993
 4384/25000 [====>.........................] - ETA: 50s - loss: 7.6876 - accuracy: 0.4986
 4416/25000 [====>.........................] - ETA: 50s - loss: 7.6736 - accuracy: 0.4995
 4448/25000 [====>.........................] - ETA: 50s - loss: 7.6770 - accuracy: 0.4993
 4480/25000 [====>.........................] - ETA: 50s - loss: 7.6700 - accuracy: 0.4998
 4512/25000 [====>.........................] - ETA: 50s - loss: 7.6564 - accuracy: 0.5007
 4544/25000 [====>.........................] - ETA: 50s - loss: 7.6497 - accuracy: 0.5011
 4576/25000 [====>.........................] - ETA: 49s - loss: 7.6465 - accuracy: 0.5013
 4608/25000 [====>.........................] - ETA: 49s - loss: 7.6500 - accuracy: 0.5011
 4640/25000 [====>.........................] - ETA: 49s - loss: 7.6699 - accuracy: 0.4998
 4672/25000 [====>.........................] - ETA: 49s - loss: 7.6535 - accuracy: 0.5009
 4704/25000 [====>.........................] - ETA: 49s - loss: 7.6438 - accuracy: 0.5015
 4736/25000 [====>.........................] - ETA: 49s - loss: 7.6342 - accuracy: 0.5021
 4768/25000 [====>.........................] - ETA: 49s - loss: 7.6377 - accuracy: 0.5019
 4800/25000 [====>.........................] - ETA: 49s - loss: 7.6251 - accuracy: 0.5027
 4832/25000 [====>.........................] - ETA: 49s - loss: 7.6285 - accuracy: 0.5025
 4864/25000 [====>.........................] - ETA: 49s - loss: 7.6288 - accuracy: 0.5025
 4896/25000 [====>.........................] - ETA: 49s - loss: 7.6259 - accuracy: 0.5027
 4928/25000 [====>.........................] - ETA: 49s - loss: 7.6293 - accuracy: 0.5024
 4960/25000 [====>.........................] - ETA: 49s - loss: 7.6295 - accuracy: 0.5024
 4992/25000 [====>.........................] - ETA: 48s - loss: 7.6205 - accuracy: 0.5030
 5024/25000 [=====>........................] - ETA: 48s - loss: 7.6300 - accuracy: 0.5024
 5056/25000 [=====>........................] - ETA: 48s - loss: 7.6242 - accuracy: 0.5028
 5088/25000 [=====>........................] - ETA: 48s - loss: 7.6124 - accuracy: 0.5035
 5120/25000 [=====>........................] - ETA: 48s - loss: 7.6097 - accuracy: 0.5037
 5152/25000 [=====>........................] - ETA: 48s - loss: 7.6130 - accuracy: 0.5035
 5184/25000 [=====>........................] - ETA: 48s - loss: 7.6075 - accuracy: 0.5039
 5216/25000 [=====>........................] - ETA: 48s - loss: 7.6078 - accuracy: 0.5038
 5248/25000 [=====>........................] - ETA: 48s - loss: 7.6082 - accuracy: 0.5038
 5280/25000 [=====>........................] - ETA: 48s - loss: 7.6056 - accuracy: 0.5040
 5312/25000 [=====>........................] - ETA: 47s - loss: 7.6089 - accuracy: 0.5038
 5344/25000 [=====>........................] - ETA: 47s - loss: 7.6236 - accuracy: 0.5028
 5376/25000 [=====>........................] - ETA: 47s - loss: 7.6096 - accuracy: 0.5037
 5408/25000 [=====>........................] - ETA: 47s - loss: 7.6099 - accuracy: 0.5037
 5440/25000 [=====>........................] - ETA: 47s - loss: 7.5962 - accuracy: 0.5046
 5472/25000 [=====>........................] - ETA: 47s - loss: 7.5798 - accuracy: 0.5057
 5504/25000 [=====>........................] - ETA: 47s - loss: 7.5747 - accuracy: 0.5060
 5536/25000 [=====>........................] - ETA: 47s - loss: 7.5614 - accuracy: 0.5069
 5568/25000 [=====>........................] - ETA: 47s - loss: 7.5620 - accuracy: 0.5068
 5600/25000 [=====>........................] - ETA: 47s - loss: 7.5653 - accuracy: 0.5066
 5632/25000 [=====>........................] - ETA: 47s - loss: 7.5713 - accuracy: 0.5062
 5664/25000 [=====>........................] - ETA: 47s - loss: 7.5692 - accuracy: 0.5064
 5696/25000 [=====>........................] - ETA: 46s - loss: 7.5697 - accuracy: 0.5063
 5728/25000 [=====>........................] - ETA: 46s - loss: 7.5595 - accuracy: 0.5070
 5760/25000 [=====>........................] - ETA: 46s - loss: 7.5681 - accuracy: 0.5064
 5792/25000 [=====>........................] - ETA: 46s - loss: 7.5713 - accuracy: 0.5062
 5824/25000 [=====>........................] - ETA: 46s - loss: 7.5718 - accuracy: 0.5062
 5856/25000 [======>.......................] - ETA: 46s - loss: 7.5750 - accuracy: 0.5060
 5888/25000 [======>.......................] - ETA: 46s - loss: 7.5729 - accuracy: 0.5061
 5920/25000 [======>.......................] - ETA: 46s - loss: 7.5708 - accuracy: 0.5063
 5952/25000 [======>.......................] - ETA: 46s - loss: 7.5687 - accuracy: 0.5064
 5984/25000 [======>.......................] - ETA: 46s - loss: 7.5667 - accuracy: 0.5065
 6016/25000 [======>.......................] - ETA: 46s - loss: 7.5596 - accuracy: 0.5070
 6048/25000 [======>.......................] - ETA: 46s - loss: 7.5627 - accuracy: 0.5068
 6080/25000 [======>.......................] - ETA: 46s - loss: 7.5607 - accuracy: 0.5069
 6112/25000 [======>.......................] - ETA: 45s - loss: 7.5663 - accuracy: 0.5065
 6144/25000 [======>.......................] - ETA: 45s - loss: 7.5718 - accuracy: 0.5062
 6176/25000 [======>.......................] - ETA: 45s - loss: 7.5698 - accuracy: 0.5063
 6208/25000 [======>.......................] - ETA: 45s - loss: 7.5728 - accuracy: 0.5061
 6240/25000 [======>.......................] - ETA: 45s - loss: 7.5683 - accuracy: 0.5064
 6272/25000 [======>.......................] - ETA: 45s - loss: 7.5688 - accuracy: 0.5064
 6304/25000 [======>.......................] - ETA: 45s - loss: 7.5693 - accuracy: 0.5063
 6336/25000 [======>.......................] - ETA: 45s - loss: 7.5601 - accuracy: 0.5069
 6368/25000 [======>.......................] - ETA: 45s - loss: 7.5535 - accuracy: 0.5074
 6400/25000 [======>.......................] - ETA: 45s - loss: 7.5588 - accuracy: 0.5070
 6432/25000 [======>.......................] - ETA: 45s - loss: 7.5689 - accuracy: 0.5064
 6464/25000 [======>.......................] - ETA: 45s - loss: 7.5694 - accuracy: 0.5063
 6496/25000 [======>.......................] - ETA: 44s - loss: 7.5628 - accuracy: 0.5068
 6528/25000 [======>.......................] - ETA: 44s - loss: 7.5633 - accuracy: 0.5067
 6560/25000 [======>.......................] - ETA: 44s - loss: 7.5591 - accuracy: 0.5070
 6592/25000 [======>.......................] - ETA: 44s - loss: 7.5619 - accuracy: 0.5068
 6624/25000 [======>.......................] - ETA: 44s - loss: 7.5740 - accuracy: 0.5060
 6656/25000 [======>.......................] - ETA: 44s - loss: 7.5768 - accuracy: 0.5059
 6688/25000 [=======>......................] - ETA: 44s - loss: 7.5726 - accuracy: 0.5061
 6720/25000 [=======>......................] - ETA: 44s - loss: 7.5571 - accuracy: 0.5071
 6752/25000 [=======>......................] - ETA: 44s - loss: 7.5667 - accuracy: 0.5065
 6784/25000 [=======>......................] - ETA: 44s - loss: 7.5785 - accuracy: 0.5057
 6816/25000 [=======>......................] - ETA: 44s - loss: 7.5766 - accuracy: 0.5059
 6848/25000 [=======>......................] - ETA: 44s - loss: 7.5771 - accuracy: 0.5058
 6880/25000 [=======>......................] - ETA: 44s - loss: 7.5819 - accuracy: 0.5055
 6912/25000 [=======>......................] - ETA: 43s - loss: 7.5801 - accuracy: 0.5056
 6944/25000 [=======>......................] - ETA: 43s - loss: 7.5805 - accuracy: 0.5056
 6976/25000 [=======>......................] - ETA: 43s - loss: 7.5897 - accuracy: 0.5050
 7008/25000 [=======>......................] - ETA: 43s - loss: 7.5857 - accuracy: 0.5053
 7040/25000 [=======>......................] - ETA: 43s - loss: 7.5904 - accuracy: 0.5050
 7072/25000 [=======>......................] - ETA: 43s - loss: 7.5929 - accuracy: 0.5048
 7104/25000 [=======>......................] - ETA: 43s - loss: 7.5781 - accuracy: 0.5058
 7136/25000 [=======>......................] - ETA: 43s - loss: 7.5678 - accuracy: 0.5064
 7168/25000 [=======>......................] - ETA: 43s - loss: 7.5682 - accuracy: 0.5064
 7200/25000 [=======>......................] - ETA: 43s - loss: 7.5750 - accuracy: 0.5060
 7232/25000 [=======>......................] - ETA: 43s - loss: 7.5712 - accuracy: 0.5062
 7264/25000 [=======>......................] - ETA: 43s - loss: 7.5653 - accuracy: 0.5066
 7296/25000 [=======>......................] - ETA: 42s - loss: 7.5594 - accuracy: 0.5070
 7328/25000 [=======>......................] - ETA: 42s - loss: 7.5578 - accuracy: 0.5071
 7360/25000 [=======>......................] - ETA: 42s - loss: 7.5583 - accuracy: 0.5071
 7392/25000 [=======>......................] - ETA: 42s - loss: 7.5671 - accuracy: 0.5065
 7424/25000 [=======>......................] - ETA: 42s - loss: 7.5592 - accuracy: 0.5070
 7456/25000 [=======>......................] - ETA: 42s - loss: 7.5617 - accuracy: 0.5068
 7488/25000 [=======>......................] - ETA: 42s - loss: 7.5683 - accuracy: 0.5064
 7520/25000 [========>.....................] - ETA: 42s - loss: 7.5728 - accuracy: 0.5061
 7552/25000 [========>.....................] - ETA: 42s - loss: 7.5610 - accuracy: 0.5069
 7584/25000 [========>.....................] - ETA: 42s - loss: 7.5494 - accuracy: 0.5076
 7616/25000 [========>.....................] - ETA: 42s - loss: 7.5458 - accuracy: 0.5079
 7648/25000 [========>.....................] - ETA: 42s - loss: 7.5523 - accuracy: 0.5075
 7680/25000 [========>.....................] - ETA: 42s - loss: 7.5468 - accuracy: 0.5078
 7712/25000 [========>.....................] - ETA: 41s - loss: 7.5433 - accuracy: 0.5080
 7744/25000 [========>.....................] - ETA: 41s - loss: 7.5399 - accuracy: 0.5083
 7776/25000 [========>.....................] - ETA: 41s - loss: 7.5424 - accuracy: 0.5081
 7808/25000 [========>.....................] - ETA: 41s - loss: 7.5390 - accuracy: 0.5083
 7840/25000 [========>.....................] - ETA: 41s - loss: 7.5434 - accuracy: 0.5080
 7872/25000 [========>.....................] - ETA: 41s - loss: 7.5459 - accuracy: 0.5079
 7904/25000 [========>.....................] - ETA: 41s - loss: 7.5444 - accuracy: 0.5080
 7936/25000 [========>.....................] - ETA: 41s - loss: 7.5468 - accuracy: 0.5078
 7968/25000 [========>.....................] - ETA: 41s - loss: 7.5531 - accuracy: 0.5074
 8000/25000 [========>.....................] - ETA: 41s - loss: 7.5478 - accuracy: 0.5077
 8032/25000 [========>.....................] - ETA: 41s - loss: 7.5464 - accuracy: 0.5078
 8064/25000 [========>.....................] - ETA: 41s - loss: 7.5487 - accuracy: 0.5077
 8096/25000 [========>.....................] - ETA: 41s - loss: 7.5492 - accuracy: 0.5077
 8128/25000 [========>.....................] - ETA: 40s - loss: 7.5534 - accuracy: 0.5074
 8160/25000 [========>.....................] - ETA: 40s - loss: 7.5614 - accuracy: 0.5069
 8192/25000 [========>.....................] - ETA: 40s - loss: 7.5562 - accuracy: 0.5072
 8224/25000 [========>.....................] - ETA: 40s - loss: 7.5529 - accuracy: 0.5074
 8256/25000 [========>.....................] - ETA: 40s - loss: 7.5552 - accuracy: 0.5073
 8288/25000 [========>.....................] - ETA: 40s - loss: 7.5575 - accuracy: 0.5071
 8320/25000 [========>.....................] - ETA: 40s - loss: 7.5597 - accuracy: 0.5070
 8352/25000 [=========>....................] - ETA: 40s - loss: 7.5675 - accuracy: 0.5065
 8384/25000 [=========>....................] - ETA: 40s - loss: 7.5752 - accuracy: 0.5060
 8416/25000 [=========>....................] - ETA: 40s - loss: 7.5792 - accuracy: 0.5057
 8448/25000 [=========>....................] - ETA: 40s - loss: 7.5741 - accuracy: 0.5060
 8480/25000 [=========>....................] - ETA: 40s - loss: 7.5744 - accuracy: 0.5060
 8512/25000 [=========>....................] - ETA: 40s - loss: 7.5784 - accuracy: 0.5058
 8544/25000 [=========>....................] - ETA: 39s - loss: 7.5877 - accuracy: 0.5051
 8576/25000 [=========>....................] - ETA: 39s - loss: 7.5897 - accuracy: 0.5050
 8608/25000 [=========>....................] - ETA: 39s - loss: 7.5882 - accuracy: 0.5051
 8640/25000 [=========>....................] - ETA: 39s - loss: 7.5850 - accuracy: 0.5053
 8672/25000 [=========>....................] - ETA: 39s - loss: 7.5888 - accuracy: 0.5051
 8704/25000 [=========>....................] - ETA: 39s - loss: 7.5891 - accuracy: 0.5051
 8736/25000 [=========>....................] - ETA: 39s - loss: 7.5929 - accuracy: 0.5048
 8768/25000 [=========>....................] - ETA: 39s - loss: 7.5932 - accuracy: 0.5048
 8800/25000 [=========>....................] - ETA: 39s - loss: 7.5865 - accuracy: 0.5052
 8832/25000 [=========>....................] - ETA: 39s - loss: 7.5850 - accuracy: 0.5053
 8864/25000 [=========>....................] - ETA: 39s - loss: 7.5905 - accuracy: 0.5050
 8896/25000 [=========>....................] - ETA: 39s - loss: 7.5942 - accuracy: 0.5047
 8928/25000 [=========>....................] - ETA: 39s - loss: 7.5979 - accuracy: 0.5045
 8960/25000 [=========>....................] - ETA: 38s - loss: 7.5965 - accuracy: 0.5046
 8992/25000 [=========>....................] - ETA: 38s - loss: 7.5950 - accuracy: 0.5047
 9024/25000 [=========>....................] - ETA: 38s - loss: 7.5851 - accuracy: 0.5053
 9056/25000 [=========>....................] - ETA: 38s - loss: 7.5870 - accuracy: 0.5052
 9088/25000 [=========>....................] - ETA: 38s - loss: 7.5924 - accuracy: 0.5048
 9120/25000 [=========>....................] - ETA: 38s - loss: 7.5926 - accuracy: 0.5048
 9152/25000 [=========>....................] - ETA: 38s - loss: 7.5912 - accuracy: 0.5049
 9184/25000 [==========>...................] - ETA: 38s - loss: 7.5882 - accuracy: 0.5051
 9216/25000 [==========>...................] - ETA: 38s - loss: 7.5967 - accuracy: 0.5046
 9248/25000 [==========>...................] - ETA: 38s - loss: 7.6036 - accuracy: 0.5041
 9280/25000 [==========>...................] - ETA: 38s - loss: 7.5989 - accuracy: 0.5044
 9312/25000 [==========>...................] - ETA: 38s - loss: 7.5942 - accuracy: 0.5047
 9344/25000 [==========>...................] - ETA: 37s - loss: 7.5928 - accuracy: 0.5048
 9376/25000 [==========>...................] - ETA: 37s - loss: 7.5979 - accuracy: 0.5045
 9408/25000 [==========>...................] - ETA: 37s - loss: 7.5998 - accuracy: 0.5044
 9440/25000 [==========>...................] - ETA: 37s - loss: 7.6049 - accuracy: 0.5040
 9472/25000 [==========>...................] - ETA: 37s - loss: 7.6035 - accuracy: 0.5041
 9504/25000 [==========>...................] - ETA: 37s - loss: 7.6021 - accuracy: 0.5042
 9536/25000 [==========>...................] - ETA: 37s - loss: 7.6055 - accuracy: 0.5040
 9568/25000 [==========>...................] - ETA: 37s - loss: 7.6025 - accuracy: 0.5042
 9600/25000 [==========>...................] - ETA: 37s - loss: 7.6059 - accuracy: 0.5040
 9632/25000 [==========>...................] - ETA: 37s - loss: 7.6125 - accuracy: 0.5035
 9664/25000 [==========>...................] - ETA: 37s - loss: 7.6174 - accuracy: 0.5032
 9696/25000 [==========>...................] - ETA: 37s - loss: 7.6129 - accuracy: 0.5035
 9728/25000 [==========>...................] - ETA: 37s - loss: 7.6130 - accuracy: 0.5035
 9760/25000 [==========>...................] - ETA: 36s - loss: 7.6211 - accuracy: 0.5030
 9792/25000 [==========>...................] - ETA: 36s - loss: 7.6290 - accuracy: 0.5025
 9824/25000 [==========>...................] - ETA: 36s - loss: 7.6323 - accuracy: 0.5022
 9856/25000 [==========>...................] - ETA: 36s - loss: 7.6308 - accuracy: 0.5023
 9888/25000 [==========>...................] - ETA: 36s - loss: 7.6325 - accuracy: 0.5022
 9920/25000 [==========>...................] - ETA: 36s - loss: 7.6357 - accuracy: 0.5020
 9952/25000 [==========>...................] - ETA: 36s - loss: 7.6327 - accuracy: 0.5022
 9984/25000 [==========>...................] - ETA: 36s - loss: 7.6298 - accuracy: 0.5024
10016/25000 [===========>..................] - ETA: 36s - loss: 7.6345 - accuracy: 0.5021
10048/25000 [===========>..................] - ETA: 36s - loss: 7.6346 - accuracy: 0.5021
10080/25000 [===========>..................] - ETA: 36s - loss: 7.6332 - accuracy: 0.5022
10112/25000 [===========>..................] - ETA: 36s - loss: 7.6302 - accuracy: 0.5024
10144/25000 [===========>..................] - ETA: 36s - loss: 7.6334 - accuracy: 0.5022
10176/25000 [===========>..................] - ETA: 35s - loss: 7.6335 - accuracy: 0.5022
10208/25000 [===========>..................] - ETA: 35s - loss: 7.6381 - accuracy: 0.5019
10240/25000 [===========>..................] - ETA: 35s - loss: 7.6352 - accuracy: 0.5021
10272/25000 [===========>..................] - ETA: 35s - loss: 7.6323 - accuracy: 0.5022
10304/25000 [===========>..................] - ETA: 35s - loss: 7.6354 - accuracy: 0.5020
10336/25000 [===========>..................] - ETA: 35s - loss: 7.6369 - accuracy: 0.5019
10368/25000 [===========>..................] - ETA: 35s - loss: 7.6400 - accuracy: 0.5017
10400/25000 [===========>..................] - ETA: 35s - loss: 7.6401 - accuracy: 0.5017
10432/25000 [===========>..................] - ETA: 35s - loss: 7.6358 - accuracy: 0.5020
10464/25000 [===========>..................] - ETA: 35s - loss: 7.6285 - accuracy: 0.5025
10496/25000 [===========>..................] - ETA: 35s - loss: 7.6330 - accuracy: 0.5022
10528/25000 [===========>..................] - ETA: 35s - loss: 7.6317 - accuracy: 0.5023
10560/25000 [===========>..................] - ETA: 35s - loss: 7.6332 - accuracy: 0.5022
10592/25000 [===========>..................] - ETA: 35s - loss: 7.6333 - accuracy: 0.5022
10624/25000 [===========>..................] - ETA: 34s - loss: 7.6349 - accuracy: 0.5021
10656/25000 [===========>..................] - ETA: 34s - loss: 7.6321 - accuracy: 0.5023
10688/25000 [===========>..................] - ETA: 34s - loss: 7.6308 - accuracy: 0.5023
10720/25000 [===========>..................] - ETA: 34s - loss: 7.6366 - accuracy: 0.5020
10752/25000 [===========>..................] - ETA: 34s - loss: 7.6295 - accuracy: 0.5024
10784/25000 [===========>..................] - ETA: 34s - loss: 7.6311 - accuracy: 0.5023
10816/25000 [===========>..................] - ETA: 34s - loss: 7.6298 - accuracy: 0.5024
10848/25000 [============>.................] - ETA: 34s - loss: 7.6270 - accuracy: 0.5026
10880/25000 [============>.................] - ETA: 34s - loss: 7.6342 - accuracy: 0.5021
10912/25000 [============>.................] - ETA: 34s - loss: 7.6357 - accuracy: 0.5020
10944/25000 [============>.................] - ETA: 34s - loss: 7.6344 - accuracy: 0.5021
10976/25000 [============>.................] - ETA: 34s - loss: 7.6387 - accuracy: 0.5018
11008/25000 [============>.................] - ETA: 34s - loss: 7.6388 - accuracy: 0.5018
11040/25000 [============>.................] - ETA: 33s - loss: 7.6375 - accuracy: 0.5019
11072/25000 [============>.................] - ETA: 33s - loss: 7.6320 - accuracy: 0.5023
11104/25000 [============>.................] - ETA: 33s - loss: 7.6349 - accuracy: 0.5021
11136/25000 [============>.................] - ETA: 33s - loss: 7.6377 - accuracy: 0.5019
11168/25000 [============>.................] - ETA: 33s - loss: 7.6378 - accuracy: 0.5019
11200/25000 [============>.................] - ETA: 33s - loss: 7.6447 - accuracy: 0.5014
11232/25000 [============>.................] - ETA: 33s - loss: 7.6475 - accuracy: 0.5012
11264/25000 [============>.................] - ETA: 33s - loss: 7.6462 - accuracy: 0.5013
11296/25000 [============>.................] - ETA: 33s - loss: 7.6435 - accuracy: 0.5015
11328/25000 [============>.................] - ETA: 33s - loss: 7.6436 - accuracy: 0.5015
11360/25000 [============>.................] - ETA: 33s - loss: 7.6464 - accuracy: 0.5013
11392/25000 [============>.................] - ETA: 33s - loss: 7.6424 - accuracy: 0.5016
11424/25000 [============>.................] - ETA: 33s - loss: 7.6425 - accuracy: 0.5016
11456/25000 [============>.................] - ETA: 32s - loss: 7.6479 - accuracy: 0.5012
11488/25000 [============>.................] - ETA: 32s - loss: 7.6479 - accuracy: 0.5012
11520/25000 [============>.................] - ETA: 32s - loss: 7.6506 - accuracy: 0.5010
11552/25000 [============>.................] - ETA: 32s - loss: 7.6454 - accuracy: 0.5014
11584/25000 [============>.................] - ETA: 32s - loss: 7.6494 - accuracy: 0.5011
11616/25000 [============>.................] - ETA: 32s - loss: 7.6521 - accuracy: 0.5009
11648/25000 [============>.................] - ETA: 32s - loss: 7.6442 - accuracy: 0.5015
11680/25000 [=============>................] - ETA: 32s - loss: 7.6430 - accuracy: 0.5015
11712/25000 [=============>................] - ETA: 32s - loss: 7.6417 - accuracy: 0.5016
11744/25000 [=============>................] - ETA: 32s - loss: 7.6392 - accuracy: 0.5018
11776/25000 [=============>................] - ETA: 32s - loss: 7.6380 - accuracy: 0.5019
11808/25000 [=============>................] - ETA: 32s - loss: 7.6342 - accuracy: 0.5021
11840/25000 [=============>................] - ETA: 32s - loss: 7.6368 - accuracy: 0.5019
11872/25000 [=============>................] - ETA: 32s - loss: 7.6447 - accuracy: 0.5014
11904/25000 [=============>................] - ETA: 31s - loss: 7.6434 - accuracy: 0.5015
11936/25000 [=============>................] - ETA: 31s - loss: 7.6461 - accuracy: 0.5013
11968/25000 [=============>................] - ETA: 31s - loss: 7.6500 - accuracy: 0.5011
12000/25000 [=============>................] - ETA: 31s - loss: 7.6487 - accuracy: 0.5012
12032/25000 [=============>................] - ETA: 31s - loss: 7.6475 - accuracy: 0.5012
12064/25000 [=============>................] - ETA: 31s - loss: 7.6476 - accuracy: 0.5012
12096/25000 [=============>................] - ETA: 31s - loss: 7.6489 - accuracy: 0.5012
12128/25000 [=============>................] - ETA: 31s - loss: 7.6477 - accuracy: 0.5012
12160/25000 [=============>................] - ETA: 31s - loss: 7.6490 - accuracy: 0.5012
12192/25000 [=============>................] - ETA: 31s - loss: 7.6515 - accuracy: 0.5010
12224/25000 [=============>................] - ETA: 31s - loss: 7.6541 - accuracy: 0.5008
12256/25000 [=============>................] - ETA: 31s - loss: 7.6541 - accuracy: 0.5008
12288/25000 [=============>................] - ETA: 30s - loss: 7.6541 - accuracy: 0.5008
12320/25000 [=============>................] - ETA: 30s - loss: 7.6492 - accuracy: 0.5011
12352/25000 [=============>................] - ETA: 30s - loss: 7.6505 - accuracy: 0.5011
12384/25000 [=============>................] - ETA: 30s - loss: 7.6493 - accuracy: 0.5011
12416/25000 [=============>................] - ETA: 30s - loss: 7.6543 - accuracy: 0.5008
12448/25000 [=============>................] - ETA: 30s - loss: 7.6543 - accuracy: 0.5008
12480/25000 [=============>................] - ETA: 30s - loss: 7.6543 - accuracy: 0.5008
12512/25000 [==============>...............] - ETA: 30s - loss: 7.6568 - accuracy: 0.5006
12544/25000 [==============>...............] - ETA: 30s - loss: 7.6556 - accuracy: 0.5007
12576/25000 [==============>...............] - ETA: 30s - loss: 7.6556 - accuracy: 0.5007
12608/25000 [==============>...............] - ETA: 30s - loss: 7.6569 - accuracy: 0.5006
12640/25000 [==============>...............] - ETA: 30s - loss: 7.6545 - accuracy: 0.5008
12672/25000 [==============>...............] - ETA: 30s - loss: 7.6569 - accuracy: 0.5006
12704/25000 [==============>...............] - ETA: 30s - loss: 7.6558 - accuracy: 0.5007
12736/25000 [==============>...............] - ETA: 29s - loss: 7.6558 - accuracy: 0.5007
12768/25000 [==============>...............] - ETA: 29s - loss: 7.6582 - accuracy: 0.5005
12800/25000 [==============>...............] - ETA: 29s - loss: 7.6546 - accuracy: 0.5008
12832/25000 [==============>...............] - ETA: 29s - loss: 7.6523 - accuracy: 0.5009
12864/25000 [==============>...............] - ETA: 29s - loss: 7.6535 - accuracy: 0.5009
12896/25000 [==============>...............] - ETA: 29s - loss: 7.6512 - accuracy: 0.5010
12928/25000 [==============>...............] - ETA: 29s - loss: 7.6488 - accuracy: 0.5012
12960/25000 [==============>...............] - ETA: 29s - loss: 7.6501 - accuracy: 0.5011
12992/25000 [==============>...............] - ETA: 29s - loss: 7.6525 - accuracy: 0.5009
13024/25000 [==============>...............] - ETA: 29s - loss: 7.6478 - accuracy: 0.5012
13056/25000 [==============>...............] - ETA: 29s - loss: 7.6467 - accuracy: 0.5013
13088/25000 [==============>...............] - ETA: 29s - loss: 7.6514 - accuracy: 0.5010
13120/25000 [==============>...............] - ETA: 28s - loss: 7.6503 - accuracy: 0.5011
13152/25000 [==============>...............] - ETA: 28s - loss: 7.6491 - accuracy: 0.5011
13184/25000 [==============>...............] - ETA: 28s - loss: 7.6527 - accuracy: 0.5009
13216/25000 [==============>...............] - ETA: 28s - loss: 7.6504 - accuracy: 0.5011
13248/25000 [==============>...............] - ETA: 28s - loss: 7.6504 - accuracy: 0.5011
13280/25000 [==============>...............] - ETA: 28s - loss: 7.6470 - accuracy: 0.5013
13312/25000 [==============>...............] - ETA: 28s - loss: 7.6505 - accuracy: 0.5011
13344/25000 [===============>..............] - ETA: 28s - loss: 7.6482 - accuracy: 0.5012
13376/25000 [===============>..............] - ETA: 28s - loss: 7.6425 - accuracy: 0.5016
13408/25000 [===============>..............] - ETA: 28s - loss: 7.6426 - accuracy: 0.5016
13440/25000 [===============>..............] - ETA: 28s - loss: 7.6415 - accuracy: 0.5016
13472/25000 [===============>..............] - ETA: 28s - loss: 7.6416 - accuracy: 0.5016
13504/25000 [===============>..............] - ETA: 28s - loss: 7.6394 - accuracy: 0.5018
13536/25000 [===============>..............] - ETA: 27s - loss: 7.6394 - accuracy: 0.5018
13568/25000 [===============>..............] - ETA: 27s - loss: 7.6440 - accuracy: 0.5015
13600/25000 [===============>..............] - ETA: 27s - loss: 7.6452 - accuracy: 0.5014
13632/25000 [===============>..............] - ETA: 27s - loss: 7.6419 - accuracy: 0.5016
13664/25000 [===============>..............] - ETA: 27s - loss: 7.6475 - accuracy: 0.5012
13696/25000 [===============>..............] - ETA: 27s - loss: 7.6453 - accuracy: 0.5014
13728/25000 [===============>..............] - ETA: 27s - loss: 7.6487 - accuracy: 0.5012
13760/25000 [===============>..............] - ETA: 27s - loss: 7.6488 - accuracy: 0.5012
13792/25000 [===============>..............] - ETA: 27s - loss: 7.6488 - accuracy: 0.5012
13824/25000 [===============>..............] - ETA: 27s - loss: 7.6455 - accuracy: 0.5014
13856/25000 [===============>..............] - ETA: 27s - loss: 7.6456 - accuracy: 0.5014
13888/25000 [===============>..............] - ETA: 27s - loss: 7.6467 - accuracy: 0.5013
13920/25000 [===============>..............] - ETA: 27s - loss: 7.6457 - accuracy: 0.5014
13952/25000 [===============>..............] - ETA: 26s - loss: 7.6435 - accuracy: 0.5015
13984/25000 [===============>..............] - ETA: 26s - loss: 7.6436 - accuracy: 0.5015
14016/25000 [===============>..............] - ETA: 26s - loss: 7.6436 - accuracy: 0.5015
14048/25000 [===============>..............] - ETA: 26s - loss: 7.6459 - accuracy: 0.5014
14080/25000 [===============>..............] - ETA: 26s - loss: 7.6394 - accuracy: 0.5018
14112/25000 [===============>..............] - ETA: 26s - loss: 7.6373 - accuracy: 0.5019
14144/25000 [===============>..............] - ETA: 26s - loss: 7.6363 - accuracy: 0.5020
14176/25000 [================>.............] - ETA: 26s - loss: 7.6407 - accuracy: 0.5017
14208/25000 [================>.............] - ETA: 26s - loss: 7.6429 - accuracy: 0.5015
14240/25000 [================>.............] - ETA: 26s - loss: 7.6440 - accuracy: 0.5015
14272/25000 [================>.............] - ETA: 26s - loss: 7.6462 - accuracy: 0.5013
14304/25000 [================>.............] - ETA: 26s - loss: 7.6463 - accuracy: 0.5013
14336/25000 [================>.............] - ETA: 26s - loss: 7.6484 - accuracy: 0.5012
14368/25000 [================>.............] - ETA: 25s - loss: 7.6527 - accuracy: 0.5009
14400/25000 [================>.............] - ETA: 25s - loss: 7.6560 - accuracy: 0.5007
14432/25000 [================>.............] - ETA: 25s - loss: 7.6560 - accuracy: 0.5007
14464/25000 [================>.............] - ETA: 25s - loss: 7.6624 - accuracy: 0.5003
14496/25000 [================>.............] - ETA: 25s - loss: 7.6634 - accuracy: 0.5002
14528/25000 [================>.............] - ETA: 25s - loss: 7.6592 - accuracy: 0.5005
14560/25000 [================>.............] - ETA: 25s - loss: 7.6561 - accuracy: 0.5007
14592/25000 [================>.............] - ETA: 25s - loss: 7.6530 - accuracy: 0.5009
14624/25000 [================>.............] - ETA: 25s - loss: 7.6561 - accuracy: 0.5007
14656/25000 [================>.............] - ETA: 25s - loss: 7.6530 - accuracy: 0.5009
14688/25000 [================>.............] - ETA: 25s - loss: 7.6468 - accuracy: 0.5013
14720/25000 [================>.............] - ETA: 25s - loss: 7.6427 - accuracy: 0.5016
14752/25000 [================>.............] - ETA: 25s - loss: 7.6406 - accuracy: 0.5017
14784/25000 [================>.............] - ETA: 24s - loss: 7.6407 - accuracy: 0.5017
14816/25000 [================>.............] - ETA: 24s - loss: 7.6407 - accuracy: 0.5017
14848/25000 [================>.............] - ETA: 24s - loss: 7.6449 - accuracy: 0.5014
14880/25000 [================>.............] - ETA: 24s - loss: 7.6470 - accuracy: 0.5013
14912/25000 [================>.............] - ETA: 24s - loss: 7.6471 - accuracy: 0.5013
14944/25000 [================>.............] - ETA: 24s - loss: 7.6451 - accuracy: 0.5014
14976/25000 [================>.............] - ETA: 24s - loss: 7.6451 - accuracy: 0.5014
15008/25000 [=================>............] - ETA: 24s - loss: 7.6482 - accuracy: 0.5012
15040/25000 [=================>............] - ETA: 24s - loss: 7.6503 - accuracy: 0.5011
15072/25000 [=================>............] - ETA: 24s - loss: 7.6514 - accuracy: 0.5010
15104/25000 [=================>............] - ETA: 24s - loss: 7.6524 - accuracy: 0.5009
15136/25000 [=================>............] - ETA: 24s - loss: 7.6565 - accuracy: 0.5007
15168/25000 [=================>............] - ETA: 24s - loss: 7.6585 - accuracy: 0.5005
15200/25000 [=================>............] - ETA: 23s - loss: 7.6616 - accuracy: 0.5003
15232/25000 [=================>............] - ETA: 23s - loss: 7.6636 - accuracy: 0.5002
15264/25000 [=================>............] - ETA: 23s - loss: 7.6666 - accuracy: 0.5000
15296/25000 [=================>............] - ETA: 23s - loss: 7.6676 - accuracy: 0.4999
15328/25000 [=================>............] - ETA: 23s - loss: 7.6626 - accuracy: 0.5003
15360/25000 [=================>............] - ETA: 23s - loss: 7.6616 - accuracy: 0.5003
15392/25000 [=================>............] - ETA: 23s - loss: 7.6596 - accuracy: 0.5005
15424/25000 [=================>............] - ETA: 23s - loss: 7.6616 - accuracy: 0.5003
15456/25000 [=================>............] - ETA: 23s - loss: 7.6597 - accuracy: 0.5005
15488/25000 [=================>............] - ETA: 23s - loss: 7.6587 - accuracy: 0.5005
15520/25000 [=================>............] - ETA: 23s - loss: 7.6587 - accuracy: 0.5005
15552/25000 [=================>............] - ETA: 23s - loss: 7.6627 - accuracy: 0.5003
15584/25000 [=================>............] - ETA: 23s - loss: 7.6666 - accuracy: 0.5000
15616/25000 [=================>............] - ETA: 22s - loss: 7.6656 - accuracy: 0.5001
15648/25000 [=================>............] - ETA: 22s - loss: 7.6686 - accuracy: 0.4999
15680/25000 [=================>............] - ETA: 22s - loss: 7.6725 - accuracy: 0.4996
15712/25000 [=================>............] - ETA: 22s - loss: 7.6725 - accuracy: 0.4996
15744/25000 [=================>............] - ETA: 22s - loss: 7.6725 - accuracy: 0.4996
15776/25000 [=================>............] - ETA: 22s - loss: 7.6744 - accuracy: 0.4995
15808/25000 [=================>............] - ETA: 22s - loss: 7.6724 - accuracy: 0.4996
15840/25000 [==================>...........] - ETA: 22s - loss: 7.6753 - accuracy: 0.4994
15872/25000 [==================>...........] - ETA: 22s - loss: 7.6801 - accuracy: 0.4991
15904/25000 [==================>...........] - ETA: 22s - loss: 7.6792 - accuracy: 0.4992
15936/25000 [==================>...........] - ETA: 22s - loss: 7.6772 - accuracy: 0.4993
15968/25000 [==================>...........] - ETA: 22s - loss: 7.6743 - accuracy: 0.4995
16000/25000 [==================>...........] - ETA: 22s - loss: 7.6743 - accuracy: 0.4995
16032/25000 [==================>...........] - ETA: 21s - loss: 7.6771 - accuracy: 0.4993
16064/25000 [==================>...........] - ETA: 21s - loss: 7.6790 - accuracy: 0.4992
16096/25000 [==================>...........] - ETA: 21s - loss: 7.6781 - accuracy: 0.4993
16128/25000 [==================>...........] - ETA: 21s - loss: 7.6818 - accuracy: 0.4990
16160/25000 [==================>...........] - ETA: 21s - loss: 7.6809 - accuracy: 0.4991
16192/25000 [==================>...........] - ETA: 21s - loss: 7.6818 - accuracy: 0.4990
16224/25000 [==================>...........] - ETA: 21s - loss: 7.6808 - accuracy: 0.4991
16256/25000 [==================>...........] - ETA: 21s - loss: 7.6808 - accuracy: 0.4991
16288/25000 [==================>...........] - ETA: 21s - loss: 7.6817 - accuracy: 0.4990
16320/25000 [==================>...........] - ETA: 21s - loss: 7.6817 - accuracy: 0.4990
16352/25000 [==================>...........] - ETA: 21s - loss: 7.6797 - accuracy: 0.4991
16384/25000 [==================>...........] - ETA: 21s - loss: 7.6788 - accuracy: 0.4992
16416/25000 [==================>...........] - ETA: 21s - loss: 7.6788 - accuracy: 0.4992
16448/25000 [==================>...........] - ETA: 20s - loss: 7.6769 - accuracy: 0.4993
16480/25000 [==================>...........] - ETA: 20s - loss: 7.6806 - accuracy: 0.4991
16512/25000 [==================>...........] - ETA: 20s - loss: 7.6778 - accuracy: 0.4993
16544/25000 [==================>...........] - ETA: 20s - loss: 7.6731 - accuracy: 0.4996
16576/25000 [==================>...........] - ETA: 20s - loss: 7.6703 - accuracy: 0.4998
16608/25000 [==================>...........] - ETA: 20s - loss: 7.6648 - accuracy: 0.5001
16640/25000 [==================>...........] - ETA: 20s - loss: 7.6666 - accuracy: 0.5000
16672/25000 [===================>..........] - ETA: 20s - loss: 7.6685 - accuracy: 0.4999
16704/25000 [===================>..........] - ETA: 20s - loss: 7.6703 - accuracy: 0.4998
16736/25000 [===================>..........] - ETA: 20s - loss: 7.6675 - accuracy: 0.4999
16768/25000 [===================>..........] - ETA: 20s - loss: 7.6703 - accuracy: 0.4998
16800/25000 [===================>..........] - ETA: 20s - loss: 7.6712 - accuracy: 0.4997
16832/25000 [===================>..........] - ETA: 19s - loss: 7.6703 - accuracy: 0.4998
16864/25000 [===================>..........] - ETA: 19s - loss: 7.6703 - accuracy: 0.4998
16896/25000 [===================>..........] - ETA: 19s - loss: 7.6748 - accuracy: 0.4995
16928/25000 [===================>..........] - ETA: 19s - loss: 7.6721 - accuracy: 0.4996
16960/25000 [===================>..........] - ETA: 19s - loss: 7.6702 - accuracy: 0.4998
16992/25000 [===================>..........] - ETA: 19s - loss: 7.6684 - accuracy: 0.4999
17024/25000 [===================>..........] - ETA: 19s - loss: 7.6684 - accuracy: 0.4999
17056/25000 [===================>..........] - ETA: 19s - loss: 7.6720 - accuracy: 0.4996
17088/25000 [===================>..........] - ETA: 19s - loss: 7.6675 - accuracy: 0.4999
17120/25000 [===================>..........] - ETA: 19s - loss: 7.6639 - accuracy: 0.5002
17152/25000 [===================>..........] - ETA: 19s - loss: 7.6648 - accuracy: 0.5001
17184/25000 [===================>..........] - ETA: 19s - loss: 7.6675 - accuracy: 0.4999
17216/25000 [===================>..........] - ETA: 19s - loss: 7.6684 - accuracy: 0.4999
17248/25000 [===================>..........] - ETA: 18s - loss: 7.6711 - accuracy: 0.4997
17280/25000 [===================>..........] - ETA: 18s - loss: 7.6755 - accuracy: 0.4994
17312/25000 [===================>..........] - ETA: 18s - loss: 7.6764 - accuracy: 0.4994
17344/25000 [===================>..........] - ETA: 18s - loss: 7.6781 - accuracy: 0.4993
17376/25000 [===================>..........] - ETA: 18s - loss: 7.6772 - accuracy: 0.4993
17408/25000 [===================>..........] - ETA: 18s - loss: 7.6816 - accuracy: 0.4990
17440/25000 [===================>..........] - ETA: 18s - loss: 7.6833 - accuracy: 0.4989
17472/25000 [===================>..........] - ETA: 18s - loss: 7.6850 - accuracy: 0.4988
17504/25000 [====================>.........] - ETA: 18s - loss: 7.6859 - accuracy: 0.4987
17536/25000 [====================>.........] - ETA: 18s - loss: 7.6876 - accuracy: 0.4986
17568/25000 [====================>.........] - ETA: 18s - loss: 7.6849 - accuracy: 0.4988
17600/25000 [====================>.........] - ETA: 18s - loss: 7.6858 - accuracy: 0.4988
17632/25000 [====================>.........] - ETA: 18s - loss: 7.6858 - accuracy: 0.4988
17664/25000 [====================>.........] - ETA: 17s - loss: 7.6857 - accuracy: 0.4988
17696/25000 [====================>.........] - ETA: 17s - loss: 7.6865 - accuracy: 0.4987
17728/25000 [====================>.........] - ETA: 17s - loss: 7.6865 - accuracy: 0.4987
17760/25000 [====================>.........] - ETA: 17s - loss: 7.6847 - accuracy: 0.4988
17792/25000 [====================>.........] - ETA: 17s - loss: 7.6847 - accuracy: 0.4988
17824/25000 [====================>.........] - ETA: 17s - loss: 7.6830 - accuracy: 0.4989
17856/25000 [====================>.........] - ETA: 17s - loss: 7.6838 - accuracy: 0.4989
17888/25000 [====================>.........] - ETA: 17s - loss: 7.6872 - accuracy: 0.4987
17920/25000 [====================>.........] - ETA: 17s - loss: 7.6880 - accuracy: 0.4986
17952/25000 [====================>.........] - ETA: 17s - loss: 7.6828 - accuracy: 0.4989
17984/25000 [====================>.........] - ETA: 17s - loss: 7.6862 - accuracy: 0.4987
18016/25000 [====================>.........] - ETA: 17s - loss: 7.6862 - accuracy: 0.4987
18048/25000 [====================>.........] - ETA: 17s - loss: 7.6862 - accuracy: 0.4987
18080/25000 [====================>.........] - ETA: 16s - loss: 7.6870 - accuracy: 0.4987
18112/25000 [====================>.........] - ETA: 16s - loss: 7.6861 - accuracy: 0.4987
18144/25000 [====================>.........] - ETA: 16s - loss: 7.6861 - accuracy: 0.4987
18176/25000 [====================>.........] - ETA: 16s - loss: 7.6860 - accuracy: 0.4987
18208/25000 [====================>.........] - ETA: 16s - loss: 7.6885 - accuracy: 0.4986
18240/25000 [====================>.........] - ETA: 16s - loss: 7.6918 - accuracy: 0.4984
18272/25000 [====================>.........] - ETA: 16s - loss: 7.6943 - accuracy: 0.4982
18304/25000 [====================>.........] - ETA: 16s - loss: 7.6918 - accuracy: 0.4984
18336/25000 [=====================>........] - ETA: 16s - loss: 7.6900 - accuracy: 0.4985
18368/25000 [=====================>........] - ETA: 16s - loss: 7.6900 - accuracy: 0.4985
18400/25000 [=====================>........] - ETA: 16s - loss: 7.6858 - accuracy: 0.4988
18432/25000 [=====================>........] - ETA: 16s - loss: 7.6866 - accuracy: 0.4987
18464/25000 [=====================>........] - ETA: 16s - loss: 7.6874 - accuracy: 0.4986
18496/25000 [=====================>........] - ETA: 15s - loss: 7.6849 - accuracy: 0.4988
18528/25000 [=====================>........] - ETA: 15s - loss: 7.6823 - accuracy: 0.4990
18560/25000 [=====================>........] - ETA: 15s - loss: 7.6823 - accuracy: 0.4990
18592/25000 [=====================>........] - ETA: 15s - loss: 7.6823 - accuracy: 0.4990
18624/25000 [=====================>........] - ETA: 15s - loss: 7.6847 - accuracy: 0.4988
18656/25000 [=====================>........] - ETA: 15s - loss: 7.6855 - accuracy: 0.4988
18688/25000 [=====================>........] - ETA: 15s - loss: 7.6863 - accuracy: 0.4987
18720/25000 [=====================>........] - ETA: 15s - loss: 7.6855 - accuracy: 0.4988
18752/25000 [=====================>........] - ETA: 15s - loss: 7.6854 - accuracy: 0.4988
18784/25000 [=====================>........] - ETA: 15s - loss: 7.6870 - accuracy: 0.4987
18816/25000 [=====================>........] - ETA: 15s - loss: 7.6862 - accuracy: 0.4987
18848/25000 [=====================>........] - ETA: 15s - loss: 7.6861 - accuracy: 0.4987
18880/25000 [=====================>........] - ETA: 15s - loss: 7.6894 - accuracy: 0.4985
18912/25000 [=====================>........] - ETA: 14s - loss: 7.6869 - accuracy: 0.4987
18944/25000 [=====================>........] - ETA: 14s - loss: 7.6869 - accuracy: 0.4987
18976/25000 [=====================>........] - ETA: 14s - loss: 7.6860 - accuracy: 0.4987
19008/25000 [=====================>........] - ETA: 14s - loss: 7.6892 - accuracy: 0.4985
19040/25000 [=====================>........] - ETA: 14s - loss: 7.6876 - accuracy: 0.4986
19072/25000 [=====================>........] - ETA: 14s - loss: 7.6867 - accuracy: 0.4987
19104/25000 [=====================>........] - ETA: 14s - loss: 7.6883 - accuracy: 0.4986
19136/25000 [=====================>........] - ETA: 14s - loss: 7.6867 - accuracy: 0.4987
19168/25000 [======================>.......] - ETA: 14s - loss: 7.6850 - accuracy: 0.4988
19200/25000 [======================>.......] - ETA: 14s - loss: 7.6842 - accuracy: 0.4989
19232/25000 [======================>.......] - ETA: 14s - loss: 7.6810 - accuracy: 0.4991
19264/25000 [======================>.......] - ETA: 14s - loss: 7.6809 - accuracy: 0.4991
19296/25000 [======================>.......] - ETA: 13s - loss: 7.6809 - accuracy: 0.4991
19328/25000 [======================>.......] - ETA: 13s - loss: 7.6793 - accuracy: 0.4992
19360/25000 [======================>.......] - ETA: 13s - loss: 7.6785 - accuracy: 0.4992
19392/25000 [======================>.......] - ETA: 13s - loss: 7.6761 - accuracy: 0.4994
19424/25000 [======================>.......] - ETA: 13s - loss: 7.6745 - accuracy: 0.4995
19456/25000 [======================>.......] - ETA: 13s - loss: 7.6737 - accuracy: 0.4995
19488/25000 [======================>.......] - ETA: 13s - loss: 7.6729 - accuracy: 0.4996
19520/25000 [======================>.......] - ETA: 13s - loss: 7.6737 - accuracy: 0.4995
19552/25000 [======================>.......] - ETA: 13s - loss: 7.6729 - accuracy: 0.4996
19584/25000 [======================>.......] - ETA: 13s - loss: 7.6721 - accuracy: 0.4996
19616/25000 [======================>.......] - ETA: 13s - loss: 7.6713 - accuracy: 0.4997
19648/25000 [======================>.......] - ETA: 13s - loss: 7.6721 - accuracy: 0.4996
19680/25000 [======================>.......] - ETA: 13s - loss: 7.6729 - accuracy: 0.4996
19712/25000 [======================>.......] - ETA: 12s - loss: 7.6728 - accuracy: 0.4996
19744/25000 [======================>.......] - ETA: 12s - loss: 7.6713 - accuracy: 0.4997
19776/25000 [======================>.......] - ETA: 12s - loss: 7.6713 - accuracy: 0.4997
19808/25000 [======================>.......] - ETA: 12s - loss: 7.6689 - accuracy: 0.4998
19840/25000 [======================>.......] - ETA: 12s - loss: 7.6689 - accuracy: 0.4998
19872/25000 [======================>.......] - ETA: 12s - loss: 7.6751 - accuracy: 0.4994
19904/25000 [======================>.......] - ETA: 12s - loss: 7.6759 - accuracy: 0.4994
19936/25000 [======================>.......] - ETA: 12s - loss: 7.6712 - accuracy: 0.4997
19968/25000 [======================>.......] - ETA: 12s - loss: 7.6697 - accuracy: 0.4998
20000/25000 [=======================>......] - ETA: 12s - loss: 7.6705 - accuracy: 0.4997
20032/25000 [=======================>......] - ETA: 12s - loss: 7.6659 - accuracy: 0.5000
20064/25000 [=======================>......] - ETA: 12s - loss: 7.6666 - accuracy: 0.5000
20096/25000 [=======================>......] - ETA: 12s - loss: 7.6636 - accuracy: 0.5002
20128/25000 [=======================>......] - ETA: 11s - loss: 7.6636 - accuracy: 0.5002
20160/25000 [=======================>......] - ETA: 11s - loss: 7.6636 - accuracy: 0.5002
20192/25000 [=======================>......] - ETA: 11s - loss: 7.6628 - accuracy: 0.5002
20224/25000 [=======================>......] - ETA: 11s - loss: 7.6659 - accuracy: 0.5000
20256/25000 [=======================>......] - ETA: 11s - loss: 7.6674 - accuracy: 0.5000
20288/25000 [=======================>......] - ETA: 11s - loss: 7.6704 - accuracy: 0.4998
20320/25000 [=======================>......] - ETA: 11s - loss: 7.6719 - accuracy: 0.4997
20352/25000 [=======================>......] - ETA: 11s - loss: 7.6674 - accuracy: 0.5000
20384/25000 [=======================>......] - ETA: 11s - loss: 7.6666 - accuracy: 0.5000
20416/25000 [=======================>......] - ETA: 11s - loss: 7.6696 - accuracy: 0.4998
20448/25000 [=======================>......] - ETA: 11s - loss: 7.6704 - accuracy: 0.4998
20480/25000 [=======================>......] - ETA: 11s - loss: 7.6689 - accuracy: 0.4999
20512/25000 [=======================>......] - ETA: 10s - loss: 7.6696 - accuracy: 0.4998
20544/25000 [=======================>......] - ETA: 10s - loss: 7.6689 - accuracy: 0.4999
20576/25000 [=======================>......] - ETA: 10s - loss: 7.6689 - accuracy: 0.4999
20608/25000 [=======================>......] - ETA: 10s - loss: 7.6674 - accuracy: 0.5000
20640/25000 [=======================>......] - ETA: 10s - loss: 7.6681 - accuracy: 0.4999
20672/25000 [=======================>......] - ETA: 10s - loss: 7.6674 - accuracy: 0.5000
20704/25000 [=======================>......] - ETA: 10s - loss: 7.6681 - accuracy: 0.4999
20736/25000 [=======================>......] - ETA: 10s - loss: 7.6659 - accuracy: 0.5000
20768/25000 [=======================>......] - ETA: 10s - loss: 7.6666 - accuracy: 0.5000
20800/25000 [=======================>......] - ETA: 10s - loss: 7.6674 - accuracy: 0.5000
20832/25000 [=======================>......] - ETA: 10s - loss: 7.6696 - accuracy: 0.4998
20864/25000 [========================>.....] - ETA: 10s - loss: 7.6688 - accuracy: 0.4999
20896/25000 [========================>.....] - ETA: 10s - loss: 7.6696 - accuracy: 0.4998
20928/25000 [========================>.....] - ETA: 9s - loss: 7.6703 - accuracy: 0.4998 
20960/25000 [========================>.....] - ETA: 9s - loss: 7.6695 - accuracy: 0.4998
20992/25000 [========================>.....] - ETA: 9s - loss: 7.6725 - accuracy: 0.4996
21024/25000 [========================>.....] - ETA: 9s - loss: 7.6695 - accuracy: 0.4998
21056/25000 [========================>.....] - ETA: 9s - loss: 7.6673 - accuracy: 0.5000
21088/25000 [========================>.....] - ETA: 9s - loss: 7.6688 - accuracy: 0.4999
21120/25000 [========================>.....] - ETA: 9s - loss: 7.6659 - accuracy: 0.5000
21152/25000 [========================>.....] - ETA: 9s - loss: 7.6644 - accuracy: 0.5001
21184/25000 [========================>.....] - ETA: 9s - loss: 7.6666 - accuracy: 0.5000
21216/25000 [========================>.....] - ETA: 9s - loss: 7.6688 - accuracy: 0.4999
21248/25000 [========================>.....] - ETA: 9s - loss: 7.6702 - accuracy: 0.4998
21280/25000 [========================>.....] - ETA: 9s - loss: 7.6731 - accuracy: 0.4996
21312/25000 [========================>.....] - ETA: 9s - loss: 7.6724 - accuracy: 0.4996
21344/25000 [========================>.....] - ETA: 8s - loss: 7.6716 - accuracy: 0.4997
21376/25000 [========================>.....] - ETA: 8s - loss: 7.6716 - accuracy: 0.4997
21408/25000 [========================>.....] - ETA: 8s - loss: 7.6731 - accuracy: 0.4996
21440/25000 [========================>.....] - ETA: 8s - loss: 7.6731 - accuracy: 0.4996
21472/25000 [========================>.....] - ETA: 8s - loss: 7.6745 - accuracy: 0.4995
21504/25000 [========================>.....] - ETA: 8s - loss: 7.6723 - accuracy: 0.4996
21536/25000 [========================>.....] - ETA: 8s - loss: 7.6752 - accuracy: 0.4994
21568/25000 [========================>.....] - ETA: 8s - loss: 7.6730 - accuracy: 0.4996
21600/25000 [========================>.....] - ETA: 8s - loss: 7.6737 - accuracy: 0.4995
21632/25000 [========================>.....] - ETA: 8s - loss: 7.6730 - accuracy: 0.4996
21664/25000 [========================>.....] - ETA: 8s - loss: 7.6695 - accuracy: 0.4998
21696/25000 [=========================>....] - ETA: 8s - loss: 7.6687 - accuracy: 0.4999
21728/25000 [=========================>....] - ETA: 8s - loss: 7.6716 - accuracy: 0.4997
21760/25000 [=========================>....] - ETA: 7s - loss: 7.6673 - accuracy: 0.5000
21792/25000 [=========================>....] - ETA: 7s - loss: 7.6680 - accuracy: 0.4999
21824/25000 [=========================>....] - ETA: 7s - loss: 7.6680 - accuracy: 0.4999
21856/25000 [=========================>....] - ETA: 7s - loss: 7.6694 - accuracy: 0.4998
21888/25000 [=========================>....] - ETA: 7s - loss: 7.6680 - accuracy: 0.4999
21920/25000 [=========================>....] - ETA: 7s - loss: 7.6680 - accuracy: 0.4999
21952/25000 [=========================>....] - ETA: 7s - loss: 7.6701 - accuracy: 0.4998
21984/25000 [=========================>....] - ETA: 7s - loss: 7.6701 - accuracy: 0.4998
22016/25000 [=========================>....] - ETA: 7s - loss: 7.6715 - accuracy: 0.4997
22048/25000 [=========================>....] - ETA: 7s - loss: 7.6750 - accuracy: 0.4995
22080/25000 [=========================>....] - ETA: 7s - loss: 7.6756 - accuracy: 0.4994
22112/25000 [=========================>....] - ETA: 7s - loss: 7.6749 - accuracy: 0.4995
22144/25000 [=========================>....] - ETA: 6s - loss: 7.6735 - accuracy: 0.4995
22176/25000 [=========================>....] - ETA: 6s - loss: 7.6742 - accuracy: 0.4995
22208/25000 [=========================>....] - ETA: 6s - loss: 7.6721 - accuracy: 0.4996
22240/25000 [=========================>....] - ETA: 6s - loss: 7.6749 - accuracy: 0.4995
22272/25000 [=========================>....] - ETA: 6s - loss: 7.6776 - accuracy: 0.4993
22304/25000 [=========================>....] - ETA: 6s - loss: 7.6790 - accuracy: 0.4992
22336/25000 [=========================>....] - ETA: 6s - loss: 7.6817 - accuracy: 0.4990
22368/25000 [=========================>....] - ETA: 6s - loss: 7.6844 - accuracy: 0.4988
22400/25000 [=========================>....] - ETA: 6s - loss: 7.6824 - accuracy: 0.4990
22432/25000 [=========================>....] - ETA: 6s - loss: 7.6823 - accuracy: 0.4990
22464/25000 [=========================>....] - ETA: 6s - loss: 7.6823 - accuracy: 0.4990
22496/25000 [=========================>....] - ETA: 6s - loss: 7.6837 - accuracy: 0.4989
22528/25000 [==========================>...] - ETA: 6s - loss: 7.6809 - accuracy: 0.4991
22560/25000 [==========================>...] - ETA: 5s - loss: 7.6795 - accuracy: 0.4992
22592/25000 [==========================>...] - ETA: 5s - loss: 7.6816 - accuracy: 0.4990
22624/25000 [==========================>...] - ETA: 5s - loss: 7.6856 - accuracy: 0.4988
22656/25000 [==========================>...] - ETA: 5s - loss: 7.6842 - accuracy: 0.4989
22688/25000 [==========================>...] - ETA: 5s - loss: 7.6822 - accuracy: 0.4990
22720/25000 [==========================>...] - ETA: 5s - loss: 7.6815 - accuracy: 0.4990
22752/25000 [==========================>...] - ETA: 5s - loss: 7.6781 - accuracy: 0.4993
22784/25000 [==========================>...] - ETA: 5s - loss: 7.6774 - accuracy: 0.4993
22816/25000 [==========================>...] - ETA: 5s - loss: 7.6794 - accuracy: 0.4992
22848/25000 [==========================>...] - ETA: 5s - loss: 7.6807 - accuracy: 0.4991
22880/25000 [==========================>...] - ETA: 5s - loss: 7.6787 - accuracy: 0.4992
22912/25000 [==========================>...] - ETA: 5s - loss: 7.6767 - accuracy: 0.4993
22944/25000 [==========================>...] - ETA: 5s - loss: 7.6760 - accuracy: 0.4994
22976/25000 [==========================>...] - ETA: 4s - loss: 7.6793 - accuracy: 0.4992
23008/25000 [==========================>...] - ETA: 4s - loss: 7.6793 - accuracy: 0.4992
23040/25000 [==========================>...] - ETA: 4s - loss: 7.6759 - accuracy: 0.4994
23072/25000 [==========================>...] - ETA: 4s - loss: 7.6733 - accuracy: 0.4996
23104/25000 [==========================>...] - ETA: 4s - loss: 7.6739 - accuracy: 0.4995
23136/25000 [==========================>...] - ETA: 4s - loss: 7.6726 - accuracy: 0.4996
23168/25000 [==========================>...] - ETA: 4s - loss: 7.6732 - accuracy: 0.4996
23200/25000 [==========================>...] - ETA: 4s - loss: 7.6739 - accuracy: 0.4995
23232/25000 [==========================>...] - ETA: 4s - loss: 7.6739 - accuracy: 0.4995
23264/25000 [==========================>...] - ETA: 4s - loss: 7.6726 - accuracy: 0.4996
23296/25000 [==========================>...] - ETA: 4s - loss: 7.6712 - accuracy: 0.4997
23328/25000 [==========================>...] - ETA: 4s - loss: 7.6712 - accuracy: 0.4997
23360/25000 [===========================>..] - ETA: 4s - loss: 7.6719 - accuracy: 0.4997
23392/25000 [===========================>..] - ETA: 3s - loss: 7.6706 - accuracy: 0.4997
23424/25000 [===========================>..] - ETA: 3s - loss: 7.6725 - accuracy: 0.4996
23456/25000 [===========================>..] - ETA: 3s - loss: 7.6699 - accuracy: 0.4998
23488/25000 [===========================>..] - ETA: 3s - loss: 7.6673 - accuracy: 0.5000
23520/25000 [===========================>..] - ETA: 3s - loss: 7.6666 - accuracy: 0.5000
23552/25000 [===========================>..] - ETA: 3s - loss: 7.6673 - accuracy: 0.5000
23584/25000 [===========================>..] - ETA: 3s - loss: 7.6653 - accuracy: 0.5001
23616/25000 [===========================>..] - ETA: 3s - loss: 7.6686 - accuracy: 0.4999
23648/25000 [===========================>..] - ETA: 3s - loss: 7.6666 - accuracy: 0.5000
23680/25000 [===========================>..] - ETA: 3s - loss: 7.6712 - accuracy: 0.4997
23712/25000 [===========================>..] - ETA: 3s - loss: 7.6705 - accuracy: 0.4997
23744/25000 [===========================>..] - ETA: 3s - loss: 7.6679 - accuracy: 0.4999
23776/25000 [===========================>..] - ETA: 2s - loss: 7.6653 - accuracy: 0.5001
23808/25000 [===========================>..] - ETA: 2s - loss: 7.6698 - accuracy: 0.4998
23840/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
23872/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
23904/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
23936/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
23968/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
24000/25000 [===========================>..] - ETA: 2s - loss: 7.6685 - accuracy: 0.4999
24032/25000 [===========================>..] - ETA: 2s - loss: 7.6692 - accuracy: 0.4998
24064/25000 [===========================>..] - ETA: 2s - loss: 7.6711 - accuracy: 0.4997
24096/25000 [===========================>..] - ETA: 2s - loss: 7.6685 - accuracy: 0.4999
24128/25000 [===========================>..] - ETA: 2s - loss: 7.6673 - accuracy: 0.5000
24160/25000 [===========================>..] - ETA: 2s - loss: 7.6698 - accuracy: 0.4998
24192/25000 [============================>.] - ETA: 1s - loss: 7.6723 - accuracy: 0.4996
24224/25000 [============================>.] - ETA: 1s - loss: 7.6711 - accuracy: 0.4997
24256/25000 [============================>.] - ETA: 1s - loss: 7.6698 - accuracy: 0.4998
24288/25000 [============================>.] - ETA: 1s - loss: 7.6691 - accuracy: 0.4998
24320/25000 [============================>.] - ETA: 1s - loss: 7.6685 - accuracy: 0.4999
24352/25000 [============================>.] - ETA: 1s - loss: 7.6691 - accuracy: 0.4998
24384/25000 [============================>.] - ETA: 1s - loss: 7.6685 - accuracy: 0.4999
24416/25000 [============================>.] - ETA: 1s - loss: 7.6685 - accuracy: 0.4999
24448/25000 [============================>.] - ETA: 1s - loss: 7.6672 - accuracy: 0.5000
24480/25000 [============================>.] - ETA: 1s - loss: 7.6647 - accuracy: 0.5001
24512/25000 [============================>.] - ETA: 1s - loss: 7.6647 - accuracy: 0.5001
24544/25000 [============================>.] - ETA: 1s - loss: 7.6635 - accuracy: 0.5002
24576/25000 [============================>.] - ETA: 1s - loss: 7.6660 - accuracy: 0.5000
24608/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24640/25000 [============================>.] - ETA: 0s - loss: 7.6679 - accuracy: 0.4999
24672/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24704/25000 [============================>.] - ETA: 0s - loss: 7.6679 - accuracy: 0.4999
24736/25000 [============================>.] - ETA: 0s - loss: 7.6685 - accuracy: 0.4999
24768/25000 [============================>.] - ETA: 0s - loss: 7.6716 - accuracy: 0.4997
24800/25000 [============================>.] - ETA: 0s - loss: 7.6703 - accuracy: 0.4998
24832/25000 [============================>.] - ETA: 0s - loss: 7.6697 - accuracy: 0.4998
24864/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24896/25000 [============================>.] - ETA: 0s - loss: 7.6648 - accuracy: 0.5001
24928/25000 [============================>.] - ETA: 0s - loss: 7.6672 - accuracy: 0.5000
24960/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24992/25000 [============================>.] - ETA: 0s - loss: 7.6654 - accuracy: 0.5001
25000/25000 [==============================] - 72s 3ms/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
Loading data...
Using TensorFlow backend.





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'../mlmodels/dataset/json/hyper_titanic_randomForest.json'[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      7[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/hyper_titanic_randomForest.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//mnist_mlmodels_.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl_titanic.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m     [0mchoice[0m[0;34m=[0m[0;34m'json'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m     [0mconfig_mode[0m[0;34m=[0m [0;34m'test'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m     [0mdata_path[0m[0;34m=[0m [0;34m'../mlmodels/dataset/json/gluon_automl.json'[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m )

[0;32m~/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py[0m in [0;36mget_params[0;34m(choice, data_path, config_mode, **kw)[0m
[1;32m     80[0m             __file__)).parent.parent / "model_gluon/gluon_automl.json" if data_path == "dataset/" else data_path
[1;32m     81[0m [0;34m[0m[0m
[0;32m---> 82[0;31m         [0;32mwith[0m [0mopen[0m[0;34m([0m[0mdata_path[0m[0;34m,[0m [0mencoding[0m[0;34m=[0m[0;34m'utf-8'[0m[0;34m)[0m [0;32mas[0m [0mconfig_f[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     83[0m             [0mconfig[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mconfig_f[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     84[0m             [0mconfig[0m [0;34m=[0m [0mconfig[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/gluon_automl.json'
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow__lstm_json.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      5[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0mmodel[0m         [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mModel[0m[0;34m([0m[0mmodel_pars[0m[0;34m=[0m[0mmodel_pars[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m)[0m             [0;31m# Create Model instance[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mmodel[0m[0;34m,[0m [0msess[0m   [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m,[0m [0mout_pars[0m[0;34m=[0m[0mout_pars[0m[0;34m)[0m          [0;31m# fit the model[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_titanic.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.py 

[0;36m  File [0;32m"/home/runner/work/mlmodels/mlmodels/mlmodels/example/vision_mnist.py"[0;36m, line [0;32m15[0m
[0;31m    !git clone https://github.com/ahmed3bbas/mlmodels.git[0m
[0m    ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_hyper.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_hyper.py[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mutil[0m [0;32mimport[0m [0mpath_norm_dict[0m[0;34m,[0m [0mpath_norm[0m[0;34m,[0m [0mparams_json_load[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mprint[0m[0;34m([0m[0mmlmodels[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;34m[0m[0m

[0;31mNameError[0m: name 'mlmodels' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.py 

Deprecaton set to False
/home/runner/work/mlmodels/mlmodels
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/lightgbm_glass.py[0m in [0;36m<module>[0;34m[0m
[1;32m     20[0m [0;34m[0m[0m
[1;32m     21[0m [0;34m[0m[0m
[0;32m---> 22[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     23[0m [0mprint[0m[0;34m([0m[0mpars[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     24[0m [0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_glass.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_model.py 

<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_model.py[0m in [0;36m<module>[0;34m[0m
[1;32m     25[0m [0;31m# Model Parameters[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m     26[0m [0;31m# model_pars, data_pars, compute_pars, out_pars[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 27[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m[0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     28[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     29[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpdict[0m   [0;34m)[0m   [0;31m###Normalize path[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'
