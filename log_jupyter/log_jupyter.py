
  test_jupyter /home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json Namespace(config_file='/home/runner/work/mlmodels/mlmodels/mlmodels/config/test_config.json', config_mode='test', do='test_jupyter', folder=None, log_file=None, save_folder='ztest/') 

  ml_test --do test_jupyter 





 ************************************************************************************************************************

 ******** TAG ::  {'github_repo_url': 'https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed', 'url_branch_file': 'https://github.com/arita37/mlmodels/blob/dev/', 'repo': 'arita37/mlmodels', 'branch': 'dev', 'sha': '14ea42490a753d0445fda074e8d3eac878d2aeed', 'workflow': 'test_jupyter'}

 ******** GITHUB_WOKFLOW : https://github.com/arita37/mlmodels/actions?query=workflow%3Atest_jupyter

 ******** GITHUB_REPO_BRANCH : https://github.com/arita37/mlmodels/tree/dev/

 ******** GITHUB_REPO_URL : https://github.com/arita37/mlmodels/tree/14ea42490a753d0445fda074e8d3eac878d2aeed

 ******** GITHUB_COMMIT_URL : https://github.com/arita37/mlmodels/commit/14ea42490a753d0445fda074e8d3eac878d2aeed
Package                   Version    Location
------------------------- ---------- -----------------------------------
absl-py                   0.9.0
alembic                   1.4.2
astor                     0.8.1
attrs                     19.3.0
autogluon                 0.0.5
backcall                  0.1.0
bcrypt                    3.1.7
bleach                    3.1.5
blis                      0.4.1
boto                      2.49.0
boto3                     1.9.187
botocore                  1.12.253
catalogue                 1.0.0
catboost                  0.23.1
certifi                   2020.4.5.1
cffi                      1.14.0
chardet                   3.0.4
cli-code                  28.1.0
click                     7.1.2
cliff                     3.1.0
cloudpickle               1.4.1
cmd2                      0.8.9
cmdstanpy                 0.4.0
colorlog                  4.1.0
configparser              5.0.0
ConfigSpace               0.4.10
convertdate               2.2.1
cryptography              2.9.2
cycler                    0.10.0
cymem                     2.0.3
Cython                    0.29.17
dask                      2.6.0
databricks-cli            0.10.0
dataclasses               0.7
decorator                 4.4.2
deepctr                   0.7.4
defusedxml                0.6.0
dill                      0.3.1.1
distributed               2.6.0
docker                    4.2.0
docutils                  0.15.2
entrypoints               0.3
ephem                     3.7.7.1
fbprophet                 0.6
Flask                     1.1.2
future                    0.18.2
gast                      0.2.2
gensim                    3.8.3
gitdb                     4.0.5
GitPython                 3.1.2
gluoncv                   0.7.0
gluonnlp                  0.8.1
gluonts                   0.4.2
google-pasta              0.2.0
googleapis-common-protos  1.51.0
gorilla                   0.3.0
graphviz                  0.8.4
grpcio                    1.29.0
gunicorn                  20.0.4
h5py                      2.10.0
HeapDict                  1.0.1
holidays                  0.10.2
hyperopt                  0.1.2
idna                      2.9
importlib-metadata        1.6.0
ipykernel                 5.2.1
ipython                   7.14.0
ipython-genutils          0.2.0
itsdangerous              1.1.0
jedi                      0.17.0
Jinja2                    2.11.2
jmespath                  0.10.0
joblib                    0.15.0
jsonschema                3.2.0
jupyter-client            6.1.3
jupyter-core              4.6.3
Keras                     2.3.1
Keras-Applications        1.0.8
keras-contrib             2.0.8
keras-mdn-layer           0.2.1
Keras-Preprocessing       1.1.2
kiwisolver                1.2.0
korean-lunar-calendar     0.2.1
lightgbm                  2.3.0
LunarCalendar             0.0.9
Mako                      1.1.2
Markdown                  3.2.2
MarkupSafe                1.1.1
matchzoo-py               1.1.1
matplotlib                3.2.1
mistune                   0.8.4
mlflow                    1.7.1
mlmodels                  0.35.2     /home/runner/work/mlmodels/mlmodels
msgpack                   1.0.0
murmurhash                1.0.2
mxnet                     1.6.0
nbconvert                 5.6.1
nbformat                  5.0.6
networkx                  2.4
nltk                      3.5
notebook                  6.0.3
numexpr                   2.7.1
numpy                     1.18.2
opt-einsum                3.2.1
optuna                    1.1.0
packaging                 20.3
pandas                    0.25.3
pandocfilters             1.4.2
paramiko                  2.7.1
parso                     0.7.0
pbr                       5.4.5
pexpect                   4.8.0
pickleshare               0.7.5
Pillow                    6.2.1
pip                       20.1
plac                      1.1.3
plotly                    4.7.1
portalocker               1.7.0
preshed                   3.0.2
prettytable               0.7.2
prometheus-client         0.7.1
prometheus-flask-exporter 0.13.0
promise                   2.3
prompt-toolkit            3.0.5
protobuf                  3.12.0
psutil                    5.7.0
ptyprocess                0.6.0
pyaml                     20.4.0
pycparser                 2.20
pydantic                  1.4
Pygments                  2.6.1
PyMeeus                   0.3.7
pymongo                   3.10.1
PyNaCl                    1.3.0
pyparsing                 2.4.7
pyperclip                 1.8.0
pyrsistent                0.16.0
pystan                    2.19.1.1
python-dateutil           2.8.0
python-editor             1.0.4
pytorch-lightning         0.7.3
pytorch-transformers      1.2.0
pytz                      2020.1
PyYAML                    5.3.1
pyzmq                     19.0.1
querystring-parser        1.2.4
regex                     2020.5.14
requests                  2.23.0
retrying                  1.3.3
s3transfer                0.2.1
sacremoses                0.0.43
scikit-learn              0.21.2
scikit-optimize           0.7.4
scipy                     1.4.1
Send2Trash                1.5.0
sentence-transformers     0.2.4
sentencepiece             0.1.90
setuptools                45.2.0
setuptools-git            1.2
simplejson                3.17.0
six                       1.14.0
smart-open                2.0.0
smmap                     3.0.4
sortedcontainers          2.1.0
spacy                     2.2.4
SQLAlchemy                1.3.13
sqlparse                  0.3.1
srsly                     1.0.2
stevedore                 1.32.0
tabulate                  0.8.7
tblib                     1.6.0
tensorboard               1.15.0
tensorboardX              2.0
tensorflow                1.15.2
tensorflow-datasets       3.0.0
tensorflow-estimator      1.15.1
tensorflow-metadata       0.22.0
tensorflow-probability    0.7.0
termcolor                 1.1.0
terminado                 0.8.3
testpath                  0.4.4
thinc                     7.4.0
toml                      0.10.1
toolz                     0.10.0
torch                     1.2.0
torchtext                 0.6.0
torchvision               0.4.0
tornado                   6.0.4
tqdm                      4.46.0
traitlets                 4.3.3
transformers              2.3.0
typing                    3.7.4.1
ujson                     1.35
urllib3                   1.25.9
versioneer                0.18
wasabi                    0.6.0
wcwidth                   0.1.9
webencodings              0.5.1
websocket-client          0.57.0
Werkzeug                  1.0.1
wheel                     0.34.2
wrapt                     1.12.1
zict                      2.0.0
zipp                      3.1.0

 ************************************************************************************************************************

 ************************************************************************************************************************
/home/runner/work/mlmodels/mlmodels/mlmodels/example/
############ List of files ################################
['ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow_1_lstm.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//keras-textcnn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_hyper.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//benchmark_timeseries_m5.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example//arun_model.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m4.py', 'ipython /home/runner/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py']





 ************************************************************************************************************************
############ Running Jupyter files ################################





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_svm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_svm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;34m[0m[0m
[1;32m      4[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0;34m[0m[0m
[0;32m----> 6[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      7[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      8[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      2[0m [0;34m[0m[0m
[1;32m      3[0m [0mmodel_uri[0m    [0;34m=[0m [0;34m"model_sklearn.sklearn.py"[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 4[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      5[0m [0;34m[0m[0m
[1;32m      6[0m model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars={

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//fashion_MNIST_mlmodels.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//fashion_MNIST_mlmodels.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_home_retail.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_home_retail.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_home_retail.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_home_retail.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras_charcnn_reuters.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//keras_charcnn_reuters.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mmodel_pars[0m      [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'model_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      3[0m [0mdata_pars[0m       [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'data_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mcompute_pars[0m    [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'compute_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mout_pars[0m        [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpars[0m[0;34m[[0m[0;34m'out_pars'[0m[0;34m][0m [0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'reuters_charcnn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl.ipynb 

/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073
Warning: `hyperparameter_tune=True` is currently experimental and may cause the process to hang. Setting `auto_stack=True` instead is recommended to achieve maximum quality models.
Beginning AutoGluon training ... Time limit = 120s
AutoGluon will save models to dataset/
Train Data Rows:    39073
Train Data Columns: 15
Preprocessing data ...
Here are the first 10 unique label values in your data:  [' Tech-support' ' Transport-moving' ' Other-service' ' ?'
 ' Handlers-cleaners' ' Sales' ' Craft-repair' ' Adm-clerical'
 ' Exec-managerial' ' Prof-specialty']
AutoGluon infers your prediction problem is: multiclass  (because dtype of label-column == object)
If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])

Feature Generator processed 39073 data points with 14 features
Original Features:
	int features: 6
	object features: 8
Generated Features:
	int features: 0
All Features:
	int features: 6
	object features: 8
	Data preprocessing and feature engineering runtime = 0.21s ...
AutoGluon will gauge predictive performance using evaluation metric: accuracy
To change this, specify the eval_metric argument of fit()
AutoGluon will early stop models using evaluation metric: accuracy
Saving dataset/learner.pkl
Beginning hyperparameter tuning for Gradient Boosting Model...
Hyperparameter search space for Gradient Boosting Model: 
num_leaves:   Int: lower=26, upper=30
learning_rate:   Real: lower=0.005, upper=0.2
feature_fraction:   Real: lower=0.75, upper=1.0
min_data_in_leaf:   Int: lower=2, upper=30
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
Warning: Exception caused LightGBMClassifier to fail during hyperparameter tuning... Skipping this model.
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/trainer/abstract_trainer.py", line 360, in train_single_full
    Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/utils/tabular/ml/models/lgb/lgb_model.py", line 283, in hyperparameter_tune
    directory=directory, lgb_model=self, **params_copy)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 69, in register_args
    self.update(**kwvars)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/decorator.py", line 79, in update
    hp = v.get_hp(name=k)
  File "/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/autogluon/core/space.py", line 451, in get_hp
    default_value=self._default)
  File "ConfigSpace/hyperparameters.pyx", line 773, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.__init__
  File "ConfigSpace/hyperparameters.pyx", line 843, in ConfigSpace.hyperparameters.UniformIntegerHyperparameter.check_default
ValueError: Illegal default value 36
Saving dataset/models/trainer.pkl
Beginning hyperparameter tuning for Neural Network...
Hyperparameter search space for Neural Network: 
network_type:   Categorical['widedeep', 'feedforward']
layers:   Categorical[[100], [1000], [200, 100], [300, 200, 100]]
activation:   Categorical['relu', 'softrelu', 'tanh']
embedding_size_factor:   Real: lower=0.5, upper=1.5
use_batchnorm:   Categorical[True, False]
dropout_prob:   Real: lower=0.0, upper=0.5
learning_rate:   Real: lower=0.0001, upper=0.01
weight_decay:   Real: lower=1e-12, upper=0.1
AutoGluon Neural Network infers features are of the following types:
{
    "continuous": [
        "age",
        "education-num",
        "hours-per-week"
    ],
    "skewed": [
        "fnlwgt",
        "capital-gain",
        "capital-loss"
    ],
    "onehot": [
        "sex",
        "class"
    ],
    "embed": [
        "workclass",
        "education",
        "marital-status",
        "relationship",
        "race",
        "native-country"
    ],
    "language": []
}


Saving dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Saving dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
Starting Experiments
Num of Finished Tasks is 0
Num of Pending Tasks is 5
  0%|          | 0/5 [00:00<?, ?it/s]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Finished Task with config: {'activation.choice': 0, 'dropout_prob': 0.1, 'embedding_size_factor': 1.0, 'layers.choice': 0, 'learning_rate': 0.0005, 'network_type.choice': 0, 'use_batchnorm.choice': 0, 'weight_decay': 1e-06} and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xb9\x99\x99\x99\x99\x99\x9aX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf0\x00\x00\x00\x00\x00\x00X\r\x00\x00\x00layers.choiceq\x04K\x00X\r\x00\x00\x00learning_rateq\x05G?@bM\xd2\xf1\xa9\xfcX\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x00X\x0c\x00\x00\x00weight_decayq\x08G>\xb0\xc6\xf7\xa0\xb5\xed\x8du.' and reward: 0.3862
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:47<01:10, 23.50s/it]Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Finished Task with config: {'activation.choice': 1, 'dropout_prob': 0.07547357258238448, 'embedding_size_factor': 0.5470991389597457, 'layers.choice': 2, 'learning_rate': 0.005218560041244485, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 0.07823277914467376} and reward: 0.2424
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xb3R<m\xf4 \xadX\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1\x81\xd6\r\xb0\x0fLX\r\x00\x00\x00layers.choiceq\x04K\x02X\r\x00\x00\x00learning_rateq\x05G?u`\x0e\x8bY\x879X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?\xb4\x07\x10;\xe6\xcc\xf1u.' and reward: 0.2424
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x01X\x0c\x00\x00\x00dropout_probq\x02G?\xb3R<m\xf4 \xadX\x15\x00\x00\x00embedding_size_factorq\x03G?\xe1\x81\xd6\r\xb0\x0fLX\r\x00\x00\x00layers.choiceq\x04K\x02X\r\x00\x00\x00learning_rateq\x05G?u`\x0e\x8bY\x879X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G?\xb4\x07\x10;\xe6\xcc\xf1u.' and reward: 0.2424
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:35<01:01, 30.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:35<01:03, 31.72s/it]
Loading: dataset/models/NeuralNetClassifier/train_tabNNdataset.pkl
Loading: dataset/models/NeuralNetClassifier/validation_tabNNdataset.pkl
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
Saving dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Finished Task with config: {'activation.choice': 0, 'dropout_prob': 0.11518656639428662, 'embedding_size_factor': 1.4974687129713549, 'layers.choice': 3, 'learning_rate': 0.0065705411970661464, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 3.896471354143596e-08} and reward: 0.3872
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xbd|\xdd\xe7\x9a\x1cBX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf7\xf5\xa1\xc0\xcf\xecBX\r\x00\x00\x00layers.choiceq\x04K\x03X\r\x00\x00\x00learning_rateq\x05G?z\xe9\xb68\xefI\x97X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G>d\xebD\xfa\xc5\x88\xc6u.' and reward: 0.3872
Finished Task with config: b'\x80\x03}q\x00(X\x11\x00\x00\x00activation.choiceq\x01K\x00X\x0c\x00\x00\x00dropout_probq\x02G?\xbd|\xdd\xe7\x9a\x1cBX\x15\x00\x00\x00embedding_size_factorq\x03G?\xf7\xf5\xa1\xc0\xcf\xecBX\r\x00\x00\x00layers.choiceq\x04K\x03X\r\x00\x00\x00learning_rateq\x05G?z\xe9\xb68\xefI\x97X\x13\x00\x00\x00network_type.choiceq\x06K\x00X\x14\x00\x00\x00use_batchnorm.choiceq\x07K\x01X\x0c\x00\x00\x00weight_decayq\x08G>d\xebD\xfa\xc5\x88\xc6u.' and reward: 0.3872
Please either provide filename or allow plot in get_training_curves
Time for Neural Network hyperparameter optimization: 145.14808893203735
Best hyperparameter configuration for Tabular Neural Network: 
{'activation.choice': 0, 'dropout_prob': 0.11518656639428662, 'embedding_size_factor': 1.4974687129713549, 'layers.choice': 3, 'learning_rate': 0.0065705411970661464, 'network_type.choice': 0, 'use_batchnorm.choice': 1, 'weight_decay': 3.896471354143596e-08}
Saving dataset/models/trainer.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Fitting model: weighted_ensemble_k0_l1 ... Training model for up to 119.79s of the -27.53s of remaining time.
Ensemble size: 45
Ensemble weights: 
[0.6        0.17777778 0.22222222]
	0.3934	 = Validation accuracy score
	0.99s	 = Training runtime
	0.0s	 = Validation runtime
Saving dataset/models/weighted_ensemble_k0_l1/model.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
Saving dataset/models/trainer.pkl
AutoGluon training complete, total runtime = 148.55s ...
Loading: dataset/models/trainer.pkl
Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769
Loading: dataset/models/trainer.pkl
Loading: dataset/models/weighted_ensemble_k0_l1/model.pkl
Loading: dataset/models/NeuralNetClassifier/trial_2_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_0_tabularNN.pkl
Loading: dataset/models/NeuralNetClassifier/trial_1_tabularNN.pkl
test

  #### Module init   ############################################ 

  <module 'mlmodels.model_gluon.gluon_automl' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py'> 

  #### Loading params   ############################################## 
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 453, in test_cli
    test_module(arg.model_uri, param_pars=param_pars)  # '1_lstm'
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 257, in test_module
    model_pars, data_pars, compute_pars, out_pars = module.get_params(param_pars)
  File "/home/runner/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py", line 109, in get_params
    return model_pars, data_pars, compute_pars, out_pars
UnboundLocalError: local variable 'model_pars' referenced before assignment





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vison_fashion_MNIST.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vison_fashion_MNIST.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow_1_lstm.ipynb 

/home/runner/work/mlmodels/mlmodels
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'size': [0, 0, 6], 'output_size': [0, 6]}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
test

  #### Module init   ############################################ 
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term

  <module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'> 

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  #### Model init   ############################################ 

  <mlmodels.model_tf.1_lstm.Model object at 0x7f9b2be23ac8> 

  #### Fit   ######################################################## 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   #################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384
[[ 0.          0.          0.          0.          0.          0.        ]
 [-0.0182854  -0.01676878  0.0564476   0.12226599 -0.12388188  0.03643176]
 [-0.09083334  0.1096662   0.04757109 -0.11926425  0.22789046  0.23131654]
 [-0.03405919  0.00382574  0.20210388 -0.05683132 -0.06393629 -0.06525341]
 [-0.09104627  0.36522019 -0.26204976  0.523       0.32685265  0.2541106 ]
 [-0.36501405  0.02774938  0.48663524  0.06449956 -0.19649227  0.07513807]
 [ 0.19228548 -0.3008514   0.48829988 -0.26739845 -0.09975055 -0.06476645]
 [ 0.10532055  0.20678449  0.04826843  0.51224905 -0.25326723  0.40797538]
 [-0.16293949  0.42345577 -0.01083543 -0.16587475 -0.09172773  0.19101809]
 [ 0.          0.          0.          0.          0.          0.        ]]

  #### Get  metrics   ################################################ 

  #### Save   ######################################################## 

  #### Load   ######################################################## 
model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.3974001407623291, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 08:39:39.116729: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 455, in test_cli
    test(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 189, in test
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()

model_tf/1_lstm.py
model_tf.1_lstm.py
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>
<module 'mlmodels.model_tf.1_lstm' from '/home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py'>

  #### Loading params   ############################################## 

  ############# Data, Params preparation   ################# 

  {'learning_rate': 0.001, 'num_layers': 1, 'size': 6, 'size_layer': 128, 'timestep': 4, 'epoch': 2, 'output_size': 6} {'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'} {} {'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'} 

  #### Loading dataset   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]

  #### Model init  ############################################# 

  #### Model fit   ############################################# 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas'}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000

  #### Predict   ##################################################### 
{'data_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv', 'data_type': 'pandas', 'train': 0}
/home/runner/work/mlmodels/mlmodels/mlmodels/dataset/timeseries/GOOG-year.csv
         Date        Open        High  ...       Close   Adj Close   Volume
0  2016-11-02  778.200012  781.650024  ...  768.700012  768.700012  1872400
1  2016-11-03  767.250000  769.950012  ...  762.130005  762.130005  1943200
2  2016-11-04  750.659973  770.359985  ...  762.020020  762.020020  2134800
3  2016-11-07  774.500000  785.190002  ...  782.520020  782.520020  1585100
4  2016-11-08  783.400024  795.632996  ...  790.510010  790.510010  1350800

[5 rows x 7 columns]
          0         1         2         3         4         5
0  0.706562  0.629914  0.682052  0.599302  0.599302  0.153665
1  0.458824  0.320251  0.598101  0.478596  0.478596  0.174523
2  0.083484  0.331101  0.437246  0.476576  0.476576  0.230969
3  0.622851  0.723606  0.854891  0.853206  0.853206  0.069025
4  0.824209  1.000000  1.000000  1.000000  1.000000  0.000000
5  0.745928  0.883387  0.838176  0.904464  0.904464  0.370110
6  1.000000  0.881878  0.467996  0.486496  0.486496  1.000000
7  0.216516  0.077549  0.433808  0.329598  0.329598  0.318466
8  0.195249  0.000000  0.000000  0.000000  0.000000  0.671960
9  0.000000  0.173783  0.369041  0.411721  0.411721  0.304384

  #### metrics   ##################################################### 
{'loss': 0.4012460298836231, 'loss_history': []}

  #### Plot   ######################################################## 

  #### Save   ######################################################## 
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/'}
Model saved in path: /home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm//model//model.ckpt

  #### Load   ######################################################## 
2020-05-16 08:39:40.205101: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key Variable not found in checkpoint
{'path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/', 'model_path': '/home/runner/work/mlmodels/mlmodels/mlmodels/ztest/model_tf/1_lstm/model'}
Failed Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key Variable not found in checkpoint
	 [[node save_1/RestoreV2 (defined at opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]

Original stack trace for 'save_1/RestoreV2':
  File "opt/hostedtoolcache/Python/3.6.10/x64/bin/ml_models", line 11, in <module>
    load_entry_point('mlmodels', 'console_scripts', 'ml_models')()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 523, in main
    test_cli(arg)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 457, in test_cli
    test_global(arg.model_uri)  # '1_lstm'
  File "home/runner/work/mlmodels/mlmodels/mlmodels/models.py", line 200, in test_global
    module.test()
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 320, in test
    session = load(out_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/model_tf/1_lstm.py", line 199, in load
    return load_tf(load_pars)
  File "home/runner/work/mlmodels/mlmodels/mlmodels/util.py", line 474, in load_tf
    saver      = tf.compat.v1.train.Saver()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 828, in __init__
    self.build()
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 840, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 878, in _build
    build_restore=build_restore)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 508, in _build_internal
    restore_sequentially, reshape)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 328, in _AddRestoreOps
    restore_sequentially)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py", line 1696, in restore_v2
    name=name)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py", line 794, in _apply_op_helper
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3357, in create_op
    attrs, op_def, compute_device)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 3426, in _create_op_internal
    op_def=op_def)
  File "opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//vision_mnist.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_glass.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m [0;34m[0m[0m
[0;32m---> 10[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//keras-textcnn.ipynb 

WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 400)          0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 400, 50)      500         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 398, 128)     19328       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 397, 128)     25728       embedding_1[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 396, 128)     32128       embedding_1[0][0]                
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   
__________________________________________________________________________________________________
global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
                                                                 global_max_pooling1d_3[0][0]     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            385         concatenate_1[0][0]              
==================================================================================================
Total params: 78,069
Trainable params: 78,069
Non-trainable params: 0
__________________________________________________________________________________________________
Loading data...
Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz

    8192/17464789 [..............................] - ETA: 0s
 2613248/17464789 [===>..........................] - ETA: 0s
11665408/17464789 [===================>..........] - ETA: 0s
17465344/17464789 [==============================] - 0s 0us/step
Pad sequences (samples x time)...
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-05-16 08:39:50.906877: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-16 08:39:50.910733: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095245000 Hz
2020-05-16 08:39:50.910886: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e56bd3ad60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-16 08:39:50.910899: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Train on 25000 samples, validate on 25000 samples
Epoch 1/1

   32/25000 [..............................] - ETA: 4:26 - loss: 7.6666 - accuracy: 0.5000
   64/25000 [..............................] - ETA: 2:42 - loss: 7.9062 - accuracy: 0.4844
   96/25000 [..............................] - ETA: 2:08 - loss: 8.3055 - accuracy: 0.4583
  128/25000 [..............................] - ETA: 1:52 - loss: 8.3854 - accuracy: 0.4531
  160/25000 [..............................] - ETA: 1:39 - loss: 8.0500 - accuracy: 0.4750
  192/25000 [..............................] - ETA: 1:31 - loss: 8.2256 - accuracy: 0.4635
  224/25000 [..............................] - ETA: 1:26 - loss: 7.5982 - accuracy: 0.5045
  256/25000 [..............................] - ETA: 1:21 - loss: 7.3671 - accuracy: 0.5195
  288/25000 [..............................] - ETA: 1:19 - loss: 7.4004 - accuracy: 0.5174
  320/25000 [..............................] - ETA: 1:17 - loss: 7.6187 - accuracy: 0.5031
  352/25000 [..............................] - ETA: 1:15 - loss: 7.6666 - accuracy: 0.5000
  384/25000 [..............................] - ETA: 1:13 - loss: 7.6666 - accuracy: 0.5000
  416/25000 [..............................] - ETA: 1:12 - loss: 7.5929 - accuracy: 0.5048
  448/25000 [..............................] - ETA: 1:11 - loss: 7.6666 - accuracy: 0.5000
  480/25000 [..............................] - ETA: 1:10 - loss: 7.6666 - accuracy: 0.5000
  512/25000 [..............................] - ETA: 1:09 - loss: 7.6966 - accuracy: 0.4980
  544/25000 [..............................] - ETA: 1:08 - loss: 7.8075 - accuracy: 0.4908
  576/25000 [..............................] - ETA: 1:08 - loss: 7.7465 - accuracy: 0.4948
  608/25000 [..............................] - ETA: 1:07 - loss: 7.6666 - accuracy: 0.5000
  640/25000 [..............................] - ETA: 1:06 - loss: 7.7145 - accuracy: 0.4969
  672/25000 [..............................] - ETA: 1:06 - loss: 7.8035 - accuracy: 0.4911
  704/25000 [..............................] - ETA: 1:06 - loss: 7.9715 - accuracy: 0.4801
  736/25000 [..............................] - ETA: 1:06 - loss: 7.9375 - accuracy: 0.4823
  768/25000 [..............................] - ETA: 1:05 - loss: 7.9661 - accuracy: 0.4805
  800/25000 [..............................] - ETA: 1:05 - loss: 7.9158 - accuracy: 0.4837
  832/25000 [..............................] - ETA: 1:05 - loss: 7.8693 - accuracy: 0.4868
  864/25000 [>.............................] - ETA: 1:04 - loss: 7.9683 - accuracy: 0.4803
  896/25000 [>.............................] - ETA: 1:04 - loss: 7.9918 - accuracy: 0.4788
  928/25000 [>.............................] - ETA: 1:03 - loss: 8.0466 - accuracy: 0.4752
  960/25000 [>.............................] - ETA: 1:03 - loss: 8.0819 - accuracy: 0.4729
  992/25000 [>.............................] - ETA: 1:02 - loss: 8.0067 - accuracy: 0.4778
 1024/25000 [>.............................] - ETA: 1:02 - loss: 7.9661 - accuracy: 0.4805
 1056/25000 [>.............................] - ETA: 1:02 - loss: 7.9280 - accuracy: 0.4830
 1088/25000 [>.............................] - ETA: 1:02 - loss: 7.9344 - accuracy: 0.4825
 1120/25000 [>.............................] - ETA: 1:01 - loss: 7.8857 - accuracy: 0.4857
 1152/25000 [>.............................] - ETA: 1:01 - loss: 7.9328 - accuracy: 0.4826
 1184/25000 [>.............................] - ETA: 1:01 - loss: 7.9904 - accuracy: 0.4789
 1216/25000 [>.............................] - ETA: 1:00 - loss: 7.9819 - accuracy: 0.4794
 1248/25000 [>.............................] - ETA: 1:00 - loss: 7.9001 - accuracy: 0.4848
 1280/25000 [>.............................] - ETA: 1:00 - loss: 7.8822 - accuracy: 0.4859
 1312/25000 [>.............................] - ETA: 59s - loss: 7.9588 - accuracy: 0.4809 
 1344/25000 [>.............................] - ETA: 59s - loss: 7.9518 - accuracy: 0.4814
 1376/25000 [>.............................] - ETA: 59s - loss: 7.9341 - accuracy: 0.4826
 1408/25000 [>.............................] - ETA: 59s - loss: 7.9280 - accuracy: 0.4830
 1440/25000 [>.............................] - ETA: 59s - loss: 7.8902 - accuracy: 0.4854
 1472/25000 [>.............................] - ETA: 59s - loss: 7.8645 - accuracy: 0.4871
 1504/25000 [>.............................] - ETA: 58s - loss: 7.8603 - accuracy: 0.4874
 1536/25000 [>.............................] - ETA: 58s - loss: 7.8663 - accuracy: 0.4870
 1568/25000 [>.............................] - ETA: 58s - loss: 7.8818 - accuracy: 0.4860
 1600/25000 [>.............................] - ETA: 58s - loss: 7.9062 - accuracy: 0.4844
 1632/25000 [>.............................] - ETA: 58s - loss: 7.9015 - accuracy: 0.4847
 1664/25000 [>.............................] - ETA: 58s - loss: 7.9246 - accuracy: 0.4832
 1696/25000 [=>............................] - ETA: 58s - loss: 7.9288 - accuracy: 0.4829
 1728/25000 [=>............................] - ETA: 57s - loss: 7.9506 - accuracy: 0.4815
 1760/25000 [=>............................] - ETA: 57s - loss: 7.9454 - accuracy: 0.4818
 1792/25000 [=>............................] - ETA: 57s - loss: 7.9832 - accuracy: 0.4794
 1824/25000 [=>............................] - ETA: 57s - loss: 7.9608 - accuracy: 0.4808
 1856/25000 [=>............................] - ETA: 57s - loss: 7.9558 - accuracy: 0.4811
 1888/25000 [=>............................] - ETA: 57s - loss: 7.9590 - accuracy: 0.4809
 1920/25000 [=>............................] - ETA: 57s - loss: 7.9381 - accuracy: 0.4823
 1952/25000 [=>............................] - ETA: 57s - loss: 7.9415 - accuracy: 0.4821
 1984/25000 [=>............................] - ETA: 56s - loss: 7.9217 - accuracy: 0.4834
 2016/25000 [=>............................] - ETA: 56s - loss: 7.9252 - accuracy: 0.4831
 2048/25000 [=>............................] - ETA: 56s - loss: 7.9436 - accuracy: 0.4819
 2080/25000 [=>............................] - ETA: 56s - loss: 7.9615 - accuracy: 0.4808
 2112/25000 [=>............................] - ETA: 56s - loss: 7.9498 - accuracy: 0.4815
 2144/25000 [=>............................] - ETA: 56s - loss: 7.9026 - accuracy: 0.4846
 2176/25000 [=>............................] - ETA: 56s - loss: 7.9273 - accuracy: 0.4830
 2208/25000 [=>............................] - ETA: 56s - loss: 7.9722 - accuracy: 0.4801
 2240/25000 [=>............................] - ETA: 56s - loss: 7.9678 - accuracy: 0.4804
 2272/25000 [=>............................] - ETA: 56s - loss: 7.9366 - accuracy: 0.4824
 2304/25000 [=>............................] - ETA: 55s - loss: 7.9062 - accuracy: 0.4844
 2336/25000 [=>............................] - ETA: 55s - loss: 7.8964 - accuracy: 0.4850
 2368/25000 [=>............................] - ETA: 55s - loss: 7.8674 - accuracy: 0.4869
 2400/25000 [=>............................] - ETA: 55s - loss: 7.8647 - accuracy: 0.4871
 2432/25000 [=>............................] - ETA: 55s - loss: 7.8305 - accuracy: 0.4893
 2464/25000 [=>............................] - ETA: 55s - loss: 7.8035 - accuracy: 0.4911
 2496/25000 [=>............................] - ETA: 55s - loss: 7.8079 - accuracy: 0.4908
 2528/25000 [==>...........................] - ETA: 55s - loss: 7.7879 - accuracy: 0.4921
 2560/25000 [==>...........................] - ETA: 55s - loss: 7.7864 - accuracy: 0.4922
 2592/25000 [==>...........................] - ETA: 55s - loss: 7.7435 - accuracy: 0.4950
 2624/25000 [==>...........................] - ETA: 55s - loss: 7.7426 - accuracy: 0.4950
 2656/25000 [==>...........................] - ETA: 55s - loss: 7.7301 - accuracy: 0.4959
 2688/25000 [==>...........................] - ETA: 54s - loss: 7.7351 - accuracy: 0.4955
 2720/25000 [==>...........................] - ETA: 54s - loss: 7.7455 - accuracy: 0.4949
 2752/25000 [==>...........................] - ETA: 54s - loss: 7.7279 - accuracy: 0.4960
 2784/25000 [==>...........................] - ETA: 54s - loss: 7.7162 - accuracy: 0.4968
 2816/25000 [==>...........................] - ETA: 54s - loss: 7.7211 - accuracy: 0.4964
 2848/25000 [==>...........................] - ETA: 54s - loss: 7.7151 - accuracy: 0.4968
 2880/25000 [==>...........................] - ETA: 54s - loss: 7.7358 - accuracy: 0.4955
 2912/25000 [==>...........................] - ETA: 54s - loss: 7.7245 - accuracy: 0.4962
 2944/25000 [==>...........................] - ETA: 54s - loss: 7.7083 - accuracy: 0.4973
 2976/25000 [==>...........................] - ETA: 54s - loss: 7.7130 - accuracy: 0.4970
 3008/25000 [==>...........................] - ETA: 53s - loss: 7.7125 - accuracy: 0.4970
 3040/25000 [==>...........................] - ETA: 53s - loss: 7.7120 - accuracy: 0.4970
 3072/25000 [==>...........................] - ETA: 53s - loss: 7.7115 - accuracy: 0.4971
 3104/25000 [==>...........................] - ETA: 53s - loss: 7.7061 - accuracy: 0.4974
 3136/25000 [==>...........................] - ETA: 53s - loss: 7.6960 - accuracy: 0.4981
 3168/25000 [==>...........................] - ETA: 53s - loss: 7.7150 - accuracy: 0.4968
 3200/25000 [==>...........................] - ETA: 53s - loss: 7.7002 - accuracy: 0.4978
 3232/25000 [==>...........................] - ETA: 53s - loss: 7.6998 - accuracy: 0.4978
 3264/25000 [==>...........................] - ETA: 53s - loss: 7.7042 - accuracy: 0.4975
 3296/25000 [==>...........................] - ETA: 53s - loss: 7.7224 - accuracy: 0.4964
 3328/25000 [==>...........................] - ETA: 53s - loss: 7.7311 - accuracy: 0.4958
 3360/25000 [===>..........................] - ETA: 52s - loss: 7.7077 - accuracy: 0.4973
 3392/25000 [===>..........................] - ETA: 52s - loss: 7.7389 - accuracy: 0.4953
 3424/25000 [===>..........................] - ETA: 52s - loss: 7.7383 - accuracy: 0.4953
 3456/25000 [===>..........................] - ETA: 52s - loss: 7.7465 - accuracy: 0.4948
 3488/25000 [===>..........................] - ETA: 52s - loss: 7.7633 - accuracy: 0.4937
 3520/25000 [===>..........................] - ETA: 52s - loss: 7.7712 - accuracy: 0.4932
 3552/25000 [===>..........................] - ETA: 52s - loss: 7.7659 - accuracy: 0.4935
 3584/25000 [===>..........................] - ETA: 52s - loss: 7.7693 - accuracy: 0.4933
 3616/25000 [===>..........................] - ETA: 52s - loss: 7.7684 - accuracy: 0.4934
 3648/25000 [===>..........................] - ETA: 52s - loss: 7.7717 - accuracy: 0.4931
 3680/25000 [===>..........................] - ETA: 52s - loss: 7.7708 - accuracy: 0.4932
 3712/25000 [===>..........................] - ETA: 51s - loss: 7.7740 - accuracy: 0.4930
 3744/25000 [===>..........................] - ETA: 51s - loss: 7.7731 - accuracy: 0.4931
 3776/25000 [===>..........................] - ETA: 51s - loss: 7.7844 - accuracy: 0.4923
 3808/25000 [===>..........................] - ETA: 51s - loss: 7.7713 - accuracy: 0.4932
 3840/25000 [===>..........................] - ETA: 51s - loss: 7.7784 - accuracy: 0.4927
 3872/25000 [===>..........................] - ETA: 51s - loss: 7.7735 - accuracy: 0.4930
 3904/25000 [===>..........................] - ETA: 51s - loss: 7.7530 - accuracy: 0.4944
 3936/25000 [===>..........................] - ETA: 51s - loss: 7.7718 - accuracy: 0.4931
 3968/25000 [===>..........................] - ETA: 51s - loss: 7.7787 - accuracy: 0.4927
 4000/25000 [===>..........................] - ETA: 51s - loss: 7.7855 - accuracy: 0.4922
 4032/25000 [===>..........................] - ETA: 51s - loss: 7.7807 - accuracy: 0.4926
 4064/25000 [===>..........................] - ETA: 51s - loss: 7.7760 - accuracy: 0.4929
 4096/25000 [===>..........................] - ETA: 51s - loss: 7.7677 - accuracy: 0.4934
 4128/25000 [===>..........................] - ETA: 50s - loss: 7.7632 - accuracy: 0.4937
 4160/25000 [===>..........................] - ETA: 50s - loss: 7.7735 - accuracy: 0.4930
 4192/25000 [====>.........................] - ETA: 50s - loss: 7.7654 - accuracy: 0.4936
 4224/25000 [====>.........................] - ETA: 50s - loss: 7.7646 - accuracy: 0.4936
 4256/25000 [====>.........................] - ETA: 50s - loss: 7.7675 - accuracy: 0.4934
 4288/25000 [====>.........................] - ETA: 50s - loss: 7.7632 - accuracy: 0.4937
 4320/25000 [====>.........................] - ETA: 50s - loss: 7.7589 - accuracy: 0.4940
 4352/25000 [====>.........................] - ETA: 50s - loss: 7.7653 - accuracy: 0.4936
 4384/25000 [====>.........................] - ETA: 50s - loss: 7.7646 - accuracy: 0.4936
 4416/25000 [====>.........................] - ETA: 50s - loss: 7.7708 - accuracy: 0.4932
 4448/25000 [====>.........................] - ETA: 50s - loss: 7.7631 - accuracy: 0.4937
 4480/25000 [====>.........................] - ETA: 50s - loss: 7.7556 - accuracy: 0.4942
 4512/25000 [====>.........................] - ETA: 50s - loss: 7.7584 - accuracy: 0.4940
 4544/25000 [====>.........................] - ETA: 49s - loss: 7.7679 - accuracy: 0.4934
 4576/25000 [====>.........................] - ETA: 49s - loss: 7.7738 - accuracy: 0.4930
 4608/25000 [====>.........................] - ETA: 49s - loss: 7.7764 - accuracy: 0.4928
 4640/25000 [====>.........................] - ETA: 49s - loss: 7.7724 - accuracy: 0.4931
 4672/25000 [====>.........................] - ETA: 49s - loss: 7.7848 - accuracy: 0.4923
 4704/25000 [====>.........................] - ETA: 49s - loss: 7.7840 - accuracy: 0.4923
 4736/25000 [====>.........................] - ETA: 49s - loss: 7.7832 - accuracy: 0.4924
 4768/25000 [====>.........................] - ETA: 49s - loss: 7.7920 - accuracy: 0.4918
 4800/25000 [====>.........................] - ETA: 49s - loss: 7.7976 - accuracy: 0.4915
 4832/25000 [====>.........................] - ETA: 49s - loss: 7.7840 - accuracy: 0.4923
 4864/25000 [====>.........................] - ETA: 49s - loss: 7.7959 - accuracy: 0.4916
 4896/25000 [====>.........................] - ETA: 48s - loss: 7.7825 - accuracy: 0.4924
 4928/25000 [====>.........................] - ETA: 48s - loss: 7.7755 - accuracy: 0.4929
 4960/25000 [====>.........................] - ETA: 48s - loss: 7.7655 - accuracy: 0.4935
 4992/25000 [====>.........................] - ETA: 48s - loss: 7.7649 - accuracy: 0.4936
 5024/25000 [=====>........................] - ETA: 48s - loss: 7.7582 - accuracy: 0.4940
 5056/25000 [=====>........................] - ETA: 48s - loss: 7.7546 - accuracy: 0.4943
 5088/25000 [=====>........................] - ETA: 48s - loss: 7.7480 - accuracy: 0.4947
 5120/25000 [=====>........................] - ETA: 48s - loss: 7.7445 - accuracy: 0.4949
 5152/25000 [=====>........................] - ETA: 48s - loss: 7.7410 - accuracy: 0.4951
 5184/25000 [=====>........................] - ETA: 48s - loss: 7.7317 - accuracy: 0.4958
 5216/25000 [=====>........................] - ETA: 48s - loss: 7.7254 - accuracy: 0.4962
 5248/25000 [=====>........................] - ETA: 48s - loss: 7.7104 - accuracy: 0.4971
 5280/25000 [=====>........................] - ETA: 48s - loss: 7.6957 - accuracy: 0.4981
 5312/25000 [=====>........................] - ETA: 47s - loss: 7.7041 - accuracy: 0.4976
 5344/25000 [=====>........................] - ETA: 47s - loss: 7.7125 - accuracy: 0.4970
 5376/25000 [=====>........................] - ETA: 47s - loss: 7.7294 - accuracy: 0.4959
 5408/25000 [=====>........................] - ETA: 47s - loss: 7.7290 - accuracy: 0.4959
 5440/25000 [=====>........................] - ETA: 47s - loss: 7.7286 - accuracy: 0.4960
 5472/25000 [=====>........................] - ETA: 47s - loss: 7.7339 - accuracy: 0.4956
 5504/25000 [=====>........................] - ETA: 47s - loss: 7.7335 - accuracy: 0.4956
 5536/25000 [=====>........................] - ETA: 47s - loss: 7.7331 - accuracy: 0.4957
 5568/25000 [=====>........................] - ETA: 47s - loss: 7.7355 - accuracy: 0.4955
 5600/25000 [=====>........................] - ETA: 47s - loss: 7.7323 - accuracy: 0.4957
 5632/25000 [=====>........................] - ETA: 47s - loss: 7.7129 - accuracy: 0.4970
 5664/25000 [=====>........................] - ETA: 47s - loss: 7.7099 - accuracy: 0.4972
 5696/25000 [=====>........................] - ETA: 46s - loss: 7.7151 - accuracy: 0.4968
 5728/25000 [=====>........................] - ETA: 46s - loss: 7.7175 - accuracy: 0.4967
 5760/25000 [=====>........................] - ETA: 46s - loss: 7.7172 - accuracy: 0.4967
 5792/25000 [=====>........................] - ETA: 46s - loss: 7.7196 - accuracy: 0.4965
 5824/25000 [=====>........................] - ETA: 46s - loss: 7.7140 - accuracy: 0.4969
 5856/25000 [======>.......................] - ETA: 46s - loss: 7.7242 - accuracy: 0.4962
 5888/25000 [======>.......................] - ETA: 46s - loss: 7.7161 - accuracy: 0.4968
 5920/25000 [======>.......................] - ETA: 46s - loss: 7.7236 - accuracy: 0.4963
 5952/25000 [======>.......................] - ETA: 46s - loss: 7.7233 - accuracy: 0.4963
 5984/25000 [======>.......................] - ETA: 46s - loss: 7.7204 - accuracy: 0.4965
 6016/25000 [======>.......................] - ETA: 46s - loss: 7.7278 - accuracy: 0.4960
 6048/25000 [======>.......................] - ETA: 46s - loss: 7.7173 - accuracy: 0.4967
 6080/25000 [======>.......................] - ETA: 46s - loss: 7.7120 - accuracy: 0.4970
 6112/25000 [======>.......................] - ETA: 46s - loss: 7.7118 - accuracy: 0.4971
 6144/25000 [======>.......................] - ETA: 45s - loss: 7.7115 - accuracy: 0.4971
 6176/25000 [======>.......................] - ETA: 45s - loss: 7.7138 - accuracy: 0.4969
 6208/25000 [======>.......................] - ETA: 45s - loss: 7.7037 - accuracy: 0.4976
 6240/25000 [======>.......................] - ETA: 45s - loss: 7.7059 - accuracy: 0.4974
 6272/25000 [======>.......................] - ETA: 45s - loss: 7.7008 - accuracy: 0.4978
 6304/25000 [======>.......................] - ETA: 45s - loss: 7.7104 - accuracy: 0.4971
 6336/25000 [======>.......................] - ETA: 45s - loss: 7.7053 - accuracy: 0.4975
 6368/25000 [======>.......................] - ETA: 45s - loss: 7.7076 - accuracy: 0.4973
 6400/25000 [======>.......................] - ETA: 45s - loss: 7.7145 - accuracy: 0.4969
 6432/25000 [======>.......................] - ETA: 45s - loss: 7.7286 - accuracy: 0.4960
 6464/25000 [======>.......................] - ETA: 45s - loss: 7.7212 - accuracy: 0.4964
 6496/25000 [======>.......................] - ETA: 45s - loss: 7.7185 - accuracy: 0.4966
 6528/25000 [======>.......................] - ETA: 45s - loss: 7.7183 - accuracy: 0.4966
 6560/25000 [======>.......................] - ETA: 44s - loss: 7.7157 - accuracy: 0.4968
 6592/25000 [======>.......................] - ETA: 44s - loss: 7.7155 - accuracy: 0.4968
 6624/25000 [======>.......................] - ETA: 44s - loss: 7.7175 - accuracy: 0.4967
 6656/25000 [======>.......................] - ETA: 44s - loss: 7.7104 - accuracy: 0.4971
 6688/25000 [=======>......................] - ETA: 44s - loss: 7.7125 - accuracy: 0.4970
 6720/25000 [=======>......................] - ETA: 44s - loss: 7.7191 - accuracy: 0.4966
 6752/25000 [=======>......................] - ETA: 44s - loss: 7.7120 - accuracy: 0.4970
 6784/25000 [=======>......................] - ETA: 44s - loss: 7.7186 - accuracy: 0.4966
 6816/25000 [=======>......................] - ETA: 44s - loss: 7.7094 - accuracy: 0.4972
 6848/25000 [=======>......................] - ETA: 44s - loss: 7.7181 - accuracy: 0.4966
 6880/25000 [=======>......................] - ETA: 44s - loss: 7.7090 - accuracy: 0.4972
 6912/25000 [=======>......................] - ETA: 44s - loss: 7.7065 - accuracy: 0.4974
 6944/25000 [=======>......................] - ETA: 44s - loss: 7.7042 - accuracy: 0.4976
 6976/25000 [=======>......................] - ETA: 44s - loss: 7.7018 - accuracy: 0.4977
 7008/25000 [=======>......................] - ETA: 43s - loss: 7.6951 - accuracy: 0.4981
 7040/25000 [=======>......................] - ETA: 43s - loss: 7.6993 - accuracy: 0.4979
 7072/25000 [=======>......................] - ETA: 43s - loss: 7.7100 - accuracy: 0.4972
 7104/25000 [=======>......................] - ETA: 43s - loss: 7.7163 - accuracy: 0.4968
 7136/25000 [=======>......................] - ETA: 43s - loss: 7.7139 - accuracy: 0.4969
 7168/25000 [=======>......................] - ETA: 43s - loss: 7.7158 - accuracy: 0.4968
 7200/25000 [=======>......................] - ETA: 43s - loss: 7.7156 - accuracy: 0.4968
 7232/25000 [=======>......................] - ETA: 43s - loss: 7.7111 - accuracy: 0.4971
 7264/25000 [=======>......................] - ETA: 43s - loss: 7.7046 - accuracy: 0.4975
 7296/25000 [=======>......................] - ETA: 43s - loss: 7.7065 - accuracy: 0.4974
 7328/25000 [=======>......................] - ETA: 43s - loss: 7.7147 - accuracy: 0.4969
 7360/25000 [=======>......................] - ETA: 43s - loss: 7.7104 - accuracy: 0.4971
 7392/25000 [=======>......................] - ETA: 42s - loss: 7.7019 - accuracy: 0.4977
 7424/25000 [=======>......................] - ETA: 42s - loss: 7.6976 - accuracy: 0.4980
 7456/25000 [=======>......................] - ETA: 42s - loss: 7.6872 - accuracy: 0.4987
 7488/25000 [=======>......................] - ETA: 42s - loss: 7.6871 - accuracy: 0.4987
 7520/25000 [========>.....................] - ETA: 42s - loss: 7.6931 - accuracy: 0.4983
 7552/25000 [========>.....................] - ETA: 42s - loss: 7.6950 - accuracy: 0.4981
 7584/25000 [========>.....................] - ETA: 42s - loss: 7.6889 - accuracy: 0.4985
 7616/25000 [========>.....................] - ETA: 42s - loss: 7.6928 - accuracy: 0.4983
 7648/25000 [========>.....................] - ETA: 42s - loss: 7.7007 - accuracy: 0.4978
 7680/25000 [========>.....................] - ETA: 42s - loss: 7.7046 - accuracy: 0.4975
 7712/25000 [========>.....................] - ETA: 42s - loss: 7.7104 - accuracy: 0.4971
 7744/25000 [========>.....................] - ETA: 42s - loss: 7.7082 - accuracy: 0.4973
 7776/25000 [========>.....................] - ETA: 42s - loss: 7.7120 - accuracy: 0.4970
 7808/25000 [========>.....................] - ETA: 41s - loss: 7.7098 - accuracy: 0.4972
 7840/25000 [========>.....................] - ETA: 41s - loss: 7.7136 - accuracy: 0.4969
 7872/25000 [========>.....................] - ETA: 41s - loss: 7.7173 - accuracy: 0.4967
 7904/25000 [========>.....................] - ETA: 41s - loss: 7.7112 - accuracy: 0.4971
 7936/25000 [========>.....................] - ETA: 41s - loss: 7.7169 - accuracy: 0.4967
 7968/25000 [========>.....................] - ETA: 41s - loss: 7.7128 - accuracy: 0.4970
 8000/25000 [========>.....................] - ETA: 41s - loss: 7.7165 - accuracy: 0.4967
 8032/25000 [========>.....................] - ETA: 41s - loss: 7.7220 - accuracy: 0.4964
 8064/25000 [========>.....................] - ETA: 41s - loss: 7.7275 - accuracy: 0.4960
 8096/25000 [========>.....................] - ETA: 41s - loss: 7.7329 - accuracy: 0.4957
 8128/25000 [========>.....................] - ETA: 41s - loss: 7.7402 - accuracy: 0.4952
 8160/25000 [========>.....................] - ETA: 41s - loss: 7.7324 - accuracy: 0.4957
 8192/25000 [========>.....................] - ETA: 41s - loss: 7.7415 - accuracy: 0.4951
 8224/25000 [========>.....................] - ETA: 40s - loss: 7.7412 - accuracy: 0.4951
 8256/25000 [========>.....................] - ETA: 40s - loss: 7.7446 - accuracy: 0.4949
 8288/25000 [========>.....................] - ETA: 40s - loss: 7.7406 - accuracy: 0.4952
 8320/25000 [========>.....................] - ETA: 40s - loss: 7.7385 - accuracy: 0.4953
 8352/25000 [=========>....................] - ETA: 40s - loss: 7.7456 - accuracy: 0.4949
 8384/25000 [=========>....................] - ETA: 40s - loss: 7.7416 - accuracy: 0.4951
 8416/25000 [=========>....................] - ETA: 40s - loss: 7.7413 - accuracy: 0.4951
 8448/25000 [=========>....................] - ETA: 40s - loss: 7.7356 - accuracy: 0.4955
 8480/25000 [=========>....................] - ETA: 40s - loss: 7.7426 - accuracy: 0.4950
 8512/25000 [=========>....................] - ETA: 40s - loss: 7.7441 - accuracy: 0.4949
 8544/25000 [=========>....................] - ETA: 40s - loss: 7.7348 - accuracy: 0.4956
 8576/25000 [=========>....................] - ETA: 40s - loss: 7.7346 - accuracy: 0.4956
 8608/25000 [=========>....................] - ETA: 40s - loss: 7.7307 - accuracy: 0.4958
 8640/25000 [=========>....................] - ETA: 39s - loss: 7.7394 - accuracy: 0.4953
 8672/25000 [=========>....................] - ETA: 39s - loss: 7.7426 - accuracy: 0.4950
 8704/25000 [=========>....................] - ETA: 39s - loss: 7.7477 - accuracy: 0.4947
 8736/25000 [=========>....................] - ETA: 39s - loss: 7.7456 - accuracy: 0.4948
 8768/25000 [=========>....................] - ETA: 39s - loss: 7.7418 - accuracy: 0.4951
 8800/25000 [=========>....................] - ETA: 39s - loss: 7.7363 - accuracy: 0.4955
 8832/25000 [=========>....................] - ETA: 39s - loss: 7.7378 - accuracy: 0.4954
 8864/25000 [=========>....................] - ETA: 39s - loss: 7.7445 - accuracy: 0.4949
 8896/25000 [=========>....................] - ETA: 39s - loss: 7.7373 - accuracy: 0.4954
 8928/25000 [=========>....................] - ETA: 39s - loss: 7.7439 - accuracy: 0.4950
 8960/25000 [=========>....................] - ETA: 39s - loss: 7.7436 - accuracy: 0.4950
 8992/25000 [=========>....................] - ETA: 39s - loss: 7.7416 - accuracy: 0.4951
 9024/25000 [=========>....................] - ETA: 39s - loss: 7.7397 - accuracy: 0.4952
 9056/25000 [=========>....................] - ETA: 39s - loss: 7.7428 - accuracy: 0.4950
 9088/25000 [=========>....................] - ETA: 38s - loss: 7.7341 - accuracy: 0.4956
 9120/25000 [=========>....................] - ETA: 38s - loss: 7.7204 - accuracy: 0.4965
 9152/25000 [=========>....................] - ETA: 38s - loss: 7.7152 - accuracy: 0.4968
 9184/25000 [==========>...................] - ETA: 38s - loss: 7.7084 - accuracy: 0.4973
 9216/25000 [==========>...................] - ETA: 38s - loss: 7.7082 - accuracy: 0.4973
 9248/25000 [==========>...................] - ETA: 38s - loss: 7.7064 - accuracy: 0.4974
 9280/25000 [==========>...................] - ETA: 38s - loss: 7.6997 - accuracy: 0.4978
 9312/25000 [==========>...................] - ETA: 38s - loss: 7.6946 - accuracy: 0.4982
 9344/25000 [==========>...................] - ETA: 38s - loss: 7.6962 - accuracy: 0.4981
 9376/25000 [==========>...................] - ETA: 38s - loss: 7.6912 - accuracy: 0.4984
 9408/25000 [==========>...................] - ETA: 38s - loss: 7.6894 - accuracy: 0.4985
 9440/25000 [==========>...................] - ETA: 38s - loss: 7.6975 - accuracy: 0.4980
 9472/25000 [==========>...................] - ETA: 38s - loss: 7.7039 - accuracy: 0.4976
 9504/25000 [==========>...................] - ETA: 37s - loss: 7.6973 - accuracy: 0.4980
 9536/25000 [==========>...................] - ETA: 37s - loss: 7.7004 - accuracy: 0.4978
 9568/25000 [==========>...................] - ETA: 37s - loss: 7.6955 - accuracy: 0.4981
 9600/25000 [==========>...................] - ETA: 37s - loss: 7.6922 - accuracy: 0.4983
 9632/25000 [==========>...................] - ETA: 37s - loss: 7.6969 - accuracy: 0.4980
 9664/25000 [==========>...................] - ETA: 37s - loss: 7.6952 - accuracy: 0.4981
 9696/25000 [==========>...................] - ETA: 37s - loss: 7.6935 - accuracy: 0.4982
 9728/25000 [==========>...................] - ETA: 37s - loss: 7.6887 - accuracy: 0.4986
 9760/25000 [==========>...................] - ETA: 37s - loss: 7.6949 - accuracy: 0.4982
 9792/25000 [==========>...................] - ETA: 37s - loss: 7.6901 - accuracy: 0.4985
 9824/25000 [==========>...................] - ETA: 37s - loss: 7.6885 - accuracy: 0.4986
 9856/25000 [==========>...................] - ETA: 37s - loss: 7.6977 - accuracy: 0.4980
 9888/25000 [==========>...................] - ETA: 36s - loss: 7.7007 - accuracy: 0.4978
 9920/25000 [==========>...................] - ETA: 36s - loss: 7.7099 - accuracy: 0.4972
 9952/25000 [==========>...................] - ETA: 36s - loss: 7.7067 - accuracy: 0.4974
 9984/25000 [==========>...................] - ETA: 36s - loss: 7.7142 - accuracy: 0.4969
10016/25000 [===========>..................] - ETA: 36s - loss: 7.7110 - accuracy: 0.4971
10048/25000 [===========>..................] - ETA: 36s - loss: 7.7063 - accuracy: 0.4974
10080/25000 [===========>..................] - ETA: 36s - loss: 7.7123 - accuracy: 0.4970
10112/25000 [===========>..................] - ETA: 36s - loss: 7.7167 - accuracy: 0.4967
10144/25000 [===========>..................] - ETA: 36s - loss: 7.7089 - accuracy: 0.4972
10176/25000 [===========>..................] - ETA: 36s - loss: 7.7148 - accuracy: 0.4969
10208/25000 [===========>..................] - ETA: 36s - loss: 7.7117 - accuracy: 0.4971
10240/25000 [===========>..................] - ETA: 36s - loss: 7.7130 - accuracy: 0.4970
10272/25000 [===========>..................] - ETA: 36s - loss: 7.7129 - accuracy: 0.4970
10304/25000 [===========>..................] - ETA: 35s - loss: 7.7098 - accuracy: 0.4972
10336/25000 [===========>..................] - ETA: 35s - loss: 7.7052 - accuracy: 0.4975
10368/25000 [===========>..................] - ETA: 35s - loss: 7.7065 - accuracy: 0.4974
10400/25000 [===========>..................] - ETA: 35s - loss: 7.7079 - accuracy: 0.4973
10432/25000 [===========>..................] - ETA: 35s - loss: 7.7107 - accuracy: 0.4971
10464/25000 [===========>..................] - ETA: 35s - loss: 7.7150 - accuracy: 0.4968
10496/25000 [===========>..................] - ETA: 35s - loss: 7.7046 - accuracy: 0.4975
10528/25000 [===========>..................] - ETA: 35s - loss: 7.7045 - accuracy: 0.4975
10560/25000 [===========>..................] - ETA: 35s - loss: 7.7029 - accuracy: 0.4976
10592/25000 [===========>..................] - ETA: 35s - loss: 7.7086 - accuracy: 0.4973
10624/25000 [===========>..................] - ETA: 35s - loss: 7.7157 - accuracy: 0.4968
10656/25000 [===========>..................] - ETA: 35s - loss: 7.7055 - accuracy: 0.4975
10688/25000 [===========>..................] - ETA: 34s - loss: 7.7054 - accuracy: 0.4975
10720/25000 [===========>..................] - ETA: 34s - loss: 7.7052 - accuracy: 0.4975
10752/25000 [===========>..................] - ETA: 34s - loss: 7.7051 - accuracy: 0.4975
10784/25000 [===========>..................] - ETA: 34s - loss: 7.7064 - accuracy: 0.4974
10816/25000 [===========>..................] - ETA: 34s - loss: 7.7134 - accuracy: 0.4969
10848/25000 [============>.................] - ETA: 34s - loss: 7.7090 - accuracy: 0.4972
10880/25000 [============>.................] - ETA: 34s - loss: 7.7047 - accuracy: 0.4975
10912/25000 [============>.................] - ETA: 34s - loss: 7.7060 - accuracy: 0.4974
10944/25000 [============>.................] - ETA: 34s - loss: 7.7129 - accuracy: 0.4970
10976/25000 [============>.................] - ETA: 34s - loss: 7.7141 - accuracy: 0.4969
11008/25000 [============>.................] - ETA: 34s - loss: 7.7154 - accuracy: 0.4968
11040/25000 [============>.................] - ETA: 34s - loss: 7.7180 - accuracy: 0.4966
11072/25000 [============>.................] - ETA: 34s - loss: 7.7206 - accuracy: 0.4965
11104/25000 [============>.................] - ETA: 33s - loss: 7.7232 - accuracy: 0.4963
11136/25000 [============>.................] - ETA: 33s - loss: 7.7231 - accuracy: 0.4963
11168/25000 [============>.................] - ETA: 33s - loss: 7.7202 - accuracy: 0.4965
11200/25000 [============>.................] - ETA: 33s - loss: 7.7214 - accuracy: 0.4964
11232/25000 [============>.................] - ETA: 33s - loss: 7.7281 - accuracy: 0.4960
11264/25000 [============>.................] - ETA: 33s - loss: 7.7320 - accuracy: 0.4957
11296/25000 [============>.................] - ETA: 33s - loss: 7.7358 - accuracy: 0.4955
11328/25000 [============>.................] - ETA: 33s - loss: 7.7370 - accuracy: 0.4954
11360/25000 [============>.................] - ETA: 33s - loss: 7.7395 - accuracy: 0.4952
11392/25000 [============>.................] - ETA: 33s - loss: 7.7393 - accuracy: 0.4953
11424/25000 [============>.................] - ETA: 33s - loss: 7.7364 - accuracy: 0.4954
11456/25000 [============>.................] - ETA: 33s - loss: 7.7349 - accuracy: 0.4955
11488/25000 [============>.................] - ETA: 33s - loss: 7.7334 - accuracy: 0.4956
11520/25000 [============>.................] - ETA: 32s - loss: 7.7398 - accuracy: 0.4952
11552/25000 [============>.................] - ETA: 32s - loss: 7.7396 - accuracy: 0.4952
11584/25000 [============>.................] - ETA: 32s - loss: 7.7447 - accuracy: 0.4949
11616/25000 [============>.................] - ETA: 32s - loss: 7.7419 - accuracy: 0.4951
11648/25000 [============>.................] - ETA: 32s - loss: 7.7443 - accuracy: 0.4949
11680/25000 [=============>................] - ETA: 32s - loss: 7.7428 - accuracy: 0.4950
11712/25000 [=============>................] - ETA: 32s - loss: 7.7386 - accuracy: 0.4953
11744/25000 [=============>................] - ETA: 32s - loss: 7.7358 - accuracy: 0.4955
11776/25000 [=============>................] - ETA: 32s - loss: 7.7356 - accuracy: 0.4955
11808/25000 [=============>................] - ETA: 32s - loss: 7.7341 - accuracy: 0.4956
11840/25000 [=============>................] - ETA: 32s - loss: 7.7366 - accuracy: 0.4954
11872/25000 [=============>................] - ETA: 32s - loss: 7.7338 - accuracy: 0.4956
11904/25000 [=============>................] - ETA: 32s - loss: 7.7336 - accuracy: 0.4956
11936/25000 [=============>................] - ETA: 32s - loss: 7.7373 - accuracy: 0.4954
11968/25000 [=============>................] - ETA: 31s - loss: 7.7409 - accuracy: 0.4952
12000/25000 [=============>................] - ETA: 31s - loss: 7.7497 - accuracy: 0.4946
12032/25000 [=============>................] - ETA: 31s - loss: 7.7533 - accuracy: 0.4943
12064/25000 [=============>................] - ETA: 31s - loss: 7.7543 - accuracy: 0.4943
12096/25000 [=============>................] - ETA: 31s - loss: 7.7541 - accuracy: 0.4943
12128/25000 [=============>................] - ETA: 31s - loss: 7.7513 - accuracy: 0.4945
12160/25000 [=============>................] - ETA: 31s - loss: 7.7511 - accuracy: 0.4945
12192/25000 [=============>................] - ETA: 31s - loss: 7.7484 - accuracy: 0.4947
12224/25000 [=============>................] - ETA: 31s - loss: 7.7431 - accuracy: 0.4950
12256/25000 [=============>................] - ETA: 31s - loss: 7.7379 - accuracy: 0.4953
12288/25000 [=============>................] - ETA: 31s - loss: 7.7365 - accuracy: 0.4954
12320/25000 [=============>................] - ETA: 31s - loss: 7.7313 - accuracy: 0.4958
12352/25000 [=============>................] - ETA: 30s - loss: 7.7324 - accuracy: 0.4957
12384/25000 [=============>................] - ETA: 30s - loss: 7.7310 - accuracy: 0.4958
12416/25000 [=============>................] - ETA: 30s - loss: 7.7321 - accuracy: 0.4957
12448/25000 [=============>................] - ETA: 30s - loss: 7.7270 - accuracy: 0.4961
12480/25000 [=============>................] - ETA: 30s - loss: 7.7256 - accuracy: 0.4962
12512/25000 [==============>...............] - ETA: 30s - loss: 7.7303 - accuracy: 0.4958
12544/25000 [==============>...............] - ETA: 30s - loss: 7.7326 - accuracy: 0.4957
12576/25000 [==============>...............] - ETA: 30s - loss: 7.7337 - accuracy: 0.4956
12608/25000 [==============>...............] - ETA: 30s - loss: 7.7335 - accuracy: 0.4956
12640/25000 [==============>...............] - ETA: 30s - loss: 7.7333 - accuracy: 0.4956
12672/25000 [==============>...............] - ETA: 30s - loss: 7.7392 - accuracy: 0.4953
12704/25000 [==============>...............] - ETA: 30s - loss: 7.7330 - accuracy: 0.4957
12736/25000 [==============>...............] - ETA: 30s - loss: 7.7304 - accuracy: 0.4958
12768/25000 [==============>...............] - ETA: 30s - loss: 7.7303 - accuracy: 0.4958
12800/25000 [==============>...............] - ETA: 29s - loss: 7.7289 - accuracy: 0.4959
12832/25000 [==============>...............] - ETA: 29s - loss: 7.7300 - accuracy: 0.4959
12864/25000 [==============>...............] - ETA: 29s - loss: 7.7262 - accuracy: 0.4961
12896/25000 [==============>...............] - ETA: 29s - loss: 7.7249 - accuracy: 0.4962
12928/25000 [==============>...............] - ETA: 29s - loss: 7.7295 - accuracy: 0.4959
12960/25000 [==============>...............] - ETA: 29s - loss: 7.7293 - accuracy: 0.4959
12992/25000 [==============>...............] - ETA: 29s - loss: 7.7292 - accuracy: 0.4959
13024/25000 [==============>...............] - ETA: 29s - loss: 7.7302 - accuracy: 0.4959
13056/25000 [==============>...............] - ETA: 29s - loss: 7.7324 - accuracy: 0.4957
13088/25000 [==============>...............] - ETA: 29s - loss: 7.7287 - accuracy: 0.4960
13120/25000 [==============>...............] - ETA: 29s - loss: 7.7251 - accuracy: 0.4962
13152/25000 [==============>...............] - ETA: 29s - loss: 7.7202 - accuracy: 0.4965
13184/25000 [==============>...............] - ETA: 28s - loss: 7.7213 - accuracy: 0.4964
13216/25000 [==============>...............] - ETA: 28s - loss: 7.7211 - accuracy: 0.4964
13248/25000 [==============>...............] - ETA: 28s - loss: 7.7164 - accuracy: 0.4968
13280/25000 [==============>...............] - ETA: 28s - loss: 7.7140 - accuracy: 0.4969
13312/25000 [==============>...............] - ETA: 28s - loss: 7.7138 - accuracy: 0.4969
13344/25000 [===============>..............] - ETA: 28s - loss: 7.7149 - accuracy: 0.4969
13376/25000 [===============>..............] - ETA: 28s - loss: 7.7102 - accuracy: 0.4972
13408/25000 [===============>..............] - ETA: 28s - loss: 7.7089 - accuracy: 0.4972
13440/25000 [===============>..............] - ETA: 28s - loss: 7.7100 - accuracy: 0.4972
13472/25000 [===============>..............] - ETA: 28s - loss: 7.7121 - accuracy: 0.4970
13504/25000 [===============>..............] - ETA: 28s - loss: 7.7109 - accuracy: 0.4971
13536/25000 [===============>..............] - ETA: 28s - loss: 7.7165 - accuracy: 0.4967
13568/25000 [===============>..............] - ETA: 28s - loss: 7.7186 - accuracy: 0.4966
13600/25000 [===============>..............] - ETA: 28s - loss: 7.7174 - accuracy: 0.4967
13632/25000 [===============>..............] - ETA: 27s - loss: 7.7161 - accuracy: 0.4968
13664/25000 [===============>..............] - ETA: 27s - loss: 7.7182 - accuracy: 0.4966
13696/25000 [===============>..............] - ETA: 27s - loss: 7.7192 - accuracy: 0.4966
13728/25000 [===============>..............] - ETA: 27s - loss: 7.7202 - accuracy: 0.4965
13760/25000 [===============>..............] - ETA: 27s - loss: 7.7223 - accuracy: 0.4964
13792/25000 [===============>..............] - ETA: 27s - loss: 7.7222 - accuracy: 0.4964
13824/25000 [===============>..............] - ETA: 27s - loss: 7.7254 - accuracy: 0.4962
13856/25000 [===============>..............] - ETA: 27s - loss: 7.7264 - accuracy: 0.4961
13888/25000 [===============>..............] - ETA: 27s - loss: 7.7307 - accuracy: 0.4958
13920/25000 [===============>..............] - ETA: 27s - loss: 7.7283 - accuracy: 0.4960
13952/25000 [===============>..............] - ETA: 27s - loss: 7.7282 - accuracy: 0.4960
13984/25000 [===============>..............] - ETA: 27s - loss: 7.7258 - accuracy: 0.4961
14016/25000 [===============>..............] - ETA: 27s - loss: 7.7235 - accuracy: 0.4963
14048/25000 [===============>..............] - ETA: 27s - loss: 7.7223 - accuracy: 0.4964
14080/25000 [===============>..............] - ETA: 26s - loss: 7.7200 - accuracy: 0.4965
14112/25000 [===============>..............] - ETA: 26s - loss: 7.7188 - accuracy: 0.4966
14144/25000 [===============>..............] - ETA: 26s - loss: 7.7187 - accuracy: 0.4966
14176/25000 [================>.............] - ETA: 26s - loss: 7.7164 - accuracy: 0.4968
14208/25000 [================>.............] - ETA: 26s - loss: 7.7173 - accuracy: 0.4967
14240/25000 [================>.............] - ETA: 26s - loss: 7.7172 - accuracy: 0.4967
14272/25000 [================>.............] - ETA: 26s - loss: 7.7182 - accuracy: 0.4966
14304/25000 [================>.............] - ETA: 26s - loss: 7.7202 - accuracy: 0.4965
14336/25000 [================>.............] - ETA: 26s - loss: 7.7222 - accuracy: 0.4964
14368/25000 [================>.............] - ETA: 26s - loss: 7.7189 - accuracy: 0.4966
14400/25000 [================>.............] - ETA: 26s - loss: 7.7124 - accuracy: 0.4970
14432/25000 [================>.............] - ETA: 26s - loss: 7.7102 - accuracy: 0.4972
14464/25000 [================>.............] - ETA: 26s - loss: 7.7111 - accuracy: 0.4971
14496/25000 [================>.............] - ETA: 25s - loss: 7.7121 - accuracy: 0.4970
14528/25000 [================>.............] - ETA: 25s - loss: 7.7046 - accuracy: 0.4975
14560/25000 [================>.............] - ETA: 25s - loss: 7.7014 - accuracy: 0.4977
14592/25000 [================>.............] - ETA: 25s - loss: 7.7044 - accuracy: 0.4975
14624/25000 [================>.............] - ETA: 25s - loss: 7.7033 - accuracy: 0.4976
14656/25000 [================>.............] - ETA: 25s - loss: 7.7043 - accuracy: 0.4975
14688/25000 [================>.............] - ETA: 25s - loss: 7.7021 - accuracy: 0.4977
14720/25000 [================>.............] - ETA: 25s - loss: 7.7031 - accuracy: 0.4976
14752/25000 [================>.............] - ETA: 25s - loss: 7.7020 - accuracy: 0.4977
14784/25000 [================>.............] - ETA: 25s - loss: 7.6957 - accuracy: 0.4981
14816/25000 [================>.............] - ETA: 25s - loss: 7.6977 - accuracy: 0.4980
14848/25000 [================>.............] - ETA: 25s - loss: 7.7007 - accuracy: 0.4978
14880/25000 [================>.............] - ETA: 25s - loss: 7.7058 - accuracy: 0.4974
14912/25000 [================>.............] - ETA: 24s - loss: 7.7067 - accuracy: 0.4974
14944/25000 [================>.............] - ETA: 24s - loss: 7.7046 - accuracy: 0.4975
14976/25000 [================>.............] - ETA: 24s - loss: 7.7065 - accuracy: 0.4974
15008/25000 [=================>............] - ETA: 24s - loss: 7.7054 - accuracy: 0.4975
15040/25000 [=================>............] - ETA: 24s - loss: 7.7033 - accuracy: 0.4976
15072/25000 [=================>............] - ETA: 24s - loss: 7.7043 - accuracy: 0.4975
15104/25000 [=================>............] - ETA: 24s - loss: 7.7072 - accuracy: 0.4974
15136/25000 [=================>............] - ETA: 24s - loss: 7.7051 - accuracy: 0.4975
15168/25000 [=================>............] - ETA: 24s - loss: 7.7091 - accuracy: 0.4972
15200/25000 [=================>............] - ETA: 24s - loss: 7.7110 - accuracy: 0.4971
15232/25000 [=================>............] - ETA: 24s - loss: 7.7099 - accuracy: 0.4972
15264/25000 [=================>............] - ETA: 24s - loss: 7.7098 - accuracy: 0.4972
15296/25000 [=================>............] - ETA: 24s - loss: 7.7107 - accuracy: 0.4971
15328/25000 [=================>............] - ETA: 24s - loss: 7.7076 - accuracy: 0.4973
15360/25000 [=================>............] - ETA: 23s - loss: 7.7046 - accuracy: 0.4975
15392/25000 [=================>............] - ETA: 23s - loss: 7.7065 - accuracy: 0.4974
15424/25000 [=================>............] - ETA: 23s - loss: 7.7074 - accuracy: 0.4973
15456/25000 [=================>............] - ETA: 23s - loss: 7.7103 - accuracy: 0.4972
15488/25000 [=================>............] - ETA: 23s - loss: 7.7052 - accuracy: 0.4975
15520/25000 [=================>............] - ETA: 23s - loss: 7.7022 - accuracy: 0.4977
15552/25000 [=================>............] - ETA: 23s - loss: 7.7061 - accuracy: 0.4974
15584/25000 [=================>............] - ETA: 23s - loss: 7.7030 - accuracy: 0.4976
15616/25000 [=================>............] - ETA: 23s - loss: 7.7039 - accuracy: 0.4976
15648/25000 [=================>............] - ETA: 23s - loss: 7.7058 - accuracy: 0.4974
15680/25000 [=================>............] - ETA: 23s - loss: 7.7067 - accuracy: 0.4974
15712/25000 [=================>............] - ETA: 23s - loss: 7.7066 - accuracy: 0.4974
15744/25000 [=================>............] - ETA: 22s - loss: 7.7007 - accuracy: 0.4978
15776/25000 [=================>............] - ETA: 22s - loss: 7.6997 - accuracy: 0.4978
15808/25000 [=================>............] - ETA: 22s - loss: 7.6986 - accuracy: 0.4979
15840/25000 [==================>...........] - ETA: 22s - loss: 7.6976 - accuracy: 0.4980
15872/25000 [==================>...........] - ETA: 22s - loss: 7.6966 - accuracy: 0.4980
15904/25000 [==================>...........] - ETA: 22s - loss: 7.6907 - accuracy: 0.4984
15936/25000 [==================>...........] - ETA: 22s - loss: 7.6887 - accuracy: 0.4986
15968/25000 [==================>...........] - ETA: 22s - loss: 7.6877 - accuracy: 0.4986
16000/25000 [==================>...........] - ETA: 22s - loss: 7.6915 - accuracy: 0.4984
16032/25000 [==================>...........] - ETA: 22s - loss: 7.6896 - accuracy: 0.4985
16064/25000 [==================>...........] - ETA: 22s - loss: 7.6905 - accuracy: 0.4984
16096/25000 [==================>...........] - ETA: 22s - loss: 7.6914 - accuracy: 0.4984
16128/25000 [==================>...........] - ETA: 22s - loss: 7.6913 - accuracy: 0.4984
16160/25000 [==================>...........] - ETA: 21s - loss: 7.6913 - accuracy: 0.4984
16192/25000 [==================>...........] - ETA: 21s - loss: 7.6922 - accuracy: 0.4983
16224/25000 [==================>...........] - ETA: 21s - loss: 7.6931 - accuracy: 0.4983
16256/25000 [==================>...........] - ETA: 21s - loss: 7.6949 - accuracy: 0.4982
16288/25000 [==================>...........] - ETA: 21s - loss: 7.6911 - accuracy: 0.4984
16320/25000 [==================>...........] - ETA: 21s - loss: 7.6929 - accuracy: 0.4983
16352/25000 [==================>...........] - ETA: 21s - loss: 7.6966 - accuracy: 0.4980
16384/25000 [==================>...........] - ETA: 21s - loss: 7.6966 - accuracy: 0.4980
16416/25000 [==================>...........] - ETA: 21s - loss: 7.6993 - accuracy: 0.4979
16448/25000 [==================>...........] - ETA: 21s - loss: 7.7011 - accuracy: 0.4978
16480/25000 [==================>...........] - ETA: 21s - loss: 7.6992 - accuracy: 0.4979
16512/25000 [==================>...........] - ETA: 21s - loss: 7.7038 - accuracy: 0.4976
16544/25000 [==================>...........] - ETA: 21s - loss: 7.7065 - accuracy: 0.4974
16576/25000 [==================>...........] - ETA: 20s - loss: 7.7082 - accuracy: 0.4973
16608/25000 [==================>...........] - ETA: 20s - loss: 7.7054 - accuracy: 0.4975
16640/25000 [==================>...........] - ETA: 20s - loss: 7.7053 - accuracy: 0.4975
16672/25000 [===================>..........] - ETA: 20s - loss: 7.7034 - accuracy: 0.4976
16704/25000 [===================>..........] - ETA: 20s - loss: 7.7033 - accuracy: 0.4976
16736/25000 [===================>..........] - ETA: 20s - loss: 7.7033 - accuracy: 0.4976
16768/25000 [===================>..........] - ETA: 20s - loss: 7.7032 - accuracy: 0.4976
16800/25000 [===================>..........] - ETA: 20s - loss: 7.7013 - accuracy: 0.4977
16832/25000 [===================>..........] - ETA: 20s - loss: 7.7031 - accuracy: 0.4976
16864/25000 [===================>..........] - ETA: 20s - loss: 7.7021 - accuracy: 0.4977
16896/25000 [===================>..........] - ETA: 20s - loss: 7.7020 - accuracy: 0.4977
16928/25000 [===================>..........] - ETA: 20s - loss: 7.7001 - accuracy: 0.4978
16960/25000 [===================>..........] - ETA: 20s - loss: 7.7001 - accuracy: 0.4978
16992/25000 [===================>..........] - ETA: 19s - loss: 7.7027 - accuracy: 0.4976
17024/25000 [===================>..........] - ETA: 19s - loss: 7.7053 - accuracy: 0.4975
17056/25000 [===================>..........] - ETA: 19s - loss: 7.7062 - accuracy: 0.4974
17088/25000 [===================>..........] - ETA: 19s - loss: 7.7025 - accuracy: 0.4977
17120/25000 [===================>..........] - ETA: 19s - loss: 7.7069 - accuracy: 0.4974
17152/25000 [===================>..........] - ETA: 19s - loss: 7.7077 - accuracy: 0.4973
17184/25000 [===================>..........] - ETA: 19s - loss: 7.7068 - accuracy: 0.4974
17216/25000 [===================>..........] - ETA: 19s - loss: 7.7120 - accuracy: 0.4970
17248/25000 [===================>..........] - ETA: 19s - loss: 7.7084 - accuracy: 0.4973
17280/25000 [===================>..........] - ETA: 19s - loss: 7.7048 - accuracy: 0.4975
17312/25000 [===================>..........] - ETA: 19s - loss: 7.7065 - accuracy: 0.4974
17344/25000 [===================>..........] - ETA: 19s - loss: 7.7064 - accuracy: 0.4974
17376/25000 [===================>..........] - ETA: 18s - loss: 7.7090 - accuracy: 0.4972
17408/25000 [===================>..........] - ETA: 18s - loss: 7.7063 - accuracy: 0.4974
17440/25000 [===================>..........] - ETA: 18s - loss: 7.7106 - accuracy: 0.4971
17472/25000 [===================>..........] - ETA: 18s - loss: 7.7123 - accuracy: 0.4970
17504/25000 [====================>.........] - ETA: 18s - loss: 7.7122 - accuracy: 0.4970
17536/25000 [====================>.........] - ETA: 18s - loss: 7.7165 - accuracy: 0.4967
17568/25000 [====================>.........] - ETA: 18s - loss: 7.7216 - accuracy: 0.4964
17600/25000 [====================>.........] - ETA: 18s - loss: 7.7250 - accuracy: 0.4962
17632/25000 [====================>.........] - ETA: 18s - loss: 7.7266 - accuracy: 0.4961
17664/25000 [====================>.........] - ETA: 18s - loss: 7.7256 - accuracy: 0.4962
17696/25000 [====================>.........] - ETA: 18s - loss: 7.7247 - accuracy: 0.4962
17728/25000 [====================>.........] - ETA: 18s - loss: 7.7289 - accuracy: 0.4959
17760/25000 [====================>.........] - ETA: 18s - loss: 7.7279 - accuracy: 0.4960
17792/25000 [====================>.........] - ETA: 17s - loss: 7.7244 - accuracy: 0.4962
17824/25000 [====================>.........] - ETA: 17s - loss: 7.7243 - accuracy: 0.4962
17856/25000 [====================>.........] - ETA: 17s - loss: 7.7233 - accuracy: 0.4963
17888/25000 [====================>.........] - ETA: 17s - loss: 7.7232 - accuracy: 0.4963
17920/25000 [====================>.........] - ETA: 17s - loss: 7.7214 - accuracy: 0.4964
17952/25000 [====================>.........] - ETA: 17s - loss: 7.7213 - accuracy: 0.4964
17984/25000 [====================>.........] - ETA: 17s - loss: 7.7246 - accuracy: 0.4962
18016/25000 [====================>.........] - ETA: 17s - loss: 7.7228 - accuracy: 0.4963
18048/25000 [====================>.........] - ETA: 17s - loss: 7.7218 - accuracy: 0.4964
18080/25000 [====================>.........] - ETA: 17s - loss: 7.7243 - accuracy: 0.4962
18112/25000 [====================>.........] - ETA: 17s - loss: 7.7250 - accuracy: 0.4962
18144/25000 [====================>.........] - ETA: 17s - loss: 7.7241 - accuracy: 0.4963
18176/25000 [====================>.........] - ETA: 17s - loss: 7.7274 - accuracy: 0.4960
18208/25000 [====================>.........] - ETA: 16s - loss: 7.7306 - accuracy: 0.4958
18240/25000 [====================>.........] - ETA: 16s - loss: 7.7297 - accuracy: 0.4959
18272/25000 [====================>.........] - ETA: 16s - loss: 7.7312 - accuracy: 0.4958
18304/25000 [====================>.........] - ETA: 16s - loss: 7.7286 - accuracy: 0.4960
18336/25000 [=====================>........] - ETA: 16s - loss: 7.7302 - accuracy: 0.4959
18368/25000 [=====================>........] - ETA: 16s - loss: 7.7292 - accuracy: 0.4959
18400/25000 [=====================>........] - ETA: 16s - loss: 7.7316 - accuracy: 0.4958
18432/25000 [=====================>........] - ETA: 16s - loss: 7.7273 - accuracy: 0.4960
18464/25000 [=====================>........] - ETA: 16s - loss: 7.7248 - accuracy: 0.4962
18496/25000 [=====================>........] - ETA: 16s - loss: 7.7213 - accuracy: 0.4964
18528/25000 [=====================>........] - ETA: 16s - loss: 7.7254 - accuracy: 0.4962
18560/25000 [=====================>........] - ETA: 16s - loss: 7.7319 - accuracy: 0.4957
18592/25000 [=====================>........] - ETA: 15s - loss: 7.7318 - accuracy: 0.4958
18624/25000 [=====================>........] - ETA: 15s - loss: 7.7275 - accuracy: 0.4960
18656/25000 [=====================>........] - ETA: 15s - loss: 7.7283 - accuracy: 0.4960
18688/25000 [=====================>........] - ETA: 15s - loss: 7.7282 - accuracy: 0.4960
18720/25000 [=====================>........] - ETA: 15s - loss: 7.7272 - accuracy: 0.4960
18752/25000 [=====================>........] - ETA: 15s - loss: 7.7255 - accuracy: 0.4962
18784/25000 [=====================>........] - ETA: 15s - loss: 7.7229 - accuracy: 0.4963
18816/25000 [=====================>........] - ETA: 15s - loss: 7.7220 - accuracy: 0.4964
18848/25000 [=====================>........] - ETA: 15s - loss: 7.7219 - accuracy: 0.4964
18880/25000 [=====================>........] - ETA: 15s - loss: 7.7235 - accuracy: 0.4963
18912/25000 [=====================>........] - ETA: 15s - loss: 7.7234 - accuracy: 0.4963
18944/25000 [=====================>........] - ETA: 15s - loss: 7.7241 - accuracy: 0.4963
18976/25000 [=====================>........] - ETA: 15s - loss: 7.7256 - accuracy: 0.4962
19008/25000 [=====================>........] - ETA: 14s - loss: 7.7239 - accuracy: 0.4963
19040/25000 [=====================>........] - ETA: 14s - loss: 7.7214 - accuracy: 0.4964
19072/25000 [=====================>........] - ETA: 14s - loss: 7.7269 - accuracy: 0.4961
19104/25000 [=====================>........] - ETA: 14s - loss: 7.7292 - accuracy: 0.4959
19136/25000 [=====================>........] - ETA: 14s - loss: 7.7299 - accuracy: 0.4959
19168/25000 [======================>.......] - ETA: 14s - loss: 7.7282 - accuracy: 0.4960
19200/25000 [======================>.......] - ETA: 14s - loss: 7.7313 - accuracy: 0.4958
19232/25000 [======================>.......] - ETA: 14s - loss: 7.7296 - accuracy: 0.4959
19264/25000 [======================>.......] - ETA: 14s - loss: 7.7295 - accuracy: 0.4959
19296/25000 [======================>.......] - ETA: 14s - loss: 7.7302 - accuracy: 0.4959
19328/25000 [======================>.......] - ETA: 14s - loss: 7.7293 - accuracy: 0.4959
19360/25000 [======================>.......] - ETA: 14s - loss: 7.7308 - accuracy: 0.4958
19392/25000 [======================>.......] - ETA: 13s - loss: 7.7291 - accuracy: 0.4959
19424/25000 [======================>.......] - ETA: 13s - loss: 7.7242 - accuracy: 0.4962
19456/25000 [======================>.......] - ETA: 13s - loss: 7.7242 - accuracy: 0.4962
19488/25000 [======================>.......] - ETA: 13s - loss: 7.7193 - accuracy: 0.4966
19520/25000 [======================>.......] - ETA: 13s - loss: 7.7200 - accuracy: 0.4965
19552/25000 [======================>.......] - ETA: 13s - loss: 7.7207 - accuracy: 0.4965
19584/25000 [======================>.......] - ETA: 13s - loss: 7.7222 - accuracy: 0.4964
19616/25000 [======================>.......] - ETA: 13s - loss: 7.7237 - accuracy: 0.4963
19648/25000 [======================>.......] - ETA: 13s - loss: 7.7236 - accuracy: 0.4963
19680/25000 [======================>.......] - ETA: 13s - loss: 7.7212 - accuracy: 0.4964
19712/25000 [======================>.......] - ETA: 13s - loss: 7.7203 - accuracy: 0.4965
19744/25000 [======================>.......] - ETA: 13s - loss: 7.7171 - accuracy: 0.4967
19776/25000 [======================>.......] - ETA: 13s - loss: 7.7155 - accuracy: 0.4968
19808/25000 [======================>.......] - ETA: 12s - loss: 7.7138 - accuracy: 0.4969
19840/25000 [======================>.......] - ETA: 12s - loss: 7.7138 - accuracy: 0.4969
19872/25000 [======================>.......] - ETA: 12s - loss: 7.7152 - accuracy: 0.4968
19904/25000 [======================>.......] - ETA: 12s - loss: 7.7167 - accuracy: 0.4967
19936/25000 [======================>.......] - ETA: 12s - loss: 7.7158 - accuracy: 0.4968
19968/25000 [======================>.......] - ETA: 12s - loss: 7.7188 - accuracy: 0.4966
20000/25000 [=======================>......] - ETA: 12s - loss: 7.7172 - accuracy: 0.4967
20032/25000 [=======================>......] - ETA: 12s - loss: 7.7187 - accuracy: 0.4966
20064/25000 [=======================>......] - ETA: 12s - loss: 7.7209 - accuracy: 0.4965
20096/25000 [=======================>......] - ETA: 12s - loss: 7.7216 - accuracy: 0.4964
20128/25000 [=======================>......] - ETA: 12s - loss: 7.7245 - accuracy: 0.4962
20160/25000 [=======================>......] - ETA: 12s - loss: 7.7252 - accuracy: 0.4962
20192/25000 [=======================>......] - ETA: 11s - loss: 7.7205 - accuracy: 0.4965
20224/25000 [=======================>......] - ETA: 11s - loss: 7.7220 - accuracy: 0.4964
20256/25000 [=======================>......] - ETA: 11s - loss: 7.7166 - accuracy: 0.4967
20288/25000 [=======================>......] - ETA: 11s - loss: 7.7180 - accuracy: 0.4966
20320/25000 [=======================>......] - ETA: 11s - loss: 7.7157 - accuracy: 0.4968
20352/25000 [=======================>......] - ETA: 11s - loss: 7.7126 - accuracy: 0.4970
20384/25000 [=======================>......] - ETA: 11s - loss: 7.7125 - accuracy: 0.4970
20416/25000 [=======================>......] - ETA: 11s - loss: 7.7132 - accuracy: 0.4970
20448/25000 [=======================>......] - ETA: 11s - loss: 7.7124 - accuracy: 0.4970
20480/25000 [=======================>......] - ETA: 11s - loss: 7.7138 - accuracy: 0.4969
20512/25000 [=======================>......] - ETA: 11s - loss: 7.7130 - accuracy: 0.4970
20544/25000 [=======================>......] - ETA: 11s - loss: 7.7114 - accuracy: 0.4971
20576/25000 [=======================>......] - ETA: 11s - loss: 7.7136 - accuracy: 0.4969
20608/25000 [=======================>......] - ETA: 10s - loss: 7.7128 - accuracy: 0.4970
20640/25000 [=======================>......] - ETA: 10s - loss: 7.7097 - accuracy: 0.4972
20672/25000 [=======================>......] - ETA: 10s - loss: 7.7119 - accuracy: 0.4970
20704/25000 [=======================>......] - ETA: 10s - loss: 7.7096 - accuracy: 0.4972
20736/25000 [=======================>......] - ETA: 10s - loss: 7.7095 - accuracy: 0.4972
20768/25000 [=======================>......] - ETA: 10s - loss: 7.7072 - accuracy: 0.4974
20800/25000 [=======================>......] - ETA: 10s - loss: 7.7050 - accuracy: 0.4975
20832/25000 [=======================>......] - ETA: 10s - loss: 7.7071 - accuracy: 0.4974
20864/25000 [========================>.....] - ETA: 10s - loss: 7.7085 - accuracy: 0.4973
20896/25000 [========================>.....] - ETA: 10s - loss: 7.7040 - accuracy: 0.4976
20928/25000 [========================>.....] - ETA: 10s - loss: 7.7047 - accuracy: 0.4975
20960/25000 [========================>.....] - ETA: 10s - loss: 7.7025 - accuracy: 0.4977
20992/25000 [========================>.....] - ETA: 9s - loss: 7.7046 - accuracy: 0.4975 
21024/25000 [========================>.....] - ETA: 9s - loss: 7.7060 - accuracy: 0.4974
21056/25000 [========================>.....] - ETA: 9s - loss: 7.7052 - accuracy: 0.4975
21088/25000 [========================>.....] - ETA: 9s - loss: 7.7059 - accuracy: 0.4974
21120/25000 [========================>.....] - ETA: 9s - loss: 7.7065 - accuracy: 0.4974
21152/25000 [========================>.....] - ETA: 9s - loss: 7.7043 - accuracy: 0.4975
21184/25000 [========================>.....] - ETA: 9s - loss: 7.7079 - accuracy: 0.4973
21216/25000 [========================>.....] - ETA: 9s - loss: 7.7071 - accuracy: 0.4974
21248/25000 [========================>.....] - ETA: 9s - loss: 7.7092 - accuracy: 0.4972
21280/25000 [========================>.....] - ETA: 9s - loss: 7.7070 - accuracy: 0.4974
21312/25000 [========================>.....] - ETA: 9s - loss: 7.7076 - accuracy: 0.4973
21344/25000 [========================>.....] - ETA: 9s - loss: 7.7061 - accuracy: 0.4974
21376/25000 [========================>.....] - ETA: 9s - loss: 7.7068 - accuracy: 0.4974
21408/25000 [========================>.....] - ETA: 8s - loss: 7.7046 - accuracy: 0.4975
21440/25000 [========================>.....] - ETA: 8s - loss: 7.7024 - accuracy: 0.4977
21472/25000 [========================>.....] - ETA: 8s - loss: 7.7023 - accuracy: 0.4977
21504/25000 [========================>.....] - ETA: 8s - loss: 7.6994 - accuracy: 0.4979
21536/25000 [========================>.....] - ETA: 8s - loss: 7.6965 - accuracy: 0.4980
21568/25000 [========================>.....] - ETA: 8s - loss: 7.6986 - accuracy: 0.4979
21600/25000 [========================>.....] - ETA: 8s - loss: 7.6957 - accuracy: 0.4981
21632/25000 [========================>.....] - ETA: 8s - loss: 7.6978 - accuracy: 0.4980
21664/25000 [========================>.....] - ETA: 8s - loss: 7.6985 - accuracy: 0.4979
21696/25000 [=========================>....] - ETA: 8s - loss: 7.6998 - accuracy: 0.4978
21728/25000 [=========================>....] - ETA: 8s - loss: 7.6991 - accuracy: 0.4979
21760/25000 [=========================>....] - ETA: 8s - loss: 7.6948 - accuracy: 0.4982
21792/25000 [=========================>....] - ETA: 7s - loss: 7.6955 - accuracy: 0.4981
21824/25000 [=========================>....] - ETA: 7s - loss: 7.6940 - accuracy: 0.4982
21856/25000 [=========================>....] - ETA: 7s - loss: 7.6954 - accuracy: 0.4981
21888/25000 [=========================>....] - ETA: 7s - loss: 7.6953 - accuracy: 0.4981
21920/25000 [=========================>....] - ETA: 7s - loss: 7.6904 - accuracy: 0.4984
21952/25000 [=========================>....] - ETA: 7s - loss: 7.6876 - accuracy: 0.4986
21984/25000 [=========================>....] - ETA: 7s - loss: 7.6868 - accuracy: 0.4987
22016/25000 [=========================>....] - ETA: 7s - loss: 7.6840 - accuracy: 0.4989
22048/25000 [=========================>....] - ETA: 7s - loss: 7.6840 - accuracy: 0.4989
22080/25000 [=========================>....] - ETA: 7s - loss: 7.6812 - accuracy: 0.4990
22112/25000 [=========================>....] - ETA: 7s - loss: 7.6826 - accuracy: 0.4990
22144/25000 [=========================>....] - ETA: 7s - loss: 7.6791 - accuracy: 0.4992
22176/25000 [=========================>....] - ETA: 7s - loss: 7.6825 - accuracy: 0.4990
22208/25000 [=========================>....] - ETA: 6s - loss: 7.6839 - accuracy: 0.4989
22240/25000 [=========================>....] - ETA: 6s - loss: 7.6839 - accuracy: 0.4989
22272/25000 [=========================>....] - ETA: 6s - loss: 7.6831 - accuracy: 0.4989
22304/25000 [=========================>....] - ETA: 6s - loss: 7.6859 - accuracy: 0.4987
22336/25000 [=========================>....] - ETA: 6s - loss: 7.6852 - accuracy: 0.4988
22368/25000 [=========================>....] - ETA: 6s - loss: 7.6810 - accuracy: 0.4991
22400/25000 [=========================>....] - ETA: 6s - loss: 7.6817 - accuracy: 0.4990
22432/25000 [=========================>....] - ETA: 6s - loss: 7.6817 - accuracy: 0.4990
22464/25000 [=========================>....] - ETA: 6s - loss: 7.6803 - accuracy: 0.4991
22496/25000 [=========================>....] - ETA: 6s - loss: 7.6803 - accuracy: 0.4991
22528/25000 [==========================>...] - ETA: 6s - loss: 7.6823 - accuracy: 0.4990
22560/25000 [==========================>...] - ETA: 6s - loss: 7.6816 - accuracy: 0.4990
22592/25000 [==========================>...] - ETA: 5s - loss: 7.6809 - accuracy: 0.4991
22624/25000 [==========================>...] - ETA: 5s - loss: 7.6781 - accuracy: 0.4992
22656/25000 [==========================>...] - ETA: 5s - loss: 7.6774 - accuracy: 0.4993
22688/25000 [==========================>...] - ETA: 5s - loss: 7.6815 - accuracy: 0.4990
22720/25000 [==========================>...] - ETA: 5s - loss: 7.6801 - accuracy: 0.4991
22752/25000 [==========================>...] - ETA: 5s - loss: 7.6814 - accuracy: 0.4990
22784/25000 [==========================>...] - ETA: 5s - loss: 7.6808 - accuracy: 0.4991
22816/25000 [==========================>...] - ETA: 5s - loss: 7.6807 - accuracy: 0.4991
22848/25000 [==========================>...] - ETA: 5s - loss: 7.6827 - accuracy: 0.4989
22880/25000 [==========================>...] - ETA: 5s - loss: 7.6861 - accuracy: 0.4987
22912/25000 [==========================>...] - ETA: 5s - loss: 7.6860 - accuracy: 0.4987
22944/25000 [==========================>...] - ETA: 5s - loss: 7.6840 - accuracy: 0.4989
22976/25000 [==========================>...] - ETA: 5s - loss: 7.6840 - accuracy: 0.4989
23008/25000 [==========================>...] - ETA: 4s - loss: 7.6826 - accuracy: 0.4990
23040/25000 [==========================>...] - ETA: 4s - loss: 7.6813 - accuracy: 0.4990
23072/25000 [==========================>...] - ETA: 4s - loss: 7.6826 - accuracy: 0.4990
23104/25000 [==========================>...] - ETA: 4s - loss: 7.6819 - accuracy: 0.4990
23136/25000 [==========================>...] - ETA: 4s - loss: 7.6812 - accuracy: 0.4990
23168/25000 [==========================>...] - ETA: 4s - loss: 7.6818 - accuracy: 0.4990
23200/25000 [==========================>...] - ETA: 4s - loss: 7.6798 - accuracy: 0.4991
23232/25000 [==========================>...] - ETA: 4s - loss: 7.6792 - accuracy: 0.4992
23264/25000 [==========================>...] - ETA: 4s - loss: 7.6772 - accuracy: 0.4993
23296/25000 [==========================>...] - ETA: 4s - loss: 7.6772 - accuracy: 0.4993
23328/25000 [==========================>...] - ETA: 4s - loss: 7.6785 - accuracy: 0.4992
23360/25000 [===========================>..] - ETA: 4s - loss: 7.6797 - accuracy: 0.4991
23392/25000 [===========================>..] - ETA: 3s - loss: 7.6804 - accuracy: 0.4991
23424/25000 [===========================>..] - ETA: 3s - loss: 7.6791 - accuracy: 0.4992
23456/25000 [===========================>..] - ETA: 3s - loss: 7.6823 - accuracy: 0.4990
23488/25000 [===========================>..] - ETA: 3s - loss: 7.6842 - accuracy: 0.4989
23520/25000 [===========================>..] - ETA: 3s - loss: 7.6842 - accuracy: 0.4989
23552/25000 [===========================>..] - ETA: 3s - loss: 7.6822 - accuracy: 0.4990
23584/25000 [===========================>..] - ETA: 3s - loss: 7.6829 - accuracy: 0.4989
23616/25000 [===========================>..] - ETA: 3s - loss: 7.6822 - accuracy: 0.4990
23648/25000 [===========================>..] - ETA: 3s - loss: 7.6809 - accuracy: 0.4991
23680/25000 [===========================>..] - ETA: 3s - loss: 7.6776 - accuracy: 0.4993
23712/25000 [===========================>..] - ETA: 3s - loss: 7.6783 - accuracy: 0.4992
23744/25000 [===========================>..] - ETA: 3s - loss: 7.6763 - accuracy: 0.4994
23776/25000 [===========================>..] - ETA: 3s - loss: 7.6789 - accuracy: 0.4992
23808/25000 [===========================>..] - ETA: 2s - loss: 7.6795 - accuracy: 0.4992
23840/25000 [===========================>..] - ETA: 2s - loss: 7.6788 - accuracy: 0.4992
23872/25000 [===========================>..] - ETA: 2s - loss: 7.6782 - accuracy: 0.4992
23904/25000 [===========================>..] - ETA: 2s - loss: 7.6756 - accuracy: 0.4994
23936/25000 [===========================>..] - ETA: 2s - loss: 7.6769 - accuracy: 0.4993
23968/25000 [===========================>..] - ETA: 2s - loss: 7.6775 - accuracy: 0.4993
24000/25000 [===========================>..] - ETA: 2s - loss: 7.6800 - accuracy: 0.4991
24032/25000 [===========================>..] - ETA: 2s - loss: 7.6819 - accuracy: 0.4990
24064/25000 [===========================>..] - ETA: 2s - loss: 7.6806 - accuracy: 0.4991
24096/25000 [===========================>..] - ETA: 2s - loss: 7.6793 - accuracy: 0.4992
24128/25000 [===========================>..] - ETA: 2s - loss: 7.6787 - accuracy: 0.4992
24160/25000 [===========================>..] - ETA: 2s - loss: 7.6787 - accuracy: 0.4992
24192/25000 [============================>.] - ETA: 1s - loss: 7.6774 - accuracy: 0.4993
24224/25000 [============================>.] - ETA: 1s - loss: 7.6761 - accuracy: 0.4994
24256/25000 [============================>.] - ETA: 1s - loss: 7.6767 - accuracy: 0.4993
24288/25000 [============================>.] - ETA: 1s - loss: 7.6742 - accuracy: 0.4995
24320/25000 [============================>.] - ETA: 1s - loss: 7.6717 - accuracy: 0.4997
24352/25000 [============================>.] - ETA: 1s - loss: 7.6717 - accuracy: 0.4997
24384/25000 [============================>.] - ETA: 1s - loss: 7.6698 - accuracy: 0.4998
24416/25000 [============================>.] - ETA: 1s - loss: 7.6704 - accuracy: 0.4998
24448/25000 [============================>.] - ETA: 1s - loss: 7.6710 - accuracy: 0.4997
24480/25000 [============================>.] - ETA: 1s - loss: 7.6716 - accuracy: 0.4997
24512/25000 [============================>.] - ETA: 1s - loss: 7.6697 - accuracy: 0.4998
24544/25000 [============================>.] - ETA: 1s - loss: 7.6679 - accuracy: 0.4999
24576/25000 [============================>.] - ETA: 1s - loss: 7.6679 - accuracy: 0.4999
24608/25000 [============================>.] - ETA: 0s - loss: 7.6672 - accuracy: 0.5000
24640/25000 [============================>.] - ETA: 0s - loss: 7.6697 - accuracy: 0.4998
24672/25000 [============================>.] - ETA: 0s - loss: 7.6691 - accuracy: 0.4998
24704/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24736/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
24768/25000 [============================>.] - ETA: 0s - loss: 7.6672 - accuracy: 0.5000
24800/25000 [============================>.] - ETA: 0s - loss: 7.6666 - accuracy: 0.5000
24832/25000 [============================>.] - ETA: 0s - loss: 7.6641 - accuracy: 0.5002
24864/25000 [============================>.] - ETA: 0s - loss: 7.6623 - accuracy: 0.5003
24896/25000 [============================>.] - ETA: 0s - loss: 7.6648 - accuracy: 0.5001
24928/25000 [============================>.] - ETA: 0s - loss: 7.6642 - accuracy: 0.5002
24960/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
24992/25000 [============================>.] - ETA: 0s - loss: 7.6660 - accuracy: 0.5000
25000/25000 [==============================] - 72s 3ms/step - loss: 7.6666 - accuracy: 0.5000 - val_loss: 7.6246 - val_accuracy: 0.5000
Loading data...
Using TensorFlow backend.





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn_titanic_randomForest_example2.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0mdata_path[0m [0;34m=[0m [0;34m'../mlmodels/dataset/json/hyper_titanic_randomForest.json'[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      7[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/hyper_titanic_randomForest.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//mnist_mlmodels_.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//mnist_mlmodels_.ipynb[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mfrom[0m [0mgoogle[0m[0;34m.[0m[0mcolab[0m [0;32mimport[0m [0mdrive[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m [0mdrive[0m[0;34m.[0m[0mmount[0m[0;34m([0m[0;34m'/content/drive'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mModuleNotFoundError[0m: No module named 'google.colab'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//gluon_automl_titanic.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//gluon_automl_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      8[0m     [0mchoice[0m[0;34m=[0m[0;34m'json'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m      9[0m     [0mconfig_mode[0m[0;34m=[0m [0;34m'test'[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 10[0;31m     [0mdata_path[0m[0;34m=[0m [0;34m'../mlmodels/dataset/json/gluon_automl.json'[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     11[0m )

[0;32m~/work/mlmodels/mlmodels/mlmodels/model_gluon/gluon_automl.py[0m in [0;36mget_params[0;34m(choice, data_path, config_mode, **kw)[0m
[1;32m     80[0m             __file__)).parent.parent / "model_gluon/gluon_automl.json" if data_path == "dataset/" else data_path
[1;32m     81[0m [0;34m[0m[0m
[0;32m---> 82[0;31m         [0;32mwith[0m [0mopen[0m[0;34m([0m[0mdata_path[0m[0;34m,[0m [0mencoding[0m[0;34m=[0m[0;34m'utf-8'[0m[0;34m)[0m [0;32mas[0m [0mconfig_f[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     83[0m             [0mconfig[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mconfig_f[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     84[0m             [0mconfig[0m [0;34m=[0m [0mconfig[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '../mlmodels/dataset/json/gluon_automl.json'
/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB
  Optimizer.opt_registry[name].__name__))





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//tensorflow__lstm_json.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//tensorflow__lstm_json.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      5[0m [0;32mimport[0m [0mjson[0m[0;34m[0m[0;34m[0m[0m
[1;32m      6[0m [0;34m[0m[0m
[0;32m----> 7[0;31m [0mprint[0m[0;34m([0m [0mos[0m[0;34m.[0m[0mgetcwd[0m[0;34m([0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;31mNameError[0m: name 'os' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//sklearn.ipynb 

[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     71[0m         [0mmodel_name[0m [0;34m=[0m [0mmodel_uri[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;34m".py"[0m[0;34m,[0m [0;34m""[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 72[0;31m         [0mmodule[0m [0;34m=[0m [0mimport_module[0m[0;34m([0m[0;34mf"mlmodels.{model_name}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     73[0m         [0;31m# module    = import_module("mlmodels.model_tf.1_lstm")[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/__init__.py[0m in [0;36mimport_module[0;34m(name, package)[0m
[1;32m    125[0m             [0mlevel[0m [0;34m+=[0m [0;36m1[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 126[0;31m     [0;32mreturn[0m [0m_bootstrap[0m[0;34m.[0m[0m_gcd_import[0m[0;34m([0m[0mname[0m[0;34m[[0m[0mlevel[0m[0;34m:[0m[0;34m][0m[0;34m,[0m [0mpackage[0m[0;34m,[0m [0mlevel[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    127[0m [0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_gcd_import[0;34m(name, package, level)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load[0;34m(name, import_)[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/importlib/_bootstrap.py[0m in [0;36m_find_and_load_unlocked[0;34m(name, import_)[0m

[0;31mModuleNotFoundError[0m: No module named 'mlmodels.model_sklearn.sklearn'

During handling of the above exception, another exception occurred:

[0;31mIndexError[0m                                Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     83[0m             [0mmodel_name[0m [0;34m=[0m [0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mstem[0m  [0;31m# remove .py[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 84[0;31m             [0mmodel_name[0m [0;34m=[0m [0mstr[0m[0;34m([0m[0mPath[0m[0;34m([0m[0mmodel_uri[0m[0;34m)[0m[0;34m.[0m[0mparts[0m[0;34m[[0m[0;34m-[0m[0;36m2[0m[0;34m][0m[0;34m)[0m [0;34m+[0m [0;34m"."[0m [0;34m+[0m [0mstr[0m[0;34m([0m[0mmodel_name[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     85[0m             [0;31m# print(model_name)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;31mIndexError[0m: tuple index out of range

During handling of the above exception, another exception occurred:

[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//sklearn.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mmodule[0m        [0;34m=[0m  [0mmodule_load[0m[0;34m([0m [0mmodel_uri[0m[0;34m=[0m [0mmodel_uri[0m [0;34m)[0m                           [0;31m# Load file definition[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0mmodel[0m         [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mModel[0m[0;34m([0m[0mmodel_pars[0m[0;34m=[0m[0mmodel_pars[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m)[0m             [0;31m# Create Model instance[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m [0mmodel[0m[0;34m,[0m [0msess[0m   [0;34m=[0m  [0mmodule[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mmodel[0m[0;34m,[0m [0mdata_pars[0m[0;34m=[0m[0mdata_pars[0m[0;34m,[0m [0mcompute_pars[0m[0;34m=[0m[0mcompute_pars[0m[0;34m,[0m [0mout_pars[0m[0;34m=[0m[0mout_pars[0m[0;34m)[0m          [0;31m# fit the model[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/work/mlmodels/mlmodels/mlmodels/models.py[0m in [0;36mmodule_load[0;34m(model_uri, verbose, env_build)[0m
[1;32m     87[0m [0;34m[0m[0m
[1;32m     88[0m         [0;32mexcept[0m [0mException[0m [0;32mas[0m [0me2[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 89[0;31m             [0;32mraise[0m [0mNameError[0m[0;34m([0m[0;34mf"Module {model_name} notfound, {e1}, {e2}"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     90[0m [0;34m[0m[0m
[1;32m     91[0m     [0;32mif[0m [0mverbose[0m[0;34m:[0m [0mprint[0m[0;34m([0m[0mmodule[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;31mNameError[0m: Module model_sklearn.sklearn notfound, No module named 'mlmodels.model_sklearn.sklearn', tuple index out of range





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_titanic.ipynb 

Deprecaton set to False
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example//lightgbm_titanic.ipynb[0m in [0;36m<module>[0;34m[0m
[1;32m      1[0m [0mdata_path[0m [0;34m=[0m [0;34m'hyper_lightgbm_titanic.json'[0m[0;34m[0m[0;34m[0m[0m
[1;32m      2[0m [0;34m[0m[0m
[0;32m----> 3[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mdata_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      4[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m      5[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpdict[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'hyper_lightgbm_titanic.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//vision_mnist.py 

[0;36m  File [0;32m"/home/runner/work/mlmodels/mlmodels/mlmodels/example/vision_mnist.py"[0;36m, line [0;32m15[0m
[0;31m    !git clone https://github.com/ahmed3bbas/mlmodels.git[0m
[0m    ^[0m
[0;31mSyntaxError[0m[0;31m:[0m invalid syntax






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_hyper.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_hyper.py[0m in [0;36m<module>[0;34m[0m
[1;32m      3[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mmodels[0m [0;32mimport[0m [0mmodule_load[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m [0;32mfrom[0m [0mmlmodels[0m[0;34m.[0m[0mutil[0m [0;32mimport[0m [0mpath_norm_dict[0m[0;34m,[0m [0mpath_norm[0m[0;34m,[0m [0mparams_json_load[0m[0;34m[0m[0;34m[0m[0m
[0;32m----> 5[0;31m [0mprint[0m[0;34m([0m[0mmlmodels[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      6[0m [0;34m[0m[0m
[1;32m      7[0m [0;34m[0m[0m

[0;31mNameError[0m: name 'mlmodels' is not defined





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//lightgbm_glass.py 

Deprecaton set to False
/home/runner/work/mlmodels/mlmodels
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/lightgbm_glass.py[0m in [0;36m<module>[0;34m[0m
[1;32m     20[0m [0;34m[0m[0m
[1;32m     21[0m [0;34m[0m[0m
[0;32m---> 22[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m [0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     23[0m [0mprint[0m[0;34m([0m[0mpars[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     24[0m [0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: 'lightgbm_glass.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example//arun_model.py 

<module 'mlmodels' from '/home/runner/work/mlmodels/mlmodels/mlmodels/__init__.py'>
/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json
[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/arun_model.py[0m in [0;36m<module>[0;34m[0m
[1;32m     25[0m [0;31m# Model Parameters[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m     26[0m [0;31m# model_pars, data_pars, compute_pars, out_pars[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 27[0;31m [0mpars[0m [0;34m=[0m [0mjson[0m[0;34m.[0m[0mload[0m[0;34m([0m[0mopen[0m[0;34m([0m[0mconfig_path[0m [0;34m,[0m [0mmode[0m[0;34m=[0m[0;34m'r'[0m[0;34m)[0m[0;34m)[0m[0;34m[[0m[0mconfig_mode[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     28[0m [0;32mfor[0m [0mkey[0m[0;34m,[0m [0mpdict[0m [0;32min[0m  [0mpars[0m[0;34m.[0m[0mitems[0m[0;34m([0m[0;34m)[0m [0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     29[0m   [0mglobals[0m[0;34m([0m[0;34m)[0m[0;34m[[0m[0mkey[0m[0;34m][0m [0;34m=[0m [0mpath_norm_dict[0m[0;34m([0m [0mpdict[0m   [0;34m)[0m   [0;31m###Normalize path[0m[0;34m[0m[0;34m[0m[0m

[0;31mFileNotFoundError[0m: [Errno 2] No such file or directory: '/home/runner/work/mlmodels/mlmodels/mlmodels/model_keras/ardmn.json'





 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m4.py 






 ************************************************************************************************************************
ipython https://github.com/arita37/mlmodels/blob/dev/mlmodels/example/benchmark_timeseries_m5.py 

[0;31m---------------------------------------------------------------------------[0m
[0;31mFileNotFoundError[0m                         Traceback (most recent call last)
[0;32m~/work/mlmodels/mlmodels/mlmodels/example/benchmark_timeseries_m5.py[0m in [0;36m<module>[0;34m[0m
[1;32m     84[0m [0;34m[0m[0m
[1;32m     85[0m """
[0;32m---> 86[0;31m [0mcalendar[0m               [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/calendar.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m     87[0m [0msales_train_val[0m        [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sales_train_val.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m     88[0m [0msample_submission[0m      [0;34m=[0m [0mpd[0m[0;34m.[0m[0mread_csv[0m[0;34m([0m[0;34mf'{m5_input_path}/sample_submission.csv'[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36mparser_f[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)[0m
[1;32m    683[0m         )
[1;32m    684[0m [0;34m[0m[0m
[0;32m--> 685[0;31m         [0;32mreturn[0m [0m_read[0m[0;34m([0m[0mfilepath_or_buffer[0m[0;34m,[0m [0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    686[0m [0;34m[0m[0m
[1;32m    687[0m     [0mparser_f[0m[0;34m.[0m[0m__name__[0m [0;34m=[0m [0mname[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_read[0;34m(filepath_or_buffer, kwds)[0m
[1;32m    455[0m [0;34m[0m[0m
[1;32m    456[0m     [0;31m# Create the parser.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 457[0;31m     [0mparser[0m [0;34m=[0m [0mTextFileReader[0m[0;34m([0m[0mfp_or_buf[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    458[0m [0;34m[0m[0m
[1;32m    459[0m     [0;32mif[0m [0mchunksize[0m [0;32mor[0m [0miterator[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, f, engine, **kwds)[0m
[1;32m    893[0m             [0mself[0m[0;34m.[0m[0moptions[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m [0;34m=[0m [0mkwds[0m[0;34m[[0m[0;34m"has_index_names"[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m
[1;32m    894[0m [0;34m[0m[0m
[0;32m--> 895[0;31m         [0mself[0m[0;34m.[0m[0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mengine[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    896[0m [0;34m[0m[0m
[1;32m    897[0m     [0;32mdef[0m [0mclose[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m_make_engine[0;34m(self, engine)[0m
[1;32m   1133[0m     [0;32mdef[0m [0m_make_engine[0m[0;34m([0m[0mself[0m[0;34m,[0m [0mengine[0m[0;34m=[0m[0;34m"c"[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1134[0m         [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"c"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1135[0;31m             [0mself[0m[0;34m.[0m[0m_engine[0m [0;34m=[0m [0mCParserWrapper[0m[0;34m([0m[0mself[0m[0;34m.[0m[0mf[0m[0;34m,[0m [0;34m**[0m[0mself[0m[0;34m.[0m[0moptions[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1136[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1137[0m             [0;32mif[0m [0mengine[0m [0;34m==[0m [0;34m"python"[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m/opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/pandas/io/parsers.py[0m in [0;36m__init__[0;34m(self, src, **kwds)[0m
[1;32m   1915[0m         [0mkwds[0m[0;34m[[0m[0;34m"usecols"[0m[0;34m][0m [0;34m=[0m [0mself[0m[0;34m.[0m[0musecols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1916[0m [0;34m[0m[0m
[0;32m-> 1917[0;31m         [0mself[0m[0;34m.[0m[0m_reader[0m [0;34m=[0m [0mparsers[0m[0;34m.[0m[0mTextReader[0m[0;34m([0m[0msrc[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1918[0m         [0mself[0m[0;34m.[0m[0munnamed_cols[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_reader[0m[0;34m.[0m[0munnamed_cols[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1919[0m [0;34m[0m[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader.__cinit__[0;34m()[0m

[0;32mpandas/_libs/parsers.pyx[0m in [0;36mpandas._libs.parsers.TextReader._setup_parser_source[0;34m()[0m

[0;31mFileNotFoundError[0m: [Errno 2] File b'./m5-forecasting-accuracy/calendar.csv' does not exist: b'./m5-forecasting-accuracy/calendar.csv'
